[
  {
    "objectID": "demos/01-into-tidyverse.html",
    "href": "demos/01-into-tidyverse.html",
    "title": "Demo 01: Into the tidyverse",
    "section": "",
    "text": "(Broadly speaking) EDA = questions about data + wrangling + visualization\nR for Data Science: “EDA is a state of mind”, an iterative cycle:\n\ngenerate questions\nanswer via transformations and visualizations\n\nExample of questions?\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\nGoal: develop understanding and become familiar with your data\n\nEDA is NOT a replacement for statistical inference and learning\nEDA is an important and necessary step to build intuition\n\nWe tackle the challenges of EDA with a data science workflow. An example of this according to Hadley Wickham in R for Data Science:\n\n\n\n\n\nAspects of data wrangling:\n\nimport: reading in data (e.g., read_csv())\ntidy: rows = observations, columns = variables (i.e. tabular data)\ntransform: filter observations, create new variables, summarize, etc."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#what-is-exploratory-data-analysis-eda",
    "href": "demos/01-into-tidyverse.html#what-is-exploratory-data-analysis-eda",
    "title": "Demo 01: Into the tidyverse",
    "section": "",
    "text": "(Broadly speaking) EDA = questions about data + wrangling + visualization\nR for Data Science: “EDA is a state of mind”, an iterative cycle:\n\ngenerate questions\nanswer via transformations and visualizations\n\nExample of questions?\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\nGoal: develop understanding and become familiar with your data\n\nEDA is NOT a replacement for statistical inference and learning\nEDA is an important and necessary step to build intuition\n\nWe tackle the challenges of EDA with a data science workflow. An example of this according to Hadley Wickham in R for Data Science:\n\n\n\n\n\nAspects of data wrangling:\n\nimport: reading in data (e.g., read_csv())\ntidy: rows = observations, columns = variables (i.e. tabular data)\ntransform: filter observations, create new variables, summarize, etc."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#working-with-penguins",
    "href": "demos/01-into-tidyverse.html#working-with-penguins",
    "title": "Demo 01: Into the tidyverse",
    "section": "Working with penguins",
    "text": "Working with penguins\nIn R, there are many libraries or packages/groups of programs that are not permanently stored in R, so we have to load them when we want to use them. You can load an R package by typing library(package_name). (Sometimes we need to download/install the package first, as described in HW0.)\nThroughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nImport the penguins dataset by loading the palmerpenguins package using the library function and then access the data with the data() function:\n\nlibrary(palmerpenguins) \ndata(penguins)\n\nView some basic info about the penguins dataset:\n\n# displays same info as c(nrow(penguins), ncol(penguins))\ndim(penguins) \n\n[1] 344   8\n\nclass(penguins)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\ntbl (pronounced tibble) is the tidyverse way of storing tabular data, like a spreadsheet or data.frame\nI assure you that you’ll run into errors as you code in R; in fact, my attitude as a coder is that something is wrong if I never get any errors while working on a project. When you run into an error, your first reaction may be to panic and post a question to Piazza. However, checking help documentation in R can be a great way to figure out what’s going wrong. (For good or bad, I end up having to read help documentation almost every day of my life - because, well, I regularly make mistakes in R.)\nLook at the help documentation for penguins by typing help(penguins) in the Console. What are the names of the variables in this dataset? How many observations are in this dataset?\n\nhelp(penguins)\n\nYou should always look at your data before doing anything: view the first 6 (by default) rows with head()\n\nhead(penguins) # Try just typing penguins into your console, what happens?\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIs our penguins dataset tidy?\n\nEach row = a single penguin\nEach column = different measurement about the penguins (can print out column names directly with colnames(penguins) or names(penguins))\n\nWe’ll now explore differences among the penguins using the tidyverse."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#let-the-data-wrangling-begin",
    "href": "demos/01-into-tidyverse.html#let-the-data-wrangling-begin",
    "title": "Demo 01: Into the tidyverse",
    "section": "Let the data wrangling begin…",
    "text": "Let the data wrangling begin…\nFirst, load the tidyverse for exploring the data - and do NOT worry about the warning messages that will pop-up! Warning messages will tell you when other packages that are loaded may have functions replaced with the most recent package you’ve loaded. In general though, you should just be concerned when an error message pops up (errors are different than warnings!).\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWe’ll start by summarizing continuous (e.g., bill_length_mm, flipper_length_mm) and categorical (e.g., species, island) variables in different ways.\nWe can compute summary statistics for continuous variables with the summary() function:\n\nsummary(penguins$bill_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCompute counts of categorical variables with table() function:\n\ntable(\"island\" = penguins$island) # be careful it ignores NA values!\n\nisland\n   Biscoe     Dream Torgersen \n      168       124        52 \n\n\nHow do we remove the penguins with missing bill_length_mm values? Within the tidyverse, dplyr is a package with functions for data wrangling (because it’s within the tidyverse that means you do NOT have to load it separately with library(dplyr) after using library(tidyverse)!). It’s considered a “grammar of data manipulation”: dplyr functions are verbs, datasets are nouns.\nWe can filter() our dataset to choose observations meeting conditions:\n\nclean_penguins &lt;- filter(penguins, !is.na(bill_length_mm))\n# Use help(is.na) to see what it returns. And then observe \n# that the ! operator means to negate what comes after it.\n# This means !TRUE == FALSE (i.e., opposite of TRUE is equal to FALSE).\nnrow(penguins) - nrow(clean_penguins) # Difference in rows\n\n[1] 2\n\n\nIf we want to only consider a subset of columns in our data, we can select() variables of interest:\n\nsel_penguins &lt;- select(clean_penguins, species, island, bill_length_mm, flipper_length_mm)\nhead(sel_penguins, n = 3)\n\n# A tibble: 3 × 4\n  species island    bill_length_mm flipper_length_mm\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;             &lt;int&gt;\n1 Adelie  Torgersen           39.1               181\n2 Adelie  Torgersen           39.5               186\n3 Adelie  Torgersen           40.3               195\n\n\nWe can arrange() our dataset to sort observations by variables:\n\nbill_penguins &lt;- arrange(sel_penguins, desc(bill_length_mm)) # use desc() for descending order\nhead(bill_penguins, n = 3)\n\n# A tibble: 3 × 4\n  species   island bill_length_mm flipper_length_mm\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;             &lt;int&gt;\n1 Gentoo    Biscoe           59.6               230\n2 Chinstrap Dream            58                 181\n3 Gentoo    Biscoe           55.9               228\n\n\nWe can summarize() our dataset to one row based on functions of variables:\n\nsummarize(bill_penguins, max(bill_length_mm), median(flipper_length_mm))\n\n# A tibble: 1 × 2\n  `max(bill_length_mm)` `median(flipper_length_mm)`\n                  &lt;dbl&gt;                       &lt;dbl&gt;\n1                  59.6                         197\n\n\nWe can mutate() our dataset to create new variables:\n\nnew_penguins &lt;- mutate(bill_penguins, \n                       bill_flipper_ratio = bill_length_mm / flipper_length_mm,\n                       flipper_bill_ratio = flipper_length_mm / bill_length_mm)\nhead(new_penguins, n = 1)\n\n# A tibble: 1 × 6\n  species island bill_length_mm flipper_length_mm bill_flipper_ratio\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;             &lt;int&gt;              &lt;dbl&gt;\n1 Gentoo  Biscoe           59.6               230              0.259\n# ℹ 1 more variable: flipper_bill_ratio &lt;dbl&gt;\n\n\nHow do we perform several of these actions?\n\nhead(arrange(select(mutate(filter(penguins, !is.na(flipper_length_mm)), bill_flipper_ratio = bill_length_mm / flipper_length_mm), species, island, bill_flipper_ratio), desc(bill_flipper_ratio)), n = 1)\n\n# A tibble: 1 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n\n\nThat’s awfully annoying to do, and also difficult to read…"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#enter-the-pipeline",
    "href": "demos/01-into-tidyverse.html#enter-the-pipeline",
    "title": "Demo 01: Into the tidyverse",
    "section": "Enter the pipeline",
    "text": "Enter the pipeline\nThe |&gt; (pipe) operator is used in the to chain commands together. Note: you can also use the tidyverse pipe %&gt;% (from magrittr), but |&gt; is the built-in pipe that is native to new versions of R without loading the tidyverse.\n|&gt; directs the data analyis pipeline: output of one function pipes into input of the next function\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  mutate(bill_flipper_ratio = bill_length_mm / flipper_length_mm) |&gt;\n  select(species, island, bill_flipper_ratio) |&gt;\n  arrange(desc(bill_flipper_ratio)) |&gt;\n  head(n = 5)\n\n# A tibble: 5 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n2 Chinstrap Dream               0.275\n3 Chinstrap Dream               0.270\n4 Chinstrap Dream               0.270\n5 Chinstrap Dream               0.268"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#more-pipeline-actions",
    "href": "demos/01-into-tidyverse.html#more-pipeline-actions",
    "title": "Demo 01: Into the tidyverse",
    "section": "More pipeline actions!",
    "text": "More pipeline actions!\nInstead of head(), we can slice() our dataset to choose the observations based on the position\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  mutate(bill_flipper_ratio = bill_length_mm / flipper_length_mm) |&gt;\n  select(species, island, bill_flipper_ratio) |&gt;\n  arrange(desc(bill_flipper_ratio)) |&gt;\n  slice(c(1, 2, 10, 100))\n\n# A tibble: 4 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n2 Chinstrap Dream               0.275\n3 Chinstrap Dream               0.264\n4 Gentoo    Biscoe              0.227"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#grouped-operations",
    "href": "demos/01-into-tidyverse.html#grouped-operations",
    "title": "Demo 01: Into the tidyverse",
    "section": "Grouped operations",
    "text": "Grouped operations\nWe group_by() to split our dataset into groups based on a variable’s values\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  group_by(island) |&gt;\n  summarize(n_penguins = n(), #counts number of rows in group\n            ave_flipper_length = mean(flipper_length_mm), \n            sum_bill_depth = sum(bill_depth_mm),\n            .groups = \"drop\") |&gt; # all levels of grouping dropping\n  arrange(desc(n_penguins)) |&gt;\n  slice(1:5)\n\n# A tibble: 3 × 4\n  island    n_penguins ave_flipper_length sum_bill_depth\n  &lt;fct&gt;          &lt;int&gt;              &lt;dbl&gt;          &lt;dbl&gt;\n1 Biscoe           167               210.          2651.\n2 Dream            124               193.          2275.\n3 Torgersen         51               191.           940.\n\n\n\ngroup_by() is only useful in a pipeline (e.g. with summarize()), and pay attention to its behavior\nspecify the .groups field to decide if observations remain grouped or not after summarizing (you can also use ungroup() for this as well)"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#putting-it-all-together",
    "href": "demos/01-into-tidyverse.html#putting-it-all-together",
    "title": "Demo 01: Into the tidyverse",
    "section": "Putting it all together…",
    "text": "Putting it all together…\nAs your own exercise, create a tidy dataset where each row == an island with the following variables:\n\nnumber of penguins,\nnumber of unique species on the island (see help(unique)),\naverage body_mass_g,\nvariance (see help(var)) of bill_depth_mm\n\nPrior to making those variables, make sure to filter missings and also only consider female penguins. Then arrange the islands in order of the average body_mass_g:\n\n# INSERT YOUR CODE HERE"
  },
  {
    "objectID": "demos/07-trends-time-series.html",
    "href": "demos/07-trends-time-series.html",
    "title": "Demo 07: Visualizing trends and time series data",
    "section": "",
    "text": "In this demo, we’ll first work with a dataset on the number of PhD degrees awarded in the US from TidyTuesday.\n\n# Read in the tidytuesday data\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nphd_field &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\n\nRows: 3370 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): broad_field, major_field, field\ndbl (2): year, n_phds\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nphd_field\n\n# A tibble: 3,370 × 5\n   broad_field   major_field                                 field   year n_phds\n   &lt;chr&gt;         &lt;chr&gt;                                       &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Life sciences Agricultural sciences and natural resources Agric…  2008    111\n 2 Life sciences Agricultural sciences and natural resources Agric…  2008     28\n 3 Life sciences Agricultural sciences and natural resources Agric…  2008      3\n 4 Life sciences Agricultural sciences and natural resources Agron…  2008     68\n 5 Life sciences Agricultural sciences and natural resources Anima…  2008     41\n 6 Life sciences Agricultural sciences and natural resources Anima…  2008     18\n 7 Life sciences Agricultural sciences and natural resources Anima…  2008     77\n 8 Life sciences Agricultural sciences and natural resources Envir…  2008    182\n 9 Life sciences Agricultural sciences and natural resources Fishi…  2008     52\n10 Life sciences Agricultural sciences and natural resources Food …  2008     96\n# ℹ 3,360 more rows\n\n\nLet’s start by grabbing the rows corresponding to Statistics PhDs. While there are a number of ways to do this, we can grab field containing “statistics” (including biostatistics) with the str_detect() function.\n\nstats_phds &lt;- phd_field |&gt;\n  filter(str_detect(tolower(field), \"statistics\"))\n\nWhat are the different fields that were captured?\n\ntable(stats_phds$field)\n\n\n                       Biometrics and biostatistics \n                                                 10 \n           Educational statistics, research methods \n                                                 10 \nManagement information systems, business statistics \n                                                 10 \n                Mathematics and statistics, general \n                                                 10 \n                  Mathematics and statistics, other \n                                                 10 \n                           Statistics (mathematics) \n                                                 10 \n                       Statistics (social sciences) \n                                                 10 \n\n\nTo start, let’s just summarize the number of PhDs by year:\n\nstat_phd_year_summary &lt;- stats_phds |&gt;\n  group_by(year) |&gt;\n  summarize(n_phds = sum(n_phds))\n\nNow, we’ll make the typical scatterplot display with n_phds on the y-axis and year on the x-axis:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nWe should fix our x-axis here and make the breaks more informative. In this case, I’ll change it so each year is labeled (that may not be appropriate for every visual but it works out here).\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  # Modify the x-axis to make the axis breaks at the unique years and show their\n  # respective labels\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nTo emphasize the ordering of the year along the x-axis, we’ll add a line connecting the points to emphasize the order:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nWe can drop the points, leaving only the connecting lines to emphasize trends:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nAnother common way to display trends is by filling in the area under the line. However, this is only appropriate when the y-axis starts at 0! It’s also redundant use of ink so just be careful when deciding whether or not to fill the area. We can fill the area under the line with the geom_area() aesthetic - but note that it changes the y-axis by default to start at 0:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  # Fill the area under the line\n  geom_area(fill = \"darkblue\", alpha = 0.5) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nYou can also make this plot using the ggridges package."
  },
  {
    "objectID": "demos/07-trends-time-series.html#formatting-date-data-in-r",
    "href": "demos/07-trends-time-series.html#formatting-date-data-in-r",
    "title": "Demo 07: Visualizing trends and time series data",
    "section": "Formatting Date data in R",
    "text": "Formatting Date data in R\nThe following code redefines creates a new column in our dataset to be the monthly dates 1/1/1959, 2/1/1959, … , 12/1/1997 using the as.Date() function (given the description of the time range in help(co2)):\n\nco2_tbl &lt;- co2_tbl |&gt;\n  # We can use the seq() function with dates which is pretty useful!\n  mutate(obs_date = seq(as.Date(\"1959/1/1\"), by = \"month\",\n                        length.out = n()))\nco2_tbl |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Year\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nUnfortunately, the default format for dates in as.Date() is Year/Month/Day. If you prefer another format, such as the common Month/Day/Year (as I used above in the previous paragraph), you need to include the format argument within as.Date(), as such (note that the Y is capitalized - yes, as.Date() is that picky):\n\nco2_tbl &lt;- co2_tbl |&gt;\n  mutate(obs_date = seq(as.Date(\"1/1/1959\", format = \"%m/%d/%Y\"), \n                        by = \"month\",\n                        length.out = n()))\nco2_tbl |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Year\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nNote that all we needed to do was convert the obs_date variable to a Date class, and then we could use ggplot as is. This is because ggplot knows to use a special date scale for the x-axis when x has class Date. As a result, we can easily play with the breaks on the date axis using scale_x_date(). For example:\n\nFor a subset of the data, maybe we only want ticks every 4 months, using date_breaks.\nWe can specify the format of the date with date_labels. (See Details section of ?strftime for the formatting options. Here, we choose abbreviated month %b and full year %Y.)\n\n\nco2_tbl[1:26,] |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  scale_x_date(date_breaks = \"4 months\", date_labels = \"%b %Y\") +\n  labs(x = \"Year\", y = \"CO2 (ppm)\") +\n  # Modify the x-axis text \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "demos/02-scatterplots-regression.html",
    "href": "demos/02-scatterplots-regression.html",
    "title": "Demo 02: Scatterplots and regression with categorical variables",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "demos/02-scatterplots-regression.html#colors",
    "href": "demos/02-scatterplots-regression.html#colors",
    "title": "Demo 02: Scatterplots and regression with categorical variables",
    "section": "Colors",
    "text": "Colors\nWe can color by a third variable (e.g., different color for each category).\nNote that you can put color = inside ggplot or geom_point, both display the same visualization:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = species))\n\n\n\n\n\n\n\n\nYou can also color by a quantitative variable using a color scale/gradient:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = body_mass_g))\n\n\n\n\n\n\n\n\nThe default color gradient is not the most appealing, while there are a number of possibilities - blue to orange is a good choice since these colors are opposites on the color spectrum:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = body_mass_g)) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\")"
  },
  {
    "objectID": "demos/02-scatterplots-regression.html#point-size-size",
    "href": "demos/02-scatterplots-regression.html#point-size-size",
    "title": "Demo 02: Scatterplots and regression with categorical variables",
    "section": "Point size (size)",
    "text": "Point size (size)\nWe can also map variables to other aesthetics, e.g. size:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(size = body_mass_g))"
  },
  {
    "objectID": "demos/02-scatterplots-regression.html#point-type-shape",
    "href": "demos/02-scatterplots-regression.html#point-type-shape",
    "title": "Demo 02: Scatterplots and regression with categorical variables",
    "section": "Point type (shape)",
    "text": "Point type (shape)\nor the type (shape) of points:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(shape = species))"
  },
  {
    "objectID": "demos/02-scatterplots-regression.html#combining-aesthetics",
    "href": "demos/02-scatterplots-regression.html#combining-aesthetics",
    "title": "Demo 02: Scatterplots and regression with categorical variables",
    "section": "Combining aesthetics",
    "text": "Combining aesthetics\nWe can even do several of these at once:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm,\n             color = species, shape = island,\n             size = body_mass_g)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nThe above graph may be a bit difficult to read, but it contains a lot of information in the sense that it is a 5-dimensional graphic:\n\nx = bill depth (mm)\ny = bill length (mm)\ncolor = species\nsize = body mass (g)\nshape = island\n\nBut be careful! The more complications you add, the more difficult your graph is to explain."
  },
  {
    "objectID": "demos/02-scatterplots-regression.html#simple-linear-regression-based-only-on-bill-length",
    "href": "demos/02-scatterplots-regression.html#simple-linear-regression-based-only-on-bill-length",
    "title": "Demo 02: Scatterplots and regression with categorical variables",
    "section": "Simple Linear Regression (based only on bill length)",
    "text": "Simple Linear Regression (based only on bill length)\nFirst, we can run a simple linear regression (the first model) based only on bill length. We can display this line via geom_smooth():\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE)\n\n\n\n\n\n\n\n\nAnd display the regression model output using summary():\n\nsummary(lm(bill_depth_mm ~ bill_length_mm, data = penguins))\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    20.88547    0.84388  24.749  &lt; 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05\n\n\nWe can write this regression model as:\n\\[\\text{depth} \\sim N(\\beta_0 + \\beta_L \\cdot \\text{length}, \\sigma^2)\\]\nNote that \\(\\beta_0\\) is the intercept and \\(\\beta_L\\) is the slope.\nThus, our estimates are:\n\n\\(\\hat{\\beta}_0 = 20.88547\\)\n\\(\\hat{\\beta}_L = 12.43\\)\n\\(\\hat{\\sigma}^2 = 1.922^2\\)"
  },
  {
    "objectID": "demos/02-scatterplots-regression.html#multiple-linear-regression-additive",
    "href": "demos/02-scatterplots-regression.html#multiple-linear-regression-additive",
    "title": "Demo 02: Scatterplots and regression with categorical variables",
    "section": "Multiple Linear Regression (Additive)",
    "text": "Multiple Linear Regression (Additive)\nWe can also run the second model, which is based on length and species, but with only additive effects. First, we’ll check the counts of the species variable to ensure that the species with the highest number of observations if the reference level (i.e., the first level for a factor variable):\n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nLooks like we’re lucky and that the Adelie species is the most popular and is already first due to alphabetical order. What function would we need to do to re-order the variable?\nNext, we’ll fit the regression that accounts for species without an interaction - so it’s just an additive effect:\n\ndepth_lm_species_add &lt;- lm(bill_depth_mm ~ bill_length_mm + species,\n                           data = penguins)\nsummary(depth_lm_species_add)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4529 -0.6864 -0.0508  0.5519  3.5915 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      10.59218    0.68302  15.508  &lt; 2e-16 ***\nbill_length_mm    0.19989    0.01749  11.427  &lt; 2e-16 ***\nspeciesChinstrap -1.93319    0.22416  -8.624 2.55e-16 ***\nspeciesGentoo    -5.10602    0.19142 -26.674  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9533 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.769, Adjusted R-squared:  0.7669 \nF-statistic: 375.1 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nThis tells us that Chinstrap is different from Adelie and Gentoo is different from Adelie, but it does NOT tell us Chinstrap is different from Gentoo. That would require another model with a reordered species variable. Exercise: Reorder species so that Gentoo is the reference level and compare to the results above.\nWe can manually extract intercepts and coefficients to use for plotting (read the code comments!):\n\n# Calculate species-specific intercepts in order:\nintercepts &lt;- # First for `Adelie` it's just the initial intercept\n  c(coef(depth_lm_species_add)[\"(Intercept)\"],\n    # Next for `Chinstrap` it's the intercept plus the `Chinstrap` term:\n    coef(depth_lm_species_add)[\"(Intercept)\"] + \n      coef(depth_lm_species_add)[\"speciesChinstrap\"],\n    # And finally for `Gentoo` it's again the intercept plus the `Gentoo` term\n    coef(depth_lm_species_add)[\"(Intercept)\"] + \n      coef(depth_lm_species_add)[\"speciesGentoo\"])\n\n# Create a small table to store the intercept, slopes, and species:\nlines_tbl &lt;- tibble(\"intercepts\" = intercepts,\n                    # Slopes are the same for each, thus use rep()\n                    \"slopes\" = rep(coef(depth_lm_species_add)[\"bill_length_mm\"],\n                                   3),\n                    # And the levels of species:\n                    \"species\" = levels(penguins$species))\n\nWe can now plot this model by specifying the regression lines with geom_abline() using the newly constructed lines_tbl as the data for this layer:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(data = lines_tbl,\n              aes(intercept = intercepts, slope = slopes,\n                  color = species)) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nThis is a great example of Simpson’s Paradox! We originally observed a negative linear relationship between depth and length, but now observe a positive linear relationship within species!"
  },
  {
    "objectID": "demos/02-scatterplots-regression.html#multiple-linear-regression-interactive",
    "href": "demos/02-scatterplots-regression.html#multiple-linear-regression-interactive",
    "title": "Demo 02: Scatterplots and regression with categorical variables",
    "section": "Multiple Linear Regression (Interactive)",
    "text": "Multiple Linear Regression (Interactive)\nNext, we can run the third model, which is based on length and species, including interaction effects. This is the default type of model displayed when we map species to the color aesthetic for the geom_smooth() layer. In the plot below, we display across both layers, geom_point() and geom_smooth() by mapping species to color in the initial ggplot canvas construction:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nWhat about the summary of this model? Is the inclusion of interaction terms relevant? Note that by default, multiplying two variables in the lm() formula below includes both the additive AND interaction terms.\n\ndepth_lm_species_int &lt;- lm(bill_depth_mm ~ bill_length_mm * species,\n                           data = penguins)\nsummary(depth_lm_species_int)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm * species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6574 -0.6675 -0.0524  0.5383  3.5032 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     11.40912    1.13812  10.025  &lt; 2e-16 ***\nbill_length_mm                   0.17883    0.02927   6.110 2.76e-09 ***\nspeciesChinstrap                -3.83998    2.05398  -1.870 0.062419 .  \nspeciesGentoo                   -6.15812    1.75451  -3.510 0.000509 ***\nbill_length_mm:speciesChinstrap  0.04338    0.04558   0.952 0.341895    \nbill_length_mm:speciesGentoo     0.02601    0.04054   0.642 0.521590    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9548 on 336 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7662 \nF-statistic: 224.5 on 5 and 336 DF,  p-value: &lt; 2.2e-16\n\n\nThe interaction terms do NOT appear to be necessary to include. This is justified by both the lack of significance and the slight drop in adjusted R-squared."
  },
  {
    "objectID": "demos/02-scatterplots-regression.html#what-about-the-intercept",
    "href": "demos/02-scatterplots-regression.html#what-about-the-intercept",
    "title": "Demo 02: Scatterplots and regression with categorical variables",
    "section": "What about the intercept?",
    "text": "What about the intercept?\nRemember the meaning of the intercept term… that is not reasonable in this setting because penguins will never have bills with length of 0mm! We should update the additive model (since we found the interaction terms to not be meaningful) to remove the intercept. This can be done by adding a 0 term to the lm() formula:\n\ndepth_lm_remove_b0 &lt;- lm(bill_depth_mm ~ 0 + bill_length_mm + species,\n                         data = penguins)\nsummary(depth_lm_remove_b0)\n\n\nCall:\nlm(formula = bill_depth_mm ~ 0 + bill_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4529 -0.6864 -0.0508  0.5519  3.5915 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nbill_length_mm    0.19989    0.01749  11.427  &lt; 2e-16 ***\nspeciesAdelie    10.59218    0.68302  15.508  &lt; 2e-16 ***\nspeciesChinstrap  8.65899    0.86207  10.044  &lt; 2e-16 ***\nspeciesGentoo     5.48616    0.83547   6.567 1.94e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9533 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.997, Adjusted R-squared:  0.997 \nF-statistic: 2.795e+04 on 4 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nWhat changed in the summary output? Why did that occur?\nWe can copy-and-paste our code from above to add these appropriate regression lines:\n\n# Calculate species-specific intercepts in order:\nnew_intercepts &lt;- # First for `Adelie` \n  c(coef(depth_lm_remove_b0)[\"speciesAdelie\"],\n    # Next for `Chinstrap` \n    coef(depth_lm_remove_b0)[\"speciesChinstrap\"],\n    # And finally for `Gentoo` \n    coef(depth_lm_remove_b0)[\"speciesGentoo\"])\n\n# Create a small table to store the intercept, slopes, and species:\nnew_lines_tbl &lt;- \n  tibble(\"intercepts\" = new_intercepts,\n         # Slopes are the same for each, thus use rep()\n         \"slopes\" = rep(coef(depth_lm_remove_b0)[\"bill_length_mm\"],\n                        3),\n         # And the levels of species:\n         \"species\" = levels(penguins$species))\n\nAgain, create the display:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(data = new_lines_tbl,\n              aes(intercept = intercepts, slope = slopes,\n                  color = species)) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nWhy is this the same display as before? Here’s a great description of why we observe a higher R-squared with the intercept-term excluded from the model."
  },
  {
    "objectID": "demos/03-simple-highdim.html",
    "href": "demos/03-simple-highdim.html",
    "title": "Demo 03: Simple visuals for high-dimensional data",
    "section": "",
    "text": "More fun with penguins\nThe graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nWe load the penguins data in the same way as the previous demos:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\nPairs plot with GGally\nWe will use the GGally package to make pairs plots in R with ggplot figures. You need to install the package:\n\ninstall.packages(\"GGally\")\n\nNext, we’ll load the package and create a pairs plot of just the continuous variables using ggpairs. The main arguments you have to worry about for ggpairs are data, columns, and mapping:\n\ndata: specifies the dataset\ncolumns: Columns of data you want in the plot (can specify with vector of column names or numbers referring to the column indices)\nmapping: aesthetics using aes(). Most important one is aes(color = &lt;variable name&gt;)\n\nFirst, let’s create a pairs plot by specifying columns as the four columns of continuous variables (columns 3 through 6):\n\nlibrary(GGally)\npenguins |&gt; ggpairs(columns = 3:6)\n\n\n\n\n\n\n\n\nObviously this suffers from over-plotting so we’ll want to adjust the alpha. An annoying thing is that we specify the alpha directionly with aes when using ggpairs:\n\npenguins |&gt; ggpairs(columns = 3:6, mapping = aes(alpha = 0.5))\n\n\n\n\n\n\n\n\nPlots along the diagonal show marginal distributions. Plots along the off-diagonal show joint (pairwise) distributions or statistical summaries (e.g., correlation) to avoid redundancy. The matrix of plots is symmetric; e.g., entry (1,2) shows the same distribution as entry (2,1). However, entry (1,2) and entry (2,1) display different bits of information (or alternative plots) about the same distribution.\nWe could also specify categorical variables in the plot. We also don’t need to specify column indices if we just select which columns to use beforehand:\n\npenguins |&gt; \n  dplyr::select(bill_length_mm, body_mass_g, species, island) |&gt;\n  ggpairs(mapping = aes(alpha = 0.5))\n\n\n\n\n\n\n\n\nAlternatively, we can use the mapping argument to display these categorical variables in a different manner - and arguably more efficiently:\n\npenguins |&gt; \n  ggpairs(columns = c(\"bill_length_mm\", \"body_mass_g\", \"island\"),\n          mapping = aes(alpha = 0.5, color = species))\n\n\n\n\n\n\n\n\nThe ggpairs function in GGally is very flexible and customizable with regards to which figures are displayed in the various panels. I encourage you to check out the vignettes and demos on the package website for more examples. For instance, in the pairs plot below I decide to display the regression lines and make other adjustments to the off-diagonal figures:\n\npenguins |&gt; \n  ggpairs(columns = c(\"bill_length_mm\", \"body_mass_g\", \"island\"),\n          mapping = aes(alpha = 0.5, color = species), \n          lower = list(\n            continuous = \"smooth_lm\", \n            combo = \"facetdensitystrip\"\n          ),\n          upper = list(\n            continuous = \"cor\",\n            combo = \"facethist\"\n          )\n  )\n\n\n\n\n\n\n\n\nYou can also proceed to customize the pairs plot in the same manner as ggplot figures:\n\npenguins |&gt;\n  dplyr::select(species, body_mass_g, ends_with(\"_mm\")) |&gt;\n  ggpairs(mapping = aes(color = species, alpha = 0.5),\n          columns = c(\"flipper_length_mm\", \"body_mass_g\",\n                      \"bill_length_mm\", \"bill_depth_mm\")) +\n  scale_colour_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  theme_bw() +\n  theme(strip.text = element_text(size = 7))\n\n\n\n\n\n\n\n\n\n\nCorrelograms with ggcorrplot\nWe can visualize the correlation matrix for the variables in a dataset using the ggcorrplot package. You need to install the package:\n\ninstall.packages(\"ggcorrplot\")\n\nNext, we’ll load the package and create a correlogram using only the continuous variables. To do this, we first need to compute the correlation matrix for these variables:\n\npenguins_cor_matrix &lt;- penguins |&gt;\n  dplyr::select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  cor(use = \"complete.obs\")\npenguins_cor_matrix\n\n                  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\nbill_length_mm         1.0000000    -0.2350529         0.6561813   0.5951098\nbill_depth_mm         -0.2350529     1.0000000        -0.5838512  -0.4719156\nflipper_length_mm      0.6561813    -0.5838512         1.0000000   0.8712018\nbody_mass_g            0.5951098    -0.4719156         0.8712018   1.0000000\n\n\nNOTE: Since there are missing values in the penguins data we need to indicate in the cor() function how to handle missing values using the use argument. By default, the correlations are returned as NA, which is not what we want. Instead, we can change this to only use observations without NA values for the considered columns (see help(cor) for more options).\nNow, we can create the correlogram using ggcorrplot() using this correlation matrix:\n\nlibrary(ggcorrplot)\nggcorrplot(penguins_cor_matrix)\n\n\n\n\n\n\n\n\nThere are several ways we can improve this correlogram:\n\nwe can avoid redundancy by only using one half of matrix by changing the type input: the default is full, we can make it lower or upper instead:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\")\n\n\n\n\n\n\n\n\n\nwe can rearrange the variables using hierarchical clustering so that variables displaying stronger levels of correlation are closer together along the diagonal by setting hc.order = TRUE:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\", hc.order = TRUE)\n\n\n\n\n\n\n\n\n\nif we want to add the correlation values directly to the plot, we can include those labels setting lab = TRUE - but we should round the correlation values first using the round() function:\n\n\nggcorrplot(round(penguins_cor_matrix, digits = 4), \n           type = \"lower\", hc.order = TRUE, lab = TRUE)\n\n\n\n\n\n\n\n\n\nif we want to place more stress on the correlation magnitude, we can change the method input to circle so that the size of the displayed circles is mapped to the absolute value of the correlation value:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\", hc.order = TRUE,\n           method = \"circle\")\n\n\n\n\n\n\n\n\nYou can ignore the Warning message that is displayed - just from the differences in ggplot implementation.\n\n\nHeatmaps to display dataset structure with color\nHeatmaps provide a way to display structure of the dataset using the fill of tiles in a matrix. The fill of the tiles is mapped to a variable’s standardized value (i.e., (x - mean(x)) / sd(x)). There is a convenient function in R called heatmap to create this type of figure:\n\nheatmap(as.matrix(dplyr::select(penguins, \n                                bill_length_mm, bill_depth_mm, \n                                flipper_length_mm, body_mass_g)),\n        scale = \"column\", \n        Rowv = NA, Colv = NA)\n\n\n\n\n\n\n\n\nIn order to manually create this figure, we’ll need to pivot our dataset from wide to long using the pivot_longer() function. This results in a dataset with one row per observation and variable combination. Then we use geom_tile as the geometric object with the standardized value mapped to the fill:\n\npenguins |&gt;\n  mutate(penguin_index = as.factor(paste0(\"Penguin-\", 1:n()))) |&gt;\n  dplyr::select(penguin_index, bill_length_mm, bill_depth_mm, \n                flipper_length_mm, body_mass_g) |&gt;\n  pivot_longer(bill_length_mm:body_mass_g,\n               names_to = \"variable\",\n               values_to = \"raw_value\") |&gt;\n  group_by(variable) |&gt;\n  mutate(std_value = (raw_value - mean(raw_value, na.rm = TRUE)) / \n           sd(raw_value, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = variable, y = penguin_index, fill = std_value)) +\n  geom_tile() +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        axis.text.y = element_text(size = 2)) \n\n\n\n\n\n\n\n\nIn order to provide some notion of the correlation structure between variables, it’s useful to reorder the observations in the heatmap display by some variable:\n\npenguins |&gt;\n  mutate(penguin_index = as.factor(paste0(\"Penguin-\", 1:n())),\n         penguin_index = fct_reorder(penguin_index, body_mass_g,\n                                     # Ignore the missings when reordering\n                                     .na_rm = TRUE)) |&gt;\n  dplyr::select(penguin_index, bill_length_mm, bill_depth_mm, \n                flipper_length_mm, body_mass_g) |&gt;\n  pivot_longer(bill_length_mm:body_mass_g,\n               names_to = \"variable\",\n               values_to = \"raw_value\") |&gt;\n  group_by(variable) |&gt;\n  mutate(std_value = (raw_value - mean(raw_value, na.rm = TRUE)) / \n           sd(raw_value, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = variable, y = penguin_index, fill = std_value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        axis.text.y = element_text(size = 2)) \n\n\n\n\n\n\n\n\n\n\nParallel coordinates plot with GGally\nIn a parallel coordinates plot, we create an axis for each varaible and align these axes side-by-side, drawing lines between observations from one axis to the next. This can be useful for visualizing structure among both the variables and observations in our dataset. These are useful when working with a moderate number of observations and variables - but can be overwhelming with too many.\nWe use the ggparcoord() function from the GGally package to make parallel coordinates plots:\n\npenguins |&gt;\n  ggparcoord(columns = 3:6)\n\n\n\n\n\n\n\n\nThere are several ways we can modify this parallel coordinates plot:\n\nwe should always adjust the transparency of the lines using the alphaLines input to help handle overlap:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2)\n\n\n\n\n\n\n\n\n\nwe can color each observation’s lines by a categorical variable, which can be useful for revealing group structure:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\")\n\n\n\n\n\n\n\n\n\nwe can change how the y-axis is constructed by modifying the scale input, which by default is std that is simply subtracting the mean and dividing by the standard deviation. We could instead use uniminmax so that minimum of the variable is zero and the maximum is one:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             scale = \"uniminmax\")\n\n\n\n\n\n\n\n\n\nwe can also reorder the variables a number of different ways with the order input (see help(ggparcoord) for details). There appears to be some weird errors however with the different options, but you can still manually provide the order of indices as follows:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             order = c(6, 5, 3, 4))"
  },
  {
    "objectID": "demos/09-areal-data.html",
    "href": "demos/09-areal-data.html",
    "title": "Demo 09: Visualizations for areal data",
    "section": "",
    "text": "What is a polygon? I know this is getting back to elementary school stuff, but in short, a polygon is a shape consisting of a finite number of edges to form an enclosed space. Geographic borders making up regions like countries, states, counties, etc., can be envisioned as very complex polygons.\n\n\nIn ggplot(), polygons are just another geometry, making it really easy to add geographic shapes (e.g. corresponding to countries, states, counties, etc.) to maps. The following code makes a county-level map of the US by utilizing the geom_polygon() function. This function just needs a set of latitude and longitude coordinates and the group that each of these coordinates belongs to. Each group corresponds to a polygon, and the individual latitude and longitude coordinates (many of which will belong to the same group) can be “connected together” to make a polygon. For example, for the state of Michigan, there are two groups, because it consists of two different polygons.\n\nlibrary(tidyverse)\nlibrary(ggmap)\nus_data &lt;- map_data(\"state\")\ncounty_data &lt;- map_data(\"county\")\n\n#For reference, this is what us_data looks like:\nhead(us_data)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nus_county_map &lt;- ggplot() + \n  #this creates all of the counties\n  geom_polygon(aes(long, lat, group = group), fill = \"darkblue\", size = 4, \n               data = county_data) + \n  #this draws outlines for the states\n  geom_polygon(aes(long, lat, group = group), color = 'white', \n               fill = NA, data = us_data) + \n  theme_bw() + theme(axis.text = element_blank(), \n                     axis.title = element_blank())\nus_county_map\n\n\n\n\n\n\n\n\nIn what follows, we’ll demonstrate how you can plot state-specific data (i.e., areal data, where each subject is a state). The workflow generalizes to other regions (e.g., maybe you want to plot country-specific data, county-specific data, etc.)\nThe workflow will be as follows:\n\nGet state boundaries (i.e., the latitude and longitude information at the state level).\nGet state-specific data. For example, we will use the state.x77 dataset in the datasets package, which contains information about each of the 50 United States in the 1970s. (Despite its name, none of the data is from 1977. See help(state.x77) for more details.)\nMatch the data from the the second bulletpoint to the data from the first bulletpoint (i.e., connect the state-specific data with the state boundary data).\nPlot the data.\n\nFirst, let’s get the state boundaries. (We actually did this earlier when making our US map above, but we’ll do it again here for demonstration.)\n\n#  Get state borders from ggmap package and maps\nlibrary(maps)\n\nWarning: package 'maps' was built under R version 4.2.3\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n#?map_data \nstate_borders &lt;- map_data(\"state\") \nhead(state_borders)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nNow we’ll load the state-specific dataset, state.x77:\n\nlibrary(datasets)\n#Note that the state.x77 dataset looks like this:\nhead(state.x77)\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n\nWe’ll need to do a minor amount of data manipulation to match the state.x77 dataset to the state_borders dataset. First, note that state.x77 technically doesn’t have a column with the state names; instead, its rownames correspond to the state names. So, first we’ll have to create a column with the state names. Second, the names within state_borders are all lowercase (see above), so we’ll need to take that into account with matching the two datasets as well.\nThe following code first grabs the rownames of the state.x77 table, then converts state.x77 to a tibble named state_data, and then adds that column with the state names and makes them lower case:\n\nstate_names &lt;- rownames(state.x77)\nstate_data &lt;- as_tibble(state.x77)\nstate_data &lt;- state_data |&gt;\n  mutate(state = state_names) |&gt;\n  mutate(state = tolower(state))\n\nNow we can match our two datasets using left_join():\n\n#  join state_data data to state_borders\nstate_borders &lt;- state_borders |&gt;\n  left_join(state_data, by = c(\"region\" = \"state\"))\n#Note that we now have the information from state_data inside\n#our state_borders dataset:\nhead(state_borders)\n\n       long      lat group order  region subregion Population Income Illiteracy\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;       3615   3624        2.1\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;       3615   3624        2.1\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;       3615   3624        2.1\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;       3615   3624        2.1\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;       3615   3624        2.1\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;       3615   3624        2.1\n  Life Exp Murder HS Grad Frost  Area\n1    69.05   15.1    41.3    20 50708\n2    69.05   15.1    41.3    20 50708\n3    69.05   15.1    41.3    20 50708\n4    69.05   15.1    41.3    20 50708\n5    69.05   15.1    41.3    20 50708\n6    69.05   15.1    41.3    20 50708\n\n\nFinally, we can make a plot of the state-specific data (in this case we focus on the illteracy rate by state in 1970). To do this, we just specify a fill within the geom_polygon() function. It’s also helpful to use the scale_fill_gradient2() function to denote what the colors should be for this fill (below, we set the midpoint within this function equal to the median of the variable we are plotting, which is common practice).\nImportant: If you do this (set the midpoint equal to the median), remember that you are forcing half the regions in your map to have “high value” colors and half the regions in your map to have “low value” colors. Furthermore, when looking at maps like the ones below, you should always keep the scale in mind. For example, there seems to be a distinction between the southern and northern US: They tend to be different colors. Ultimately, though, this difference corresponds to a 1-2% difference in illiteracy rates; we would need to (ironically) read up on illiteracy rates to better understand if this is a scientifically meaningful difference.\n\n#  Make the plot!  \n# Before running the following code, you need to have the `mapproj` package\n# install.packages(\"mapproj\")\n#  (Change the fill variable below as you see fit)\n#  (Change the color gradient as you see fit)\nggplot(state_borders) + \n  geom_polygon(aes(x = long, y = lat, group = group,\n                   fill = Illiteracy), color = \"black\") + \n  scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                       high = \"darkorchid4\", midpoint = 0.95) +\n  theme_void() +\n  coord_map(\"polyconic\") + \n  labs(\n    title = \"Spatial Distribution of Illiteracy Percent by State\",\n    subtitle = \"Percent of Population in State\",\n    caption = \"U.S. Department of Commerce, Bureau of the Census (1977)\",\n    fill = \"Illiteracy %\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nIn the above code, note that we have the line coord_map(\"polyconic\"). This specifies a certain kind of map projection for our plot. We can consider other projections. For example, an old-school projection we discussed in lecture was the Mercator projection. The below map is the same map as above, but with the Mercator projection.\n\nggplot(state_borders) + \n  geom_polygon(aes(x = long, y = lat, group = group,\n                   fill = Illiteracy), color = \"black\") + \n  scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                       high = \"darkorchid4\", midpoint = 0.95) +\n  theme_void() +\n  coord_map(\"mercator\") + \n  labs(\n    title = \"Spatial Distribution of Illiteracy Percent by State\",\n    subtitle = \"Percent of Population in State\",\n    caption = \"U.S. Department of Commerce, Bureau of the Census (1977)\",\n    fill = \"Illiteracy %\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nNote that the coord_map() function is specifying the coordinates of our map.\nUnfortunately, some popular projections (like the Robinson projection and the Winkel Tripel projection) are not readily available. It’s a little bit annoying to get these projections working in R, but it can be done: See this tutorial. There isn’t one “right” projection, but some are better than others."
  },
  {
    "objectID": "demos/09-areal-data.html#plotting-spatial-objects-with-geom_polygon",
    "href": "demos/09-areal-data.html#plotting-spatial-objects-with-geom_polygon",
    "title": "Demo 09: Visualizations for areal data",
    "section": "",
    "text": "In ggplot(), polygons are just another geometry, making it really easy to add geographic shapes (e.g. corresponding to countries, states, counties, etc.) to maps. The following code makes a county-level map of the US by utilizing the geom_polygon() function. This function just needs a set of latitude and longitude coordinates and the group that each of these coordinates belongs to. Each group corresponds to a polygon, and the individual latitude and longitude coordinates (many of which will belong to the same group) can be “connected together” to make a polygon. For example, for the state of Michigan, there are two groups, because it consists of two different polygons.\n\nlibrary(tidyverse)\nlibrary(ggmap)\nus_data &lt;- map_data(\"state\")\ncounty_data &lt;- map_data(\"county\")\n\n#For reference, this is what us_data looks like:\nhead(us_data)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nus_county_map &lt;- ggplot() + \n  #this creates all of the counties\n  geom_polygon(aes(long, lat, group = group), fill = \"darkblue\", size = 4, \n               data = county_data) + \n  #this draws outlines for the states\n  geom_polygon(aes(long, lat, group = group), color = 'white', \n               fill = NA, data = us_data) + \n  theme_bw() + theme(axis.text = element_blank(), \n                     axis.title = element_blank())\nus_county_map\n\n\n\n\n\n\n\n\nIn what follows, we’ll demonstrate how you can plot state-specific data (i.e., areal data, where each subject is a state). The workflow generalizes to other regions (e.g., maybe you want to plot country-specific data, county-specific data, etc.)\nThe workflow will be as follows:\n\nGet state boundaries (i.e., the latitude and longitude information at the state level).\nGet state-specific data. For example, we will use the state.x77 dataset in the datasets package, which contains information about each of the 50 United States in the 1970s. (Despite its name, none of the data is from 1977. See help(state.x77) for more details.)\nMatch the data from the the second bulletpoint to the data from the first bulletpoint (i.e., connect the state-specific data with the state boundary data).\nPlot the data.\n\nFirst, let’s get the state boundaries. (We actually did this earlier when making our US map above, but we’ll do it again here for demonstration.)\n\n#  Get state borders from ggmap package and maps\nlibrary(maps)\n\nWarning: package 'maps' was built under R version 4.2.3\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n#?map_data \nstate_borders &lt;- map_data(\"state\") \nhead(state_borders)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nNow we’ll load the state-specific dataset, state.x77:\n\nlibrary(datasets)\n#Note that the state.x77 dataset looks like this:\nhead(state.x77)\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n\nWe’ll need to do a minor amount of data manipulation to match the state.x77 dataset to the state_borders dataset. First, note that state.x77 technically doesn’t have a column with the state names; instead, its rownames correspond to the state names. So, first we’ll have to create a column with the state names. Second, the names within state_borders are all lowercase (see above), so we’ll need to take that into account with matching the two datasets as well.\nThe following code first grabs the rownames of the state.x77 table, then converts state.x77 to a tibble named state_data, and then adds that column with the state names and makes them lower case:\n\nstate_names &lt;- rownames(state.x77)\nstate_data &lt;- as_tibble(state.x77)\nstate_data &lt;- state_data |&gt;\n  mutate(state = state_names) |&gt;\n  mutate(state = tolower(state))\n\nNow we can match our two datasets using left_join():\n\n#  join state_data data to state_borders\nstate_borders &lt;- state_borders |&gt;\n  left_join(state_data, by = c(\"region\" = \"state\"))\n#Note that we now have the information from state_data inside\n#our state_borders dataset:\nhead(state_borders)\n\n       long      lat group order  region subregion Population Income Illiteracy\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;       3615   3624        2.1\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;       3615   3624        2.1\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;       3615   3624        2.1\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;       3615   3624        2.1\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;       3615   3624        2.1\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;       3615   3624        2.1\n  Life Exp Murder HS Grad Frost  Area\n1    69.05   15.1    41.3    20 50708\n2    69.05   15.1    41.3    20 50708\n3    69.05   15.1    41.3    20 50708\n4    69.05   15.1    41.3    20 50708\n5    69.05   15.1    41.3    20 50708\n6    69.05   15.1    41.3    20 50708\n\n\nFinally, we can make a plot of the state-specific data (in this case we focus on the illteracy rate by state in 1970). To do this, we just specify a fill within the geom_polygon() function. It’s also helpful to use the scale_fill_gradient2() function to denote what the colors should be for this fill (below, we set the midpoint within this function equal to the median of the variable we are plotting, which is common practice).\nImportant: If you do this (set the midpoint equal to the median), remember that you are forcing half the regions in your map to have “high value” colors and half the regions in your map to have “low value” colors. Furthermore, when looking at maps like the ones below, you should always keep the scale in mind. For example, there seems to be a distinction between the southern and northern US: They tend to be different colors. Ultimately, though, this difference corresponds to a 1-2% difference in illiteracy rates; we would need to (ironically) read up on illiteracy rates to better understand if this is a scientifically meaningful difference.\n\n#  Make the plot!  \n# Before running the following code, you need to have the `mapproj` package\n# install.packages(\"mapproj\")\n#  (Change the fill variable below as you see fit)\n#  (Change the color gradient as you see fit)\nggplot(state_borders) + \n  geom_polygon(aes(x = long, y = lat, group = group,\n                   fill = Illiteracy), color = \"black\") + \n  scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                       high = \"darkorchid4\", midpoint = 0.95) +\n  theme_void() +\n  coord_map(\"polyconic\") + \n  labs(\n    title = \"Spatial Distribution of Illiteracy Percent by State\",\n    subtitle = \"Percent of Population in State\",\n    caption = \"U.S. Department of Commerce, Bureau of the Census (1977)\",\n    fill = \"Illiteracy %\"\n  ) + \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "demos/09-areal-data.html#use-coord_map-to-specify-your-map-projection",
    "href": "demos/09-areal-data.html#use-coord_map-to-specify-your-map-projection",
    "title": "Demo 09: Visualizations for areal data",
    "section": "",
    "text": "In the above code, note that we have the line coord_map(\"polyconic\"). This specifies a certain kind of map projection for our plot. We can consider other projections. For example, an old-school projection we discussed in lecture was the Mercator projection. The below map is the same map as above, but with the Mercator projection.\n\nggplot(state_borders) + \n  geom_polygon(aes(x = long, y = lat, group = group,\n                   fill = Illiteracy), color = \"black\") + \n  scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                       high = \"darkorchid4\", midpoint = 0.95) +\n  theme_void() +\n  coord_map(\"mercator\") + \n  labs(\n    title = \"Spatial Distribution of Illiteracy Percent by State\",\n    subtitle = \"Percent of Population in State\",\n    caption = \"U.S. Department of Commerce, Bureau of the Census (1977)\",\n    fill = \"Illiteracy %\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nNote that the coord_map() function is specifying the coordinates of our map.\nUnfortunately, some popular projections (like the Robinson projection and the Winkel Tripel projection) are not readily available. It’s a little bit annoying to get these projections working in R, but it can be done: See this tutorial. There isn’t one “right” projection, but some are better than others."
  },
  {
    "objectID": "demos/09-areal-data.html#visual-randomization-tests",
    "href": "demos/09-areal-data.html#visual-randomization-tests",
    "title": "Demo 09: Visualizations for areal data",
    "section": "Visual Randomization Tests",
    "text": "Visual Randomization Tests\nIn the above graph, we want to assess if illiteracy rates tend to depend on geography. So, consider the null hypothesis that illiteracy rates do not depend on geography. If the null hypothesis is true, what are the chances that our map just “happened” to look like the map above?\nTo answer this question, we will use a visual randomization test. The workflow for a visual randomization test is as follows:\n\nMake an areal graph of outcomes (as we did above for illiteracy rates).\nShuffle the outcomes randomly a few times (e.g., 8 times). Make a new areal graph for each of these shuffles. (In short, you’re taking the colors of your original graph and just shuffling them around the different geographic regions on the map.)\nPlot your original map along with all of the “shuffled” graphs, and show your graphs to someone else. Tell them that one graph is the “real graph” and the rest are “random graphs.” Can they tell which graph is the real graph? If so, then the graph we have is “significantly non-random” in terms of geography.\n\nBelow is the code to implement this workflow for the illiteracy example. We already completed the first bulletpoint, so now we just need to do the last two bulletpoints (below).\n\n# It'll be helpful to write a function that automatically makes a ggplot for us.\n# This is literally the same code we used to generate the original areal map above, but for any dataset called \"state_data\".\nget_state_map_illit &lt;- function(state_data){\n  plot &lt;- ggplot(state_data) + \n    geom_polygon(aes(x = long, y = lat, group = group,\n                     fill = Illiteracy), color = \"black\") +\n    scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                         high = \"darkorchid4\", midpoint = 0.95) +\n    theme_void() +\n    coord_map(\"polyconic\")\n  return(plot)\n}\n\n# Now we're going to permute (i.e., \"shuffle\") the outcomes a few times. \n# number of randomizations/permutations/shuffles:\nn_shuffles &lt;- 9\n\n# It's helpful to store ggplot objects in lists in R.\n# We haven't talked much about lists in this class, but they\n# are quite flexible and easy to use.\n# For example, we're going to create an object called plot_list.\n# plot_list[[1]] refers to the first object in the list,\n# plot_list[[2]] refers to the second object in the list,\n# and so on.\nplot_list &lt;- list(length = n_shuffles)\n# Will use a for loop to do this\nfor(i in 1:n_shuffles){\n  #create a \"randomized\" dataset\n  state_borders_rand &lt;- state_borders\n  #shuffle the outcomes\n  state_borders_rand$Illiteracy &lt;- sample(state_borders_rand$Illiteracy)\n  #create the plot and store it\n  plot_list[[i]] = get_state_map_illit(state_borders_rand)\n}\n# Could have also do the following for those that don't like for loops... (even\n# though this is still a for loop but calling compiled code underneath)\n# plot_list &lt;- lapply(1:nshuffles, \n#                     function(i) {\n#                       state_borders_rand &lt;- state_borders\n#                       # shuffle the outcomes\n#                       state_borders_rand$Illiteracy &lt;- sample(state_borders_rand$Illiteracy)\n#                       # Return the plot \n#                       get_state_map_illit(state_borders_rand)\n#                     })\n\n\n# pick a random entry of plot_list to be the \"real\" plot\nplot_list[[sample(1:n_shuffles, size = 1)]] = get_state_map_illit(state_borders)\n\n# Plot all the plots together using the cowplot package:\n# install.packages(\"cowplot\")\nlibrary(cowplot)\n\nWarning: package 'cowplot' was built under R version 4.2.3\n\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:ggmap':\n\n    theme_nothing\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nplot_grid(plotlist = plot_list, ncol = 3)\n\n\n\n\n\n\n\n# library(gridExtra)\n# grid.arrange(plot_list[[1]], plot_list[[2]], plot_list[[3]],\n#              plot_list[[4]], plot_list[[5]], plot_list[[6]],\n#              plot_list[[7]], plot_list[[8]], plot_list[[9]],\n#              nrow = 3)\n\nBecause it’s really annoying to have all of the legends displayed together, we can use the cowplot package to display a single legend below each of these maps. First, we grab a legend using the get_legend() function:\n\n# Grab the legend for just the first plot, since they are all the same\nmap_legend &lt;- get_legend(plot_list[[1]])\n\nWarning in get_plot_component(plot, \"guide-box\"): Multiple components found;\nreturning the first one. To return all, use `return_all = TRUE`.\n\n\nNext, we are going to update our plot_list so that the legends are removed from each of them. We can do this quickly using the lapply() function:\n\nlegend_free_plot_list &lt;- \n  lapply(1:length(plot_list),\n         function(i) plot_list[[i]] + theme(legend.position = \"none\"))\n\nAnd finally, we will now use multiple plot_grid function calls to display the shuffled maps next to the legend:\n\nplot_grid(\n  plot_grid(plotlist = legend_free_plot_list, ncol = 3),\n  map_legend, ncol = 2,\n  # Adjust so the maps are much larger:\n  rel_widths = c(4, 1)\n)\n\n\n\n\n\n\n\n\nIf you can spot which plot is the real plot (without having seen it previously!!), then illiteracy rates are significantly non-random across geography."
  },
  {
    "objectID": "demos/05-pca.html",
    "href": "demos/05-pca.html",
    "title": "Demo 05: Principal Component Analysis",
    "section": "",
    "text": "Principal Components of Starbucks\nThroughout this demo we will again use the dataset about Starbucks drinks available in the #TidyTuesday project.\nYou can read in and manipulate various columns in the dataset with the following code:\n\nlibrary(tidyverse)\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))\n\nWe will apply principal component analysis (PCA) to the quantitative variables in this dataset:\n\n# Select the variables of interest:\nstarbucks_quant_data &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg)\ndim(starbucks_quant_data)\n\n[1] 1147   11\n\n\nAs seen above, there are 11 quantitative variables in the dataset, and it’s difficult to visualize 11 quantitative variables simultaneously. Maybe we can “get away” with just plotting two dimensions that make up the majority of the variation among these 11 variables (i.e., the first two principal components).\nTo conduct PCA, you must center and standardize your variables. We can either do that manually with the scale() function:\n\nscaled_starbucks_quant_data &lt;- scale(starbucks_quant_data)\n\nOr we can tell R do that for us before performing PCA using the prcomp() function:\n\n# perform PCA\nstarbucks_pca &lt;- prcomp(starbucks_quant_data, \n                        # Center and scale variables:\n                        center = TRUE, scale. = TRUE)\n# This is equivalent to the following commented out code:\n# starbucks_pca &lt;- prcomp(scaled_starbucks_quant_data, \n#                         center = FALSE, scale. = FALSE)\n# View the summary\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n\n\nThere are 11 variables in this dataset, and thus there are 11 principal components. However, we can see that the first principal component accounts for over half of the variation in the dataset(!), while the second accounts for about 15% of the variation. As we can see, the variation accounted by each component adds up to the total variation in the data (i.e., the “cumulative proportion” equals 100% in the PC11 column). Also, in the first row, we can see that \\(\\text{Var}(Z_1) &gt; \\text{Var}(Z_2) &gt; \\cdots &gt; \\text{Var}(Z_{11})\\), as expected given what we talked about in lecture.\nWe haven’t actually computed the principal components \\(Z_1,\\dots,Z_{11}\\) yet. In brief, PCA provides a \\(p \\times p\\) “rotation matrix,” and the matrix \\(\\boldsymbol{Z} = (Z_1,\\dots,Z_{11})\\) is equal to the original data matrix \\(X\\) times the rotation matrix. The prcomp() function returns us the result of this matrix multiplication: the matrix of the principal component scores \\(\\boldsymbol{Z} = (Z_1,\\dots,Z_{11})\\) which can be accessed in the following way:\n\nstarbucks_pc_matrix &lt;- starbucks_pca$x\nhead(starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nWe could have manually computed this using the returned rotation matrix and the original data (but centered and scaled). You perform matrix multiplication in R using the %*% operator:\n\nmanual_starbucks_pc_matrix &lt;- \n  as.matrix(scaled_starbucks_quant_data) %*% starbucks_pca$rotation\nhead(manual_starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nAs you can see from just the first so many rows, these matrices match. If we view the dimensionality of this matrix (just the one returned to us by prcomp), we can seee that it matches the dimensionality of the original dataset:\n\ndim(starbucks_pc_matrix)\n\n[1] 1147   11\n\n\nIndeed, it is literally an 11-dimensional rotation of our dataset. However, the first column of this matrix accounts for over half of the variation in the data and the second column accounts for over 15% of the variation, so maybe we can “get away” with plotting just those first two dimensions.\nTo recreate what the summary output of prcomp function gave us above, the following line of code computes the standard deviation of each \\(Z\\) (the numbers match what’s given in the first row of numbers above):\n\napply(starbucks_pc_matrix, MARGIN = 2, FUN = sd)\n\n       PC1        PC2        PC3        PC4        PC5        PC6        PC7 \n2.47478380 1.30742010 1.05712064 0.97918632 0.67836258 0.56399067 0.44130936 \n       PC8        PC9       PC10       PC11 \n0.28122634 0.16874262 0.08701525 0.04048139 \n\n\nThis corresponds to the singular values, i.e., \\(\\sqrt{\\lambda_j}\\). We can then compute the proportion of variance explained by each component (also displayed in the summary output) by squaring these values and dividing by the number of columns:\n\n# Note that I can just replace the sd function above with the var function\napply(starbucks_pc_matrix, MARGIN = 2, FUN = var) / \n  ncol(starbucks_pc_matrix)\n\n         PC1          PC2          PC3          PC4          PC5          PC6 \n0.5567777142 0.1553952108 0.1015912775 0.0871641674 0.0418341630 0.0289168610 \n         PC7          PC8          PC9         PC10         PC11 \n0.0177049043 0.0071898413 0.0025885519 0.0006883321 0.0001489766 \n\n\nThe plot below displays the first two PCs \\(Z_1\\) and \\(Z_2\\):\n\n# First add these columns to the original dataset:\nstarbucks &lt;- starbucks |&gt;\n  mutate(pc1 = starbucks_pc_matrix[,1], \n         pc2 = starbucks_pc_matrix[,2])\nstarbucks |&gt;\n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.25) +\n  labs(x = \"PC 1\", y = \"PC 2\")\n\n\n\n\n\n\n\n\nThis matches what we saw returned by MDS!\n\n\nOne Biplot to Rule Them All\nHowever, the components by themselves aren’t very interpretable - how do they relate to original variables? At this point, it’s important to remember that the principal components are linear combinations of the original variables. So, there is a (deterministic) linear relationship between the original variables and the principal components that we are plotting here.\nUsing the popular R package factoextra, we can plot these linear relationships on top of the scatterplot. We can do so using what’s called a biplot, which is essentially just a fancy expression for “scatterplots with arrows on top”. After installing the factoextra package, we can create the biplot using the fviz_pca_biplot() function on the prcomp output directly (but with the observation labels turned off!):\n\n# install.packages(\"factoextra\")\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n# Designate to only label the variables:\nfviz_pca_biplot(starbucks_pca, label = \"var\", \n                # Change the alpha for the observations - \n                # which is represented by ind\n                alpha.ind = .25,\n                # Modify the alpha for the variables (var):\n                alpha.var = .75,\n                # Modify the color of the variables\n                col.var = \"darkblue\")\n\n\n\n\n\n\n\n\nThe above plot tells us a lot of information:\n\nThe direction of a particular arrow is indicative of “as this variable increases….” For example, the far left arrow for caffeine_mg suggests that, as caffeine_mg increases, \\(Z_1\\) and \\(Z_2\\) tend to decrease (in other words, within the definition of \\(Z_1\\) and \\(Z_2\\), the coefficient for caffeine_mg is negative; this is verified below). You can contrast this with serv_size_m_l which is pointing to the upper right, indicating that as serv_size_m_l increases then both \\(Z_1\\) and \\(Z_2\\) tend to increase.\nThe angle of the different vectors is also indicative of the correlation between different variables. If two vectors are at a right angle (90 degrees), that suggests that they are uncorrelated, e.g., serv_size_m_l and saturated_fag_g. If two vectors are in similar directions (i.e., their angle is less than 90 degrees), that suggests that they are positively correlated, e.g., sugar_g and total_carbs_g. If two vectors are in different directions (i.e., their angle is greater than 90 degrees), that suggests that they are negatively correlated, e.g., caffeine_mg and calories.\nThe length of the lines also indicate how strongly related the principal components are with the individual variables. For example, serv_size_m_l has a fairly long line because it has a large positive coefficient for \\(Z_1\\) in the rotation matrix (see below). Meanwhile, caffeine_mg has a relatively short arrow because its coefficients are relatively small.\n\nFor reference, the below code shows the rotation matrix we used to create the \\(Z\\)s. You’ll see that the directions of the vectors in the above plot are the first two columns of this matrix.\n\nstarbucks_pca$rotation\n\n                        PC1         PC2         PC3          PC4         PC5\nserv_size_m_l    0.20078297  0.44103545  0.35053466 -0.117331692 -0.71633828\ncalories         0.39488151  0.10314156 -0.01048587  0.055814030  0.11487225\ntotal_fat_g      0.35254969 -0.31687231  0.06598414  0.046196797  0.07677253\nsaturated_fat_g  0.33929914 -0.30565133  0.05310592 -0.003731227  0.16145662\ntrans_fat_g      0.29974182 -0.39855899  0.01855869 -0.092804122 -0.35695525\ncholesterol_mg   0.33049434 -0.37077805  0.01219867 -0.105617624 -0.18815364\nsodium_mg        0.33573598  0.24647412 -0.09107538 -0.083512068  0.34969486\ntotal_carbs_g    0.34858318  0.34483762 -0.09623296  0.002842153  0.12386718\nfiber_g          0.11351848  0.04137855  0.17814948  0.956078124 -0.04719036\nsugar_g          0.34234584  0.35100839 -0.13314389 -0.109371714  0.12108189\ncaffeine_mg     -0.03085327 -0.01056235  0.89572768 -0.167846419  0.35265479\n                        PC6         PC7         PC8         PC9         PC10\nserv_size_m_l    0.30806678  0.13668394  0.04039275  0.01194522 -0.001076764\ncalories        -0.01331210 -0.18521073  0.09836135 -0.45551398  0.744248239\ntotal_fat_g      0.37698224 -0.03833030  0.03871096 -0.58859673 -0.518643989\nsaturated_fat_g  0.57285718 -0.06553378 -0.26369346  0.56257742  0.209355859\ntrans_fat_g     -0.50043224  0.15197176 -0.58086994 -0.05398876  0.032105721\ncholesterol_mg  -0.26449384 -0.04594580  0.74615325  0.27703165 -0.032124871\nsodium_mg       -0.06228905  0.82317144  0.06292570  0.04230447 -0.037304757\ntotal_carbs_g   -0.17619489 -0.34490217 -0.08588651  0.12501079 -0.148886253\nfiber_g         -0.11365528  0.06192955  0.01207815  0.10654914 -0.061378250\nsugar_g         -0.16729497 -0.33345131 -0.10758116  0.14408095 -0.321644156\ncaffeine_mg     -0.19600402 -0.06671121 -0.02122274  0.01530108 -0.020691492\n                         PC11\nserv_size_m_l    0.0053899973\ncalories        -0.1070327163\ntotal_fat_g      0.0489644534\nsaturated_fat_g -0.0152794817\ntrans_fat_g      0.0069417249\ncholesterol_mg  -0.0004710159\nsodium_mg        0.0185545403\ntotal_carbs_g    0.7347049650\nfiber_g         -0.0730283725\nsugar_g         -0.6635335478\ncaffeine_mg     -0.0094861578\n\n\nIn the above example, we plotted the first two principal components; thus, implicitly, we have chosen \\(k = 2\\), the only reason being that it is easy to visualize. However, how many principal components should we actually be using?\n\n\nCreating and Interpreting Scree Plots\nThere is a common visual used to answer this question, but first let’s build some intuition. We already know that \\(Z_1\\) accounts for the most variation in our data, \\(Z_2\\) accounts for the next most, and so on. Thus, each time we add a new principal component dimension, we capture a “higher proportion of the information in the data,” but that increase in proportion decreases for each new dimension we add. (You may have to read those last two sentences a few times to get what I mean.) Thus, in practice, it is recommended to keep adding principal components until the marginal gain “levels off,” i.e., decreases to the point that it isn’t too beneficial to add another dimension to the data.\nThis trade-off between dimensions and marginal gain in information is often inspected visually using a scree plot, or what is more commonly known as an elbow plot. In an elbow plot, the x-axis has the numbers \\(1,2,\\dots,p\\) (i.e., the dimensions in the data), and the y-axis has the proportion of variation that the particular principal component \\(Z_j\\) accounts for. We can construct the scree plot using the fviz_screeplot() function from `factoextra\n\nfviz_eig(starbucks_pca, addlabels = TRUE) # Add the labels \n\n\n\n\n\n\n\n\nThe graphical rule-of-thumb is to then look for the “elbow,” i.e., where the proportion of variation starts to become flat. Unfortunately there is not a definitive “this is the elbow for sure” rule, and it is up to your judgment. Another useful rule-of-thumb is to consider drawing a horizontal line at 1 divided by the number of variables in your original dataset. Why do you think that is a useful rule? We easily do this because factoextra generates ggplot objects, so we can add another geometric layer corresponding to our reference:\n\nfviz_eig(starbucks_pca, addlabels = TRUE) +\n  # Have to multiply by 100 to get on percent scale\n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_quant_data)),\n             linetype = \"dashed\", color = \"darkred\")\n\n\n\n\n\n\n\n\nBased on this plot, I think there’s a strong argument to stop at \\(k = 3\\) (but maybe go up to \\(k = 5\\) for another substantial drop in the elbow).\nLet’s say we decide \\(k = 3\\). This means that we should use the first three principal components in our graphics and other analyses in order for a “satisfactory” amount of the variation in the data to be captured. Our above visual only plots the first two principal components, and so we are “hiding” some data information that we are better off plotting in some way if possible (specifically, we are hiding about 30% of the information, i.e., the total amount of information captured by principal components 3, 4, …, 11, and about a third of this remaining information is captured by that third component that we are not plotting). This means that, theoretically, we should plot three quantitative variables, and we’ve discussed a bit about how to do this - you could use the size of points, transparency, or even a 3D scatterplot if you wanted to - but we are not going to explore that further here. Alternatively, you could just make three scatterplots (one for each pair of principal components).\nIf you’re having issues with the factoextra package then you can easily remake the scree plot manually. All we need to do is grab the proportion of variance explained by each component, turn it into a table, and then display it in some way. We already computed these values earlier in the demo, but we also just grab the singular values directly provided to us by R:\n\n# Manual creation of elbow plot, start by computing the eigenvalues and dividing by\n# the total variance in the data:\ntibble(prop_var = (starbucks_pca$sdev)^2 / ncol(starbucks_quant_data)) |&gt;\n  # Add a column for the PC index:\n  mutate(pc_index = 1:n()) |&gt;\n  # Now make the plot!\n  ggplot(aes(x = pc_index, y = prop_var)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", alpha = 0.75) +\n  geom_point(color = \"black\", size = 2) +\n  geom_line() +\n  # Add the horizontal reference line:\n  geom_hline(yintercept = (1 / ncol(starbucks_quant_data)),\n             linetype = \"dashed\", color = \"darkred\") +\n  # And label:\n  labs(x = \"Dimensions\", y = \"Proportion of explained variance\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nMaking a biplot from scratch is much more difficult… Instead, you can try the ggfortify package (which is useful for making model diagnostic plots). The following code demonstrates how to do this (after you installed ggfortify):\n\n# install.packages(\"ggfortify\")\nlibrary(ggfortify)\n\nWarning: package 'ggfortify' was built under R version 4.2.3\n\nautoplot(starbucks_pca, \n         data = starbucks_quant_data,\n         alpha = 0.25,\n         loadings = TRUE, loadings.colour = 'darkblue',\n         loadings.label.colour = 'darkblue',\n         loadings.label = TRUE, loadings.label.size = 3,\n         loadings.label.repel = TRUE) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/02-categorical.html#reminders-previously-and-today",
    "href": "lectures/02-categorical.html#reminders-previously-and-today",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Reminders, previously, and today…",
    "text": "Reminders, previously, and today…\n\nHW1 was UPDATED and is due next Wednesday - complete the GenAI Literacy module ON TIME!\nComplete HW0 by Thursday night! Confirms you have everything installed and can render .qmd files to PDF via tinytex\n\n\n\nWalked through course logistics (READ THE SYLLABUS)\nIntroduced the Grammar of Graphics and ggplot2 basics\n\n\n\nTODAY:\n\nDiscuss data visualization principles and the role of infographics\nVisualizing categorical data (starting with 1D)"
  },
  {
    "objectID": "lectures/02-categorical.html#in-the-beginning",
    "href": "lectures/02-categorical.html#in-the-beginning",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "In the beginning…",
    "text": "In the beginning…\n\nMichael Florent van Langren published the first (known) statistical graphic in 1644\n\n\n\n\n\n\nPlots different estimates of the longitudinal distance between Toledo, Spain and Rome, Italy\ni.e., visualization of collected data to aid in estimation of parameter"
  },
  {
    "objectID": "lectures/02-categorical.html#john-snow-knows-something-about-cholera",
    "href": "lectures/02-categorical.html#john-snow-knows-something-about-cholera",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "John Snow Knows Something About Cholera",
    "text": "John Snow Knows Something About Cholera"
  },
  {
    "objectID": "lectures/02-categorical.html#charles-minards-map-of-napoleons-russian-disaster",
    "href": "lectures/02-categorical.html#charles-minards-map-of-napoleons-russian-disaster",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Charles Minard’s Map of Napoleon’s Russian Disaster",
    "text": "Charles Minard’s Map of Napoleon’s Russian Disaster"
  },
  {
    "objectID": "lectures/02-categorical.html#florence-nightingales-rose-diagram",
    "href": "lectures/02-categorical.html#florence-nightingales-rose-diagram",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Florence Nightingale’s Rose Diagram",
    "text": "Florence Nightingale’s Rose Diagram"
  },
  {
    "objectID": "lectures/02-categorical.html#milestones-in-data-visualization-history",
    "href": "lectures/02-categorical.html#milestones-in-data-visualization-history",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Milestones in Data Visualization History",
    "text": "Milestones in Data Visualization History"
  },
  {
    "objectID": "lectures/02-categorical.html#edward-tuftes-principles-of-data-visualization",
    "href": "lectures/02-categorical.html#edward-tuftes-principles-of-data-visualization",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Edward Tufte’s Principles of Data Visualization",
    "text": "Edward Tufte’s Principles of Data Visualization\nGraphics: visually display measured quantities by combining points, lines, coordinate systems, numbers, symbols, words, shading, color\nOften our goal is to show data and/or communicate a story\n\n\nInduce viewer to think about substance, not graphical methodology\nMake large, complex datasets more coherent\nEncourage comparison of different pieces of data\nDescribe, explore, and identify relationships\nAvoid data distortion and data decoration\nUse consistent graph design\n\n\n\nAvoid graphs that lead to misleading conclusions!"
  },
  {
    "objectID": "lectures/02-categorical.html#how-to-fail-this-class",
    "href": "lectures/02-categorical.html#how-to-fail-this-class",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "How to Fail this Class:",
    "text": "How to Fail this Class:"
  },
  {
    "objectID": "lectures/02-categorical.html#what-about-this-spiral",
    "href": "lectures/02-categorical.html#what-about-this-spiral",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "What about this spiral?",
    "text": "What about this spiral?\n\n\nRequires distortion"
  },
  {
    "objectID": "lectures/02-categorical.html#infographics-to-communicate-a-story-check-out-flowingdata-for-more-examples",
    "href": "lectures/02-categorical.html#infographics-to-communicate-a-story-check-out-flowingdata-for-more-examples",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Infographics to communicate a story (check out FlowingData for more examples)",
    "text": "Infographics to communicate a story (check out FlowingData for more examples)"
  },
  {
    "objectID": "lectures/02-categorical.html#alberto-cairo-and-the-art-of-insight",
    "href": "lectures/02-categorical.html#alberto-cairo-and-the-art-of-insight",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Alberto Cairo and the art of insight",
    "text": "Alberto Cairo and the art of insight"
  },
  {
    "objectID": "lectures/02-categorical.html#d-categorical-data",
    "href": "lectures/02-categorical.html#d-categorical-data",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "1D Categorical Data",
    "text": "1D Categorical Data\nTwo different versions of categorical:\n\nNominal: coded with arbitrary numbers, i.e., no real order\n\n\nExamples: race, gender, species, text\n\n\n\nOrdinal: levels with a meaningful order\n\n\nExamples: education level, grades, ranks\n\n\n\nNOTE: R and ggplot considers a categorical variable to be factor\n\nR will always treat categorical variables as ordinal! Defaults to alphabetical…\nWe will need to manually define the factor levels"
  },
  {
    "objectID": "lectures/02-categorical.html#d-categorical-data-structure",
    "href": "lectures/02-categorical.html#d-categorical-data-structure",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "1D categorical data structure",
    "text": "1D categorical data structure\n\nObservations are collected into a vector \\((x_1, \\dots, x_n)\\), where \\(n\\) is number of observations\nEach observed value \\(x_i\\) can only belong to one category level \\(\\{ C_1, C_2, \\dots \\}\\)\n\n\nLook at penguins data from the palmerpenguins package, focusing on species:\n\nlibrary(palmerpenguins)\nhead(penguins$species)\n\n[1] Adelie Adelie Adelie Adelie Adelie Adelie\nLevels: Adelie Chinstrap Gentoo\n\n\nHow could we summarize these data? What information would you report?\n\n\n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124"
  },
  {
    "objectID": "lectures/02-categorical.html#area-plots",
    "href": "lectures/02-categorical.html#area-plots",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Area plots",
    "text": "Area plots\n\n\nEach area corresponds to one categorical level\nArea is proportional to counts/frequencies/percentages\nDifferences between areas correspond to differences between counts/frequencies/percentages"
  },
  {
    "objectID": "lectures/02-categorical.html#bar-charts",
    "href": "lectures/02-categorical.html#bar-charts",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Bar charts",
    "text": "Bar charts\n\nlibrary(tidyverse)\npenguins |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/02-categorical.html#behind-the-scenes-statistical-summaries",
    "href": "lectures/02-categorical.html#behind-the-scenes-statistical-summaries",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Behind the scenes: statistical summaries",
    "text": "Behind the scenes: statistical summaries\n\nFrom Chapter 3 of R for Data Science"
  },
  {
    "objectID": "lectures/02-categorical.html#spine-charts---height-version",
    "href": "lectures/02-categorical.html#spine-charts---height-version",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Spine charts - height version",
    "text": "Spine charts - height version\n\npenguins |&gt;\n  ggplot(aes(fill = species, x = \"\")) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/02-categorical.html#spine-charts---width-version",
    "href": "lectures/02-categorical.html#spine-charts---width-version",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Spine charts - width version",
    "text": "Spine charts - width version\n\npenguins |&gt;\n  ggplot(aes(fill = species, x = \"\")) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/02-categorical.html#what-does-a-bar-chart-show",
    "href": "lectures/02-categorical.html#what-does-a-bar-chart-show",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "What does a bar chart show?",
    "text": "What does a bar chart show?\nMarginal Distribution\n\nAssume categorical variable \\(X\\) has \\(K\\) categories: \\(C_1, \\dots, C_K\\)\nTrue marginal distribution of \\(X\\):\n\n\\[\nP(X = C_j) = p_j,\\ j \\in \\{ 1, \\dots, K \\}\n\\]\n\nWe have access to the Empirical Marginal Distribution\n\nObserved distribution of \\(X\\), our best estimate (MLE) of the marginal distribution of \\(X\\): \\(\\hat{p}_1\\), \\(\\hat{p}_2\\), \\(\\dots\\), \\(\\hat{p}_K\\)\n\n\ntable(penguins$species) / nrow(penguins)\n\n\n   Adelie Chinstrap    Gentoo \n0.4418605 0.1976744 0.3604651"
  },
  {
    "objectID": "lectures/02-categorical.html#bar-charts-with-proportions",
    "href": "lectures/02-categorical.html#bar-charts-with-proportions",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Bar charts with proportions",
    "text": "Bar charts with proportions\n\nafter_stat() indicates the aesthetic mapping is performed after statistical transformation\nUse after_stat(count) to access the stat_count() called by geom_bar()\n\n\npenguins |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = after_stat(count) / sum(after_stat(count)))) + \n  labs(y = \"Proportion\")"
  },
  {
    "objectID": "lectures/02-categorical.html#compute-and-display-the-proportions-directly",
    "href": "lectures/02-categorical.html#compute-and-display-the-proportions-directly",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Compute and display the proportions directly",
    "text": "Compute and display the proportions directly\n\nUse group_by(), summarize(), and mutate() in a pipeline to compute then display the proportions directly\nNeed to indicate we are displaying the y axis as given, i.e., the identity function\n\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total) |&gt; \n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\")"
  },
  {
    "objectID": "lectures/02-categorical.html#compute-and-display-the-proportions-directly-output",
    "href": "lectures/02-categorical.html#compute-and-display-the-proportions-directly-output",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Compute and display the proportions directly",
    "text": "Compute and display the proportions directly"
  },
  {
    "objectID": "lectures/02-categorical.html#what-about-uncertainty",
    "href": "lectures/02-categorical.html#what-about-uncertainty",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "What about uncertainty?",
    "text": "What about uncertainty?\n\nQuantify uncertainty for our estimate \\(\\hat{p}_j = \\frac{n_j}{n}\\) with the standard error:\n\n\\[\nSE(\\hat{p}_j) = \\sqrt{\\frac{\\hat{p}_j(1 - \\hat{p}_j)}{n}}\n\\]\n\n\nCompute \\(\\alpha\\)-level confidence interval (CI) as \\(\\hat{p}_j \\pm z_{1 - \\alpha / 2} \\cdot SE(\\hat{p}_j)\\)\nGood rule-of-thumb: construct 95% CI using \\(\\hat{p}_j \\pm 2 \\cdot SE(\\hat{p}_j)\\)\nApproximation justified by CLT, so CI could include values outside of [0,1]"
  },
  {
    "objectID": "lectures/02-categorical.html#add-standard-errors-to-bars",
    "href": "lectures/02-categorical.html#add-standard-errors-to-bars",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Add standard errors to bars",
    "text": "Add standard errors to bars\n\nNeed to remember each CI is for each \\(\\hat{p}_j\\) marginally, not jointly\nHave to be careful with multiple testing\n\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total,\n         se = sqrt(prop * (1 - prop) / total), \n         lower = prop - 2 * se, \n         upper = prop + 2 * se) |&gt; \n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                color = \"red\")"
  },
  {
    "objectID": "lectures/02-categorical.html#add-standard-errors-to-bars-output",
    "href": "lectures/02-categorical.html#add-standard-errors-to-bars-output",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Add standard errors to bars",
    "text": "Add standard errors to bars"
  },
  {
    "objectID": "lectures/02-categorical.html#why-does-this-matter",
    "href": "lectures/02-categorical.html#why-does-this-matter",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Why does this matter?",
    "text": "Why does this matter?"
  },
  {
    "objectID": "lectures/02-categorical.html#graphs-can-appear-the-same-with-very-different-statistical-conclusions---mainly-due-to-sample-size",
    "href": "lectures/02-categorical.html#graphs-can-appear-the-same-with-very-different-statistical-conclusions---mainly-due-to-sample-size",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Graphs can appear the same with very different statistical conclusions - mainly due to sample size",
    "text": "Graphs can appear the same with very different statistical conclusions - mainly due to sample size"
  },
  {
    "objectID": "lectures/02-categorical.html#useful-to-order-categories-by-frequency-with-forcats",
    "href": "lectures/02-categorical.html#useful-to-order-categories-by-frequency-with-forcats",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Useful to order categories by frequency with forcats",
    "text": "Useful to order categories by frequency with forcats\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total,\n         se = sqrt(prop * (1 - prop) / total), \n         lower = prop - 2 * se, \n         upper = prop + 2 * se,\n         species = fct_reorder(species, prop)) |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                color = \"red\")"
  },
  {
    "objectID": "lectures/02-categorical.html#useful-to-order-categories-by-frequency-with-forcats-output",
    "href": "lectures/02-categorical.html#useful-to-order-categories-by-frequency-with-forcats-output",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Useful to order categories by frequency with forcats",
    "text": "Useful to order categories by frequency with forcats"
  },
  {
    "objectID": "lectures/02-categorical.html#so-you-want-to-make-pie-charts",
    "href": "lectures/02-categorical.html#so-you-want-to-make-pie-charts",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "So you want to make pie charts…",
    "text": "So you want to make pie charts…\n\npenguins |&gt; \n  ggplot(aes(fill = species, x = \"\")) + \n  geom_bar(aes(y = after_stat(count))) +\n  coord_polar(theta = \"y\") +\n  theme_void()"
  },
  {
    "objectID": "lectures/02-categorical.html#friends-dont-let-friends-make-pie-charts",
    "href": "lectures/02-categorical.html#friends-dont-let-friends-make-pie-charts",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Friends Don’t Let Friends Make Pie Charts",
    "text": "Friends Don’t Let Friends Make Pie Charts"
  },
  {
    "objectID": "lectures/02-categorical.html#waffle-charts-are-cooler-anyway",
    "href": "lectures/02-categorical.html#waffle-charts-are-cooler-anyway",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Waffle charts are cooler anyway…",
    "text": "Waffle charts are cooler anyway…\n\nlibrary(waffle)\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  ggplot(aes(fill = species, values = count)) +\n  geom_waffle(n_rows = 20, color = \"white\", flip = TRUE) +\n  coord_equal() +\n  theme_void()"
  },
  {
    "objectID": "lectures/02-categorical.html#recap-and-next-steps",
    "href": "lectures/02-categorical.html#recap-and-next-steps",
    "title": "Principles and Visualizations for 1D Categorical Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nDiscussed basic principles of data visualization and walked through variety of examples\nVisualize categorical data with bars!\nDisplay uncertainty with standard errors\n\n\n\nHW1 is due next Wednesday - complete GenAI module ON TIME!\nComplete HW0 by Thursday night! Confirms you have everything installed and can render .qmd files to PDF via tinytex\n\n\n\n\nNext time: Visualizing 2D categorical and 1D quantitative data\nRecommended reading:\n\nCW Chapter 10 Visualizing proportions, CW Chapter 16.2 Visualizing the uncertainty of point estimates, CW Chapter 11 Visualizing nested proportions\n\n\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#reminders-previously-and-today",
    "href": "lectures/07-highdim-methods.html#reminders-previously-and-today",
    "title": "High-Dimensional Data",
    "section": "Reminders, previously, and today…",
    "text": "Reminders, previously, and today…\n\nHW3 is due TONIGHT!\nHW4 is due next Wednesday Sept 25th\n\n\n\nDiscussed creating pairs plots for initial inspection of several variables\nBegan thinking about ways to displays dataset structure via correlations\nUsed heatmaps and parallel coordinates plot to capture observation and variable structure\n\n\n\nTODAY:\n\nProjections based on some notion of “distance”\nIntuition: Take high-dimensional data and represent it in 2-3 dimensions, then visualize those dimensions"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#thinking-about-distance",
    "href": "lectures/07-highdim-methods.html#thinking-about-distance",
    "title": "High-Dimensional Data",
    "section": "Thinking about distance…",
    "text": "Thinking about distance…\nWhen describing visuals, we’ve implicitly “clustered” observations together\nThese types of task require characterizing the distance between observations\nThis is easy to do for 2 quantitative variables: just make a scatterplot (possibly with contours or heatmap)\nBut how do we define “distance” for high-dimensional data?\n\nLet \\(\\boldsymbol{x}_i = (x_{i1}, \\dots, x_{ip})\\) be a vector of \\(p\\) features for observation \\(i\\)\nQuestion of interest: How “far away” is \\(\\boldsymbol{x}_i\\) from \\(\\boldsymbol{x}_j\\)?\n\n\nWhen looking at a scatterplot, you’re using Euclidean distance (length of the line in \\(p\\)-dimensional space):\n\\[d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\dots + (x_{ip} - x_{jp})^2}\\]"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#distances-in-general",
    "href": "lectures/07-highdim-methods.html#distances-in-general",
    "title": "High-Dimensional Data",
    "section": "Distances in general",
    "text": "Distances in general\nThere’s a variety of different types of distance metrics: Manhattan, Mahalanobis, Cosine, Kullback-Leiber Divergence, Wasserstein, but we’re just going to focus on Euclidean distance\n\n\\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\) measures pairwise distance between two observations \\(i,j\\) and has the following properties:\n\nIdentity: \\(\\boldsymbol{x}_i = \\boldsymbol{x}_j \\iff d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = 0\\)\nNon-Negativity: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\geq 0\\)\nSymmetry: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = d(\\boldsymbol{x}_j, \\boldsymbol{x}_i)\\)\nTriange Inequality: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\leq d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) + d(\\boldsymbol{x}_k, \\boldsymbol{x}_j)\\)\n\n\n\nDistance Matrix: matrix \\(D\\) of all pairwise distances\n\n\\(D_{ij} = d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\)\nwhere \\(D_{ii} = 0\\) and \\(D_{ij} = D_{ji}\\)"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#multi-dimensional-scaling-mds",
    "href": "lectures/07-highdim-methods.html#multi-dimensional-scaling-mds",
    "title": "High-Dimensional Data",
    "section": "Multi-dimensional scaling (MDS)",
    "text": "Multi-dimensional scaling (MDS)\nGeneral approach for visualizing distance matrices\n\nPuts \\(n\\) observations in a \\(k\\)-dimensional space such that the distances are preserved as much as possible (where \\(k &lt;&lt; p\\) typically choose \\(k = 2\\))\n\n\nMDS attempts to create new point \\(\\boldsymbol{y}_i = (y_{i1}, y_{i2})\\) for each unit such that:\n\\[\\sqrt{(y_{i1} - y_{j1})^2 + (y_{i2} - y_{j2})^2} \\approx D_{ij}\\]\n\ni.e., distance in 2D MDS world is approximately equal to the actual distance\n\n\n\nThen plot the new \\(\\boldsymbol{y}\\)s on a scatterplot\n\nUse the scale() function to ensure variables are comparable\nMake a distance matrix for this dataset\nVisualize it with MDS"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#mds-example-with-starbucks-drinks",
    "href": "lectures/07-highdim-methods.html#mds-example-with-starbucks-drinks",
    "title": "High-Dimensional Data",
    "section": "MDS example with Starbucks drinks",
    "text": "MDS example with Starbucks drinks\n\nstarbucks_scaled_quant_data &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) %&gt;%\n  scale(center = FALSE, scale = apply(., 2, sd, na.rm = TRUE))\n\ndist_euc &lt;- dist(starbucks_scaled_quant_data)\n\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\nstarbucks &lt;- starbucks |&gt;\n  mutate(mds1 = starbucks_mds[,1],\n         mds2 = starbucks_mds[,2])\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#mds-example-with-starbucks-drinks-output",
    "href": "lectures/07-highdim-methods.html#mds-example-with-starbucks-drinks-output",
    "title": "High-Dimensional Data",
    "section": "MDS example with Starbucks drinks",
    "text": "MDS example with Starbucks drinks"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#view-structure-with-additional-variables",
    "href": "lectures/07-highdim-methods.html#view-structure-with-additional-variables",
    "title": "High-Dimensional Data",
    "section": "View structure with additional variables",
    "text": "View structure with additional variables"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#view-structure-with-additional-variables-1",
    "href": "lectures/07-highdim-methods.html#view-structure-with-additional-variables-1",
    "title": "High-Dimensional Data",
    "section": "View structure with additional variables",
    "text": "View structure with additional variables"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#dimension-reduction---searching-for-variance",
    "href": "lectures/07-highdim-methods.html#dimension-reduction---searching-for-variance",
    "title": "High-Dimensional Data",
    "section": "Dimension reduction - searching for variance",
    "text": "Dimension reduction - searching for variance\nGOAL: Focus on reducing dimensionality of feature space while retaining most of the information in a lower dimensional space\n\n\\(n \\times p\\) matrix \\(\\rightarrow\\) dimension reduction technique \\(\\rightarrow\\) \\(n \\times k\\) matrix\n\n\nSpecial case we just discussed: MDS\n\n\\(n \\times n\\) distance matrix \\(\\rightarrow\\) MDS \\(\\rightarrow\\) \\(n \\times k\\) matrix (usually \\(k = 2\\))\n\n\nReduce data to a distance matrix\nReduce distance matrix to \\(k = 2\\) dimensions"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#principal-component-analysis-pca",
    "href": "lectures/07-highdim-methods.html#principal-component-analysis-pca",
    "title": "High-Dimensional Data",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\\[\n\\begin{pmatrix}\n& & \\text{really} & & \\\\\n& & \\text{wide} & & \\\\\n& & \\text{matrix} & &\n\\end{pmatrix}\n\\rightarrow \\text{matrix algebra stuff} \\rightarrow\n\\begin{pmatrix}\n\\text{much}  \\\\\n\\text{thinner}  \\\\\n\\text{matrix}\n\\end{pmatrix}\n\\]\n\nStart with \\(n \\times p\\) matrix of correlated variables \\(\\rightarrow\\) \\(n \\times k\\) matrix of uncorrelated variables\n\n\n\nEach of the \\(k\\) columns in the right-hand matrix are principal components, all uncorrelated with each other\nFirst column accounts for most variation in the data, second column for second-most variation, and so on\n\nIntuition: first few principal components account for most of the variation in the data"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#what-are-principal-components",
    "href": "lectures/07-highdim-methods.html#what-are-principal-components",
    "title": "High-Dimensional Data",
    "section": "What are principal components?",
    "text": "What are principal components?\n\nAssume \\(\\boldsymbol{X}\\) is a \\(n \\times p\\) matrix that is centered and stardardized\nTotal variation \\(= p\\), since Var( \\(\\boldsymbol{x}_j\\) ) = 1 for all \\(j = 1, \\dots, p\\)\nPCA will give us \\(p\\) principal components that are \\(n\\)-length columns - call these \\(Z_1, \\dots, Z_p\\)\n\n\nFirst principal component (aka PC1):\n\\[Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\dots + \\phi_{p1} X_p\\]\n\n\n\n\\(\\phi_{j1}\\) are the weights indicating the contributions of each variable \\(j \\in 1, \\dots, p\\)\nWeights are normalized \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\\(\\phi_{1} = (\\phi_{11}, \\phi_{21}, \\dots, \\phi_{p1})\\) is the loading vector for PC1\n\n\n\n\n\\(Z_1\\) is a linear combination of the \\(p\\) variables that has the largest variance"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#what-are-principal-components-1",
    "href": "lectures/07-highdim-methods.html#what-are-principal-components-1",
    "title": "High-Dimensional Data",
    "section": "What are principal components?",
    "text": "What are principal components?\nSecond principal component:\n\\[Z_2 = \\phi_{12} X_1 + \\phi_{22} X_2 + \\dots + \\phi_{p2} X_p\\]\n\n\\(\\phi_{j2}\\) are the weights indicating the contributions of each variable \\(j \\in 1, \\dots, p\\)\nWeights are normalized \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\\(\\phi_{2} = (\\phi_{12}, \\phi_{22}, \\dots, \\phi_{p2})\\) is the loading vector for PC2\n\\(Z_2\\) is a linear combination of the \\(p\\) variables that has the largest variance\n\nSubject to constraint it is uncorrelated with \\(Z_1\\)"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#what-are-principal-components-2",
    "href": "lectures/07-highdim-methods.html#what-are-principal-components-2",
    "title": "High-Dimensional Data",
    "section": "What are principal components?",
    "text": "What are principal components?\nWe repeat this process to create \\(p\\) principal components\n\nUncorrelated: Each (\\(Z_j, Z_{j'}\\)) is uncorrelated with each other\nOrdered Variance: Var( \\(Z_1\\) ) \\(&gt;\\) Var( \\(Z_2\\) ) \\(&gt; \\dots &gt;\\) Var( \\(Z_p\\) )\nTotal Variance: \\(\\sum_{j=1}^p \\text{Var}(Z_j) = p\\)\n\nIntuition: pick some \\(k &lt;&lt; p\\) such that if \\(\\sum_{j=1}^k \\text{Var}(Z_j) \\approx p\\), then just using \\(Z_1, \\dots, Z_k\\)"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#visualizing-pca-in-two-dimensions",
    "href": "lectures/07-highdim-methods.html#visualizing-pca-in-two-dimensions",
    "title": "High-Dimensional Data",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#visualizing-pca-in-two-dimensions-1",
    "href": "lectures/07-highdim-methods.html#visualizing-pca-in-two-dimensions-1",
    "title": "High-Dimensional Data",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#visualizing-pca-in-two-dimensions-2",
    "href": "lectures/07-highdim-methods.html#visualizing-pca-in-two-dimensions-2",
    "title": "High-Dimensional Data",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#visualizing-pca-in-two-dimensions-3",
    "href": "lectures/07-highdim-methods.html#visualizing-pca-in-two-dimensions-3",
    "title": "High-Dimensional Data",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#visualizing-pca-in-two-dimensions-4",
    "href": "lectures/07-highdim-methods.html#visualizing-pca-in-two-dimensions-4",
    "title": "High-Dimensional Data",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#so-what-do-we-do-with-the-principal-components",
    "href": "lectures/07-highdim-methods.html#so-what-do-we-do-with-the-principal-components",
    "title": "High-Dimensional Data",
    "section": "So what do we do with the principal components?",
    "text": "So what do we do with the principal components?\nThe point: given a dataset with \\(p\\) variables, we can find \\(k\\) variables \\((k &lt;&lt; p)\\) that account for most of the variation in the data\n\nNote that the principal components are NOT easy to interpret - these are combinations of all variables\nPCA is similar to MDS with these main differences:\n\nMDS reduces a distance matrix while PCA reduces a data matrix\nPCA has a principled way to choose \\(k\\)\nCan visualize how the principal components are related to variables in data"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#working-with-pca-on-starbucks-drinks",
    "href": "lectures/07-highdim-methods.html#working-with-pca-on-starbucks-drinks",
    "title": "High-Dimensional Data",
    "section": "Working with PCA on Starbucks drinks",
    "text": "Working with PCA on Starbucks drinks\nUse the prcomp() function (based on SVD) for PCA on centered and scaled data\n\nstarbucks_pca &lt;- prcomp(dplyr::select(starbucks, serv_size_m_l:caffeine_mg),\n                        center = TRUE, scale. = TRUE)\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#computing-principal-components",
    "href": "lectures/07-highdim-methods.html#computing-principal-components",
    "title": "High-Dimensional Data",
    "section": "Computing Principal Components",
    "text": "Computing Principal Components\nExtract the matrix of principal components \\(\\boldsymbol{Z} = XV\\) (dimension of \\(\\boldsymbol{Z}\\) will match original data)\n\nstarbucks_pc_matrix &lt;- starbucks_pca$x\nhead(starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nColumns are uncorrelated, such that Var( \\(Z_1\\) ) \\(&gt;\\) Var( \\(Z_2\\) ) \\(&gt; \\dots &gt;\\) Var( \\(Z_p\\) ) - can start with a scatterplot of \\(Z_1, Z_2\\)"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#starbucks-drinks-pc1-and-pc2",
    "href": "lectures/07-highdim-methods.html#starbucks-drinks-pc1-and-pc2",
    "title": "High-Dimensional Data",
    "section": "Starbucks drinks: PC1 and PC2",
    "text": "Starbucks drinks: PC1 and PC2\n\nstarbucks &lt;- starbucks |&gt;\n  mutate(pc1 = starbucks_pc_matrix[,1], \n         pc2 = starbucks_pc_matrix[,2])\nstarbucks |&gt;\n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"PC 1\", y = \"PC 2\")"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#starbucks-drinks-pc1-and-pc2-output",
    "href": "lectures/07-highdim-methods.html#starbucks-drinks-pc1-and-pc2-output",
    "title": "High-Dimensional Data",
    "section": "Starbucks drinks: PC1 and PC2",
    "text": "Starbucks drinks: PC1 and PC2"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#making-pcs-interpretable-with-biplots-factoextra",
    "href": "lectures/07-highdim-methods.html#making-pcs-interpretable-with-biplots-factoextra",
    "title": "High-Dimensional Data",
    "section": "Making PCs interpretable with biplots (factoextra)",
    "text": "Making PCs interpretable with biplots (factoextra)\n\nlibrary(factoextra)\n# Designate to only label the variables:\nfviz_pca_biplot( \n  starbucks_pca, label = \"var\",\n  # Change the alpha for observations \n  # which is represented by ind\n  alpha.ind = .25,\n  # Modify the alpha for variables (var):\n  alpha.var = .75,\n  col.var = \"darkblue\")"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#making-pcs-interpretable-with-biplots-factoextra-output",
    "href": "lectures/07-highdim-methods.html#making-pcs-interpretable-with-biplots-factoextra-output",
    "title": "High-Dimensional Data",
    "section": "Making PCs interpretable with biplots (factoextra)",
    "text": "Making PCs interpretable with biplots (factoextra)"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#how-many-principal-components-to-use",
    "href": "lectures/07-highdim-methods.html#how-many-principal-components-to-use",
    "title": "High-Dimensional Data",
    "section": "How many principal components to use?",
    "text": "How many principal components to use?\nIntuition: Additional principal components will add smaller and smaller variance\n\nKeep adding components until the added variance drops off\n\n\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#create-scree-plot-aka-elbow-plot-to-choose",
    "href": "lectures/07-highdim-methods.html#create-scree-plot-aka-elbow-plot-to-choose",
    "title": "High-Dimensional Data",
    "section": "Create scree plot (aka “elbow plot”) to choose",
    "text": "Create scree plot (aka “elbow plot”) to choose\n\nfviz_eig(starbucks_pca, addlabels = TRUE) + \n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_pca$x)), linetype = \"dashed\", color = \"darkred\")"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#nonlinear-dimension-reduction-e.g.-t-sne",
    "href": "lectures/07-highdim-methods.html#nonlinear-dimension-reduction-e.g.-t-sne",
    "title": "High-Dimensional Data",
    "section": "Nonlinear dimension reduction, e.g., t-SNE",
    "text": "Nonlinear dimension reduction, e.g., t-SNE"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#consider-the-following-spiral-structure",
    "href": "lectures/07-highdim-methods.html#consider-the-following-spiral-structure",
    "title": "High-Dimensional Data",
    "section": "Consider the following spiral structure…",
    "text": "Consider the following spiral structure…"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#pca-simply-rotates-the-data",
    "href": "lectures/07-highdim-methods.html#pca-simply-rotates-the-data",
    "title": "High-Dimensional Data",
    "section": "PCA simply rotates the data…",
    "text": "PCA simply rotates the data…"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#nonlinear-dimension-reduction-with-t-sne",
    "href": "lectures/07-highdim-methods.html#nonlinear-dimension-reduction-with-t-sne",
    "title": "High-Dimensional Data",
    "section": "Nonlinear dimension reduction with t-SNE",
    "text": "Nonlinear dimension reduction with t-SNE"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#t-distributed-stochastic-neighbor-embedding",
    "href": "lectures/07-highdim-methods.html#t-distributed-stochastic-neighbor-embedding",
    "title": "High-Dimensional Data",
    "section": "t-distributed stochastic neighbor embedding",
    "text": "t-distributed stochastic neighbor embedding\n\nConstruct conditional probability for similarity between observations in original space, i.e., probability \\(x_i\\) will pick \\(x_j\\) as its neighbor\n\n\\[p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|x_i-x_j\\right\\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|x_i-x_k\\right\\|^2 / 2 \\sigma_i^2\\right)},\\quad p_{i j}=\\frac{\\left(p_{j \\mid i}+p_{i \\mid j}\\right)}{2 n}\\]\n\n\\(\\sigma_i\\) is the variance of Gaussian centered at \\(x_i\\) controlled by perplexity: \\(\\log (\\text { perplexity })=-\\sum_j p_{j \\mid i} \\log _2 p_{j \\mid i}\\)"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#t-distributed-stochastic-neighbor-embedding-1",
    "href": "lectures/07-highdim-methods.html#t-distributed-stochastic-neighbor-embedding-1",
    "title": "High-Dimensional Data",
    "section": "t-distributed stochastic neighbor embedding",
    "text": "t-distributed stochastic neighbor embedding\n\nFind points \\(y_i\\) in lower dimensional space with symmetrized student t-distribution\n\n\\[q_{j \\mid i}=\\frac{\\left(1+\\left\\|y_i-y_j\\right\\|^2\\right)^{-1}}{\\sum_{k \\neq i}\\left(1+\\left\\|y_i-y_k\\right\\|^2\\right)^{-1}}, \\quad q_{i j}=\\frac{q_{i \\mid j}+q_{j \\mid i}}{2 n}\\]\n\nMatch conditional probabilities by minimize sum of KL divergences \\(C=\\sum_{i j} p_{i j} \\log \\left(\\frac{p_{i j}}{q_{i j}}\\right)\\)"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#starbucks-t-sne-plot-with-rtsne",
    "href": "lectures/07-highdim-methods.html#starbucks-t-sne-plot-with-rtsne",
    "title": "High-Dimensional Data",
    "section": "Starbucks t-SNE plot with Rtsne",
    "text": "Starbucks t-SNE plot with Rtsne\n\nset.seed(2013)\ntsne_fit &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) |&gt;\n  scale() |&gt;\n  Rtsne(check_duplicates = FALSE) \n\nstarbucks |&gt;\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |&gt;\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) + \n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#starbucks-t-sne-plot-with-rtsne-output",
    "href": "lectures/07-highdim-methods.html#starbucks-t-sne-plot-with-rtsne-output",
    "title": "High-Dimensional Data",
    "section": "Starbucks t-SNE plot with Rtsne",
    "text": "Starbucks t-SNE plot with Rtsne"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#starbucks-t-sne-plot---involves-randomness",
    "href": "lectures/07-highdim-methods.html#starbucks-t-sne-plot---involves-randomness",
    "title": "High-Dimensional Data",
    "section": "Starbucks t-SNE plot - involves randomness!",
    "text": "Starbucks t-SNE plot - involves randomness!\n\nset.seed(2014) \ntsne_fit &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) |&gt;\n  scale() |&gt;\n  Rtsne(check_duplicates = FALSE) \n\nstarbucks |&gt;\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |&gt;\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#starbucks-t-sne-plot---involves-randomness-output",
    "href": "lectures/07-highdim-methods.html#starbucks-t-sne-plot---involves-randomness-output",
    "title": "High-Dimensional Data",
    "section": "Starbucks t-SNE plot - involves randomness!",
    "text": "Starbucks t-SNE plot - involves randomness!"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#starbucks-t-sne-plot---watch-the-perplexity",
    "href": "lectures/07-highdim-methods.html#starbucks-t-sne-plot---watch-the-perplexity",
    "title": "High-Dimensional Data",
    "section": "Starbucks t-SNE plot - watch the perplexity!",
    "text": "Starbucks t-SNE plot - watch the perplexity!\n\nset.seed(2013) \ntsne_fit &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) |&gt;\n  scale() |&gt;\n  Rtsne(perplexity = 100, \n        check_duplicates = FALSE)\n\nstarbucks |&gt;\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |&gt;\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#starbucks-t-sne-plot---watch-the-perplexity-output",
    "href": "lectures/07-highdim-methods.html#starbucks-t-sne-plot---watch-the-perplexity-output",
    "title": "High-Dimensional Data",
    "section": "Starbucks t-SNE plot - watch the perplexity!",
    "text": "Starbucks t-SNE plot - watch the perplexity!"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#back-to-the-spirals-results-depend-on-perplexity",
    "href": "lectures/07-highdim-methods.html#back-to-the-spirals-results-depend-on-perplexity",
    "title": "High-Dimensional Data",
    "section": "Back to the spirals: results depend on perplexity!",
    "text": "Back to the spirals: results depend on perplexity!"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#criticisms-of-t-sne-plots",
    "href": "lectures/07-highdim-methods.html#criticisms-of-t-sne-plots",
    "title": "High-Dimensional Data",
    "section": "Criticisms of t-SNE plots",
    "text": "Criticisms of t-SNE plots\n\nPoor scalability: does not scale well for large data, can practically only embed into 2 or 3 dimensions\nMeaningless global structure: distance between clusters might not have clear interpretation and cluster size doesn’t have any meaning to it\nPoor performance with very high dimensional data: need PCA as pre-dimension reduction step\nSometime random noise can lead to false positive structure in the t-SNE projection\nCan NOT interpret like PCA!"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#recap-and-next-steps",
    "href": "lectures/07-highdim-methods.html#recap-and-next-steps",
    "title": "High-Dimensional Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWalked through PCA for dimension reduction\nDiscussed non-linear dimension reduction with t-SNE plots\n\n\n\nHW3 is due TONIGHT!\nHW4 is posted due next Wednesday Sept 25th\n\n\n\n\nNext time: Visualizing trends and time series data\nRecommended reading: CW Chapter 12 Visualizing associations among two or more quantitative variables, How to Use t-SNE Effectively, Understanding UMAP"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#pca-singular-value-decomposition-svd",
    "href": "lectures/07-highdim-methods.html#pca-singular-value-decomposition-svd",
    "title": "High-Dimensional Data",
    "section": "PCA: singular value decomposition (SVD)",
    "text": "PCA: singular value decomposition (SVD)\n\\[\nX = U D V^T\n\\]\n\nMatrices \\(U\\) and \\(V\\) contain the left and right (respectively) singular vectors of scaled matrix \\(X\\)\n\\(D\\) is the diagonal matrix of the singular values\nSVD simplifies matrix-vector multiplication as rotate, scale, and rotate again\n\n\\(V\\) is called the loading matrix for \\(X\\) with \\(\\phi_{j}\\) as columns,\n\n\\(Z = X  V\\) is the PC matrix"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#eigenvalue-decomposition-aka-spectral-decomposition",
    "href": "lectures/07-highdim-methods.html#eigenvalue-decomposition-aka-spectral-decomposition",
    "title": "High-Dimensional Data",
    "section": "Eigenvalue decomposition (aka spectral decomposition)",
    "text": "Eigenvalue decomposition (aka spectral decomposition)\n\\[\nX = U D V^T\n\\]\n\n\\(V\\) are the eigenvectors of \\(X^TX\\) (covariance matrix, \\(^T\\) means transpose)\n\\(U\\) are the eigenvectors of \\(XX^T\\)\nThe singular values (diagonal of \\(D\\)) are square roots of the eigenvalues of \\(X^TX\\) or \\(XX^T\\)\nMeaning that \\(Z = UD\\)"
  },
  {
    "objectID": "lectures/07-highdim-methods.html#eigenvalues-guide-dimension-reduction",
    "href": "lectures/07-highdim-methods.html#eigenvalues-guide-dimension-reduction",
    "title": "High-Dimensional Data",
    "section": "Eigenvalues guide dimension reduction",
    "text": "Eigenvalues guide dimension reduction\nWe want to choose \\(p^* &lt; p\\) such that we are explaining variation in the data\nEigenvalues \\(\\lambda_j\\) for \\(j \\in 1, \\dots, p\\) indicate the variance explained by each component\n\n\\(\\sum_j^p \\lambda_j = p\\), meaning \\(\\lambda_j \\geq 1\\) indicates \\(\\text{PC}j\\) contains at least one variable’s worth in variability\n\\(\\lambda_j / p\\) equals proportion of variance explained by \\(\\text{PC}j\\)\nArranged in descending order so that \\(\\lambda_1\\) is largest eigenvalue and corresponds to PC1\nCan compute the cumulative proportion of variance explained (CVE) with \\(p^*\\) components:\n\n\\[\\text{CVE}_{p^*} = \\frac{\\sum_j^{p*} \\lambda_j}{p}\\]\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "lectures/05-2dquant.html#reminders-previously-and-today",
    "href": "lectures/05-2dquant.html#reminders-previously-and-today",
    "title": "2D Quantitative Data",
    "section": "Reminders, previously, and today…",
    "text": "Reminders, previously, and today…\n\nHW2 is due TONIGHT!\nHW3 is posted and due next Wednesday Sept 18th\n\n\n\nFinished up discussion of 1D quantitative visualizations\nDiscussed impact of bins on histograms\nCovered ECDFs and connection to KS-tests\nWalked through density estimation and ways of visualizing conditional distributions\n\n\n\nTODAY:\n\nVisualize 2D quantitative data\nDiscuss approaches for visualizing conditional and joint distributions"
  },
  {
    "objectID": "lectures/05-2dquant.html#d-quantitative-data",
    "href": "lectures/05-2dquant.html#d-quantitative-data",
    "title": "2D Quantitative Data",
    "section": "2D quantitative data",
    "text": "2D quantitative data\n\nWe’re working with two variables: \\((X, Y) \\in \\mathbb{R}^2\\), i.e., dataset with \\(n\\) rows and 2 columns\n\n\n\nGoals:\n\ndescribing the relationships between two variables\ndescribing the conditional distribution \\(Y | X\\) via regression analysis\ndescribing the joint distribution \\(X,Y\\) via contours, heatmaps, etc.\n\n\n\n\n\nFew big picture ideas to keep in mind:\n\nscatterplots are by far the most common visual\nregression analysis is by far the most popular analysis (you have a whole class on this…)\nrelationships may vary across other variables, e.g., categorical variables"
  },
  {
    "objectID": "lectures/05-2dquant.html#making-scatterplots-with-geom_point",
    "href": "lectures/05-2dquant.html#making-scatterplots-with-geom_point",
    "title": "2D Quantitative Data",
    "section": "Making scatterplots with geom_point()",
    "text": "Making scatterplots with geom_point()\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/05-2dquant.html#making-scatterplots-always-adjust-the-alpha",
    "href": "lectures/05-2dquant.html#making-scatterplots-always-adjust-the-alpha",
    "title": "2D Quantitative Data",
    "section": "Making scatterplots: ALWAYS adjust the alpha",
    "text": "Making scatterplots: ALWAYS adjust the alpha\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/05-2dquant.html#displaying-trend-lines-linear-regression",
    "href": "lectures/05-2dquant.html#displaying-trend-lines-linear-regression",
    "title": "2D Quantitative Data",
    "section": "Displaying trend lines: linear regression",
    "text": "Displaying trend lines: linear regression\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lectures/05-2dquant.html#assessing-assumptions-of-linear-regression",
    "href": "lectures/05-2dquant.html#assessing-assumptions-of-linear-regression",
    "title": "2D Quantitative Data",
    "section": "Assessing assumptions of linear regression",
    "text": "Assessing assumptions of linear regression\nLinear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\nIf this is true, then \\(Y_i - \\hat{Y}_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\nPlot residuals against \\(\\hat{Y}_i\\), residuals vs fit plot\n\nUsed to assess linearity, any divergence from mean 0\nUsed to assess equal variance, i.e., if \\(\\sigma^2\\) is homogenous across predictions/fits \\(\\hat{Y}_i\\)\n\n\n\nMore difficult to assess the independence and fixed \\(X\\) assumptions\n\nMake these assumptions based on subject-matter knowledge"
  },
  {
    "objectID": "lectures/05-2dquant.html#residual-vs-fit-plots",
    "href": "lectures/05-2dquant.html#residual-vs-fit-plots",
    "title": "2D Quantitative Data",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\nlin_reg &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "lectures/05-2dquant.html#residual-vs-fit-plots-1",
    "href": "lectures/05-2dquant.html#residual-vs-fit-plots-1",
    "title": "2D Quantitative Data",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth()"
  },
  {
    "objectID": "lectures/05-2dquant.html#local-linear-regression-via-loess",
    "href": "lectures/05-2dquant.html#local-linear-regression-via-loess",
    "title": "2D Quantitative Data",
    "section": "Local linear regression via LOESS",
    "text": "Local linear regression via LOESS\n\\(Y_i \\overset{iid}{\\sim} N(f(x), \\sigma^2)\\), where \\(f(x)\\) is some unknown function\n\nIn local linear regression, we estimate \\(f(X_i)\\):\n\\[\\text{arg }\\underset{\\beta_0, \\beta_1}{\\text{min}} \\sum_i^n w_i(x) \\cdot \\big(Y_i - \\beta_0 - \\beta_1 X_i \\big)^2\\]\n\n\ngeom_smooth() uses tri-cubic weighting:\n\\[w_i(d_i) = \\begin{cases} (1 - |d_i|^3)^3, \\text{ if } i \\in \\text{neighborhood of  } x, \\\\\n0 \\text{ if } i \\notin \\text{neighborhood of  } x \\end{cases}\\]\n\n\\(d_i\\) is the distance between \\(x\\) and \\(X_i\\) scaled to be between 0 and 1\nspan: decides proportion of observations in neighborhood (default is 0.75)"
  },
  {
    "objectID": "lectures/05-2dquant.html#displaying-trend-lines-loess",
    "href": "lectures/05-2dquant.html#displaying-trend-lines-loess",
    "title": "2D Quantitative Data",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth()\n\n\nFor \\(n &gt; 1000\\), mgcv::gam() is used with formula = y ~ s(x, bs = \"cs\") and method = \"REML\""
  },
  {
    "objectID": "lectures/05-2dquant.html#displaying-trend-lines-loess-1",
    "href": "lectures/05-2dquant.html#displaying-trend-lines-loess-1",
    "title": "2D Quantitative Data",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = .1)"
  },
  {
    "objectID": "lectures/05-2dquant.html#can-also-update-formula-within-plot",
    "href": "lectures/05-2dquant.html#can-also-update-formula-within-plot",
    "title": "2D Quantitative Data",
    "section": "Can also update formula within plot",
    "text": "Can also update formula within plot\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2))\n\n\nExercise: check the updated residual plot with this model"
  },
  {
    "objectID": "lectures/05-2dquant.html#what-about-focusing-on-the-joint-distribution",
    "href": "lectures/05-2dquant.html#what-about-focusing-on-the-joint-distribution",
    "title": "2D Quantitative Data",
    "section": "What about focusing on the joint distribution?",
    "text": "What about focusing on the joint distribution?\nExample dataset of pitches thrown by baseball superstar Shohei Ohtani\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_point(alpha = 0.2) +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/05-2dquant.html#going-from-1d-to-2d-density-estimation",
    "href": "lectures/05-2dquant.html#going-from-1d-to-2d-density-estimation",
    "title": "2D Quantitative Data",
    "section": "Going from 1D to 2D density estimation",
    "text": "Going from 1D to 2D density estimation\nIn 1D: estimate density \\(f(x)\\), assuming that \\(f(x)\\) is smooth:\n\\[\n\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\nIn 2D: estimate joint density \\(f(x_1, x_2)\\)\n\\[\\hat{f}(x_1, x_2) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h_1h_2} K(\\frac{x_1 - x_{i1}}{h_1}) K(\\frac{x_2 - x_{i2}}{h_2})\\]\n\n\nIn 1D there was one bandwidth, now we have two bandwidths\n\n\\(h_1\\): controls smoothness as \\(X_1\\) changes, holding \\(X_2\\) fixed\n\\(h_2\\): controls smoothness as \\(X_2\\) changes, holding \\(X_1\\) fixed\n\nAgain Gaussian kernels are the most popular…"
  },
  {
    "objectID": "lectures/05-2dquant.html#so-how-do-we-display-densities-for-2d-data",
    "href": "lectures/05-2dquant.html#so-how-do-we-display-densities-for-2d-data",
    "title": "2D Quantitative Data",
    "section": "So how do we display densities for 2D data?",
    "text": "So how do we display densities for 2D data?"
  },
  {
    "objectID": "lectures/05-2dquant.html#how-to-read-contour-plots",
    "href": "lectures/05-2dquant.html#how-to-read-contour-plots",
    "title": "2D Quantitative Data",
    "section": "How to read contour plots?",
    "text": "How to read contour plots?\nBest known in topology: outlines (contours) denote levels of elevation"
  },
  {
    "objectID": "lectures/05-2dquant.html#display-2d-contour-plot",
    "href": "lectures/05-2dquant.html#display-2d-contour-plot",
    "title": "2D Quantitative Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_point(alpha = 0.2) +\n  geom_density2d() +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/05-2dquant.html#display-2d-contour-plot-1",
    "href": "lectures/05-2dquant.html#display-2d-contour-plot-1",
    "title": "2D Quantitative Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_density2d() +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/05-2dquant.html#display-2d-contour-plot-2",
    "href": "lectures/05-2dquant.html#display-2d-contour-plot-2",
    "title": "2D Quantitative Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  stat_density2d(aes(fill = after_stat(level)), geom = \"polygon\") +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/05-2dquant.html#visualizing-grid-heat-maps",
    "href": "lectures/05-2dquant.html#visualizing-grid-heat-maps",
    "title": "2D Quantitative Data",
    "section": "Visualizing grid heat maps",
    "text": "Visualizing grid heat maps\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  stat_density2d(aes(fill = after_stat(density)), \n                 geom = \"tile\", contour = FALSE) + \n  coord_fixed() +\n  scale_fill_gradient(low = \"white\", high = \"red\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/05-2dquant.html#alternative-idea-hexagonal-binning",
    "href": "lectures/05-2dquant.html#alternative-idea-hexagonal-binning",
    "title": "2D Quantitative Data",
    "section": "Alternative idea: hexagonal binning",
    "text": "Alternative idea: hexagonal binning\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "lectures/05-2dquant.html#recap-and-next-steps",
    "href": "lectures/05-2dquant.html#recap-and-next-steps",
    "title": "2D Quantitative Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nUse scatterplots to visualize 2D quantitative\nBe careful of over-plotting! May motivate contours or hexagonal bins…\nDiscussed approaches for visualizing conditional relationships\n\n\n\nHW2 is due TONIGHT!\nHW3 is posted due next Wednesday Sept 18th\n\n\n\n\nNext time: Into high-dimensional data\nRecommended reading:\nCW Chapter 12 Visualizing associations among two or more quantitative variables\n\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#reminders-previously-and-today",
    "href": "lectures/08-tsne-trends-time-series.html#reminders-previously-and-today",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Reminders, previously, and today…",
    "text": "Reminders, previously, and today…\n\nHW4 is due Wednesday Sept 25th\nYou need to email me a draft of your EDA report! (1 per group)\n\n\n\nWalked through PCA for dimension reduction\nDiscussed choosing the number of PCs with scree plots\nCreated biplots for interpreting variable contributions to PCs\n\n\n\nTODAY:\n\nIntroduce non-linear dimension reduction with t-SNE plots\nDiscuss visualizing trends\nWalk through the basics of time series data"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#consider-the-following-spiral-structure",
    "href": "lectures/08-tsne-trends-time-series.html#consider-the-following-spiral-structure",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Consider the following spiral structure…",
    "text": "Consider the following spiral structure…"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#pca-simply-rotates-the-data",
    "href": "lectures/08-tsne-trends-time-series.html#pca-simply-rotates-the-data",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "PCA simply rotates the data…",
    "text": "PCA simply rotates the data…"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#nonlinear-dimension-reduction-with-t-sne",
    "href": "lectures/08-tsne-trends-time-series.html#nonlinear-dimension-reduction-with-t-sne",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Nonlinear dimension reduction with t-SNE",
    "text": "Nonlinear dimension reduction with t-SNE"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#t-distributed-stochastic-neighbor-embedding",
    "href": "lectures/08-tsne-trends-time-series.html#t-distributed-stochastic-neighbor-embedding",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "t-distributed stochastic neighbor embedding",
    "text": "t-distributed stochastic neighbor embedding\n\nConstruct conditional probability for similarity between observations in original space, i.e., probability \\(x_i\\) will pick \\(x_j\\) as its neighbor\n\n\\[p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|x_i-x_j\\right\\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|x_i-x_k\\right\\|^2 / 2 \\sigma_i^2\\right)},\\quad p_{i j}=\\frac{\\left(p_{j \\mid i}+p_{i \\mid j}\\right)}{2 n}\\]\n\n\\(\\sigma_i\\) is the variance of Gaussian centered at \\(x_i\\) controlled by perplexity: \\(\\log (\\text { perplexity })=-\\sum_j p_{j \\mid i} \\log _2 p_{j \\mid i}\\)"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#t-distributed-stochastic-neighbor-embedding-1",
    "href": "lectures/08-tsne-trends-time-series.html#t-distributed-stochastic-neighbor-embedding-1",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "t-distributed stochastic neighbor embedding",
    "text": "t-distributed stochastic neighbor embedding\n\nFind points \\(y_i\\) in lower dimensional space with symmetrized student t-distribution\n\n\\[q_{j \\mid i}=\\frac{\\left(1+\\left\\|y_i-y_j\\right\\|^2\\right)^{-1}}{\\sum_{k \\neq i}\\left(1+\\left\\|y_i-y_k\\right\\|^2\\right)^{-1}}, \\quad q_{i j}=\\frac{q_{i \\mid j}+q_{j \\mid i}}{2 n}\\]\n\nMatch conditional probabilities by minimize sum of KL divergences \\(C=\\sum_{i j} p_{i j} \\log \\left(\\frac{p_{i j}}{q_{i j}}\\right)\\)"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot-with-rtsne",
    "href": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot-with-rtsne",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Starbucks t-SNE plot with Rtsne",
    "text": "Starbucks t-SNE plot with Rtsne\n\nset.seed(2013)\ntsne_fit &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) |&gt;\n  scale() |&gt;\n  Rtsne(check_duplicates = FALSE) \n\nstarbucks |&gt;\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |&gt;\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) + \n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot-with-rtsne-output",
    "href": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot-with-rtsne-output",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Starbucks t-SNE plot with Rtsne",
    "text": "Starbucks t-SNE plot with Rtsne"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot---involves-randomness",
    "href": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot---involves-randomness",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Starbucks t-SNE plot - involves randomness!",
    "text": "Starbucks t-SNE plot - involves randomness!\n\nset.seed(2014) \ntsne_fit &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) |&gt;\n  scale() |&gt;\n  Rtsne(check_duplicates = FALSE) \n\nstarbucks |&gt;\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |&gt;\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot---involves-randomness-output",
    "href": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot---involves-randomness-output",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Starbucks t-SNE plot - involves randomness!",
    "text": "Starbucks t-SNE plot - involves randomness!"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot---watch-the-perplexity",
    "href": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot---watch-the-perplexity",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Starbucks t-SNE plot - watch the perplexity!",
    "text": "Starbucks t-SNE plot - watch the perplexity!\n\nset.seed(2013) \ntsne_fit &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) |&gt;\n  scale() |&gt;\n  Rtsne(perplexity = 100, \n        check_duplicates = FALSE)\n\nstarbucks |&gt;\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |&gt;\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot---watch-the-perplexity-output",
    "href": "lectures/08-tsne-trends-time-series.html#starbucks-t-sne-plot---watch-the-perplexity-output",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Starbucks t-SNE plot - watch the perplexity!",
    "text": "Starbucks t-SNE plot - watch the perplexity!"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#back-to-the-spirals-results-depend-on-perplexity",
    "href": "lectures/08-tsne-trends-time-series.html#back-to-the-spirals-results-depend-on-perplexity",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Back to the spirals: results depend on perplexity!",
    "text": "Back to the spirals: results depend on perplexity!"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#criticisms-of-t-sne-plots",
    "href": "lectures/08-tsne-trends-time-series.html#criticisms-of-t-sne-plots",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Criticisms of t-SNE plots",
    "text": "Criticisms of t-SNE plots\n\nPoor scalability: does not scale well for large data, can practically only embed into 2 or 3 dimensions\nMeaningless global structure: distance between clusters might not have clear interpretation and cluster size doesn’t have any meaning to it\nPoor performance with very high dimensional data: need PCA as pre-dimension reduction step\nSometime random noise can lead to false positive structure in the t-SNE projection\nCan NOT interpret like PCA!"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#longitudinal-data-and-time-series-structure",
    "href": "lectures/08-tsne-trends-time-series.html#longitudinal-data-and-time-series-structure",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Longitudinal data and time series structure",
    "text": "Longitudinal data and time series structure\n\nConsider a single observation measured across time\n\n\n\n\nVariable\n\\(T_1\\)\n\\(T_2\\)\n\\(\\dots\\)\n\\(T_J\\)\n\n\n\n\n\\(X_1\\)\n\\(x_{11}\\)\n\\(x_{12}\\)\n\\(\\dots\\)\n\\(x_{1J}\\)\n\n\n\\(X_2\\)\n\\(x_{21}\\)\n\\(x_{22}\\)\n\\(\\dots\\)\n\\(x_{2J}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\dots\\)\n\\(\\vdots\\)\n\n\n\\(X_P\\)\n\\(x_{P1}\\)\n\\(x_{P2}\\)\n\\(\\dots\\)\n\\(x_{PJ}\\)\n\n\n\n\nWith \\(N\\) observations we have \\(N\\) of these matrices\nTime may consist of regularly spaced intervals\n\nFor example, \\(T_1 = t\\), \\(T_2 = t + h\\), \\(T_3 = t + 2h\\), etc.\n\nIrregularly spaced intervals, then work with the raw \\(T_1,T_2,...\\)"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#example-statistics-phds-by-year",
    "href": "lectures/08-tsne-trends-time-series.html#example-statistics-phds-by-year",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Example: Statistics PhDs by year",
    "text": "Example: Statistics PhDs by year\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\", title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#example-statistics-phds-by-year-1",
    "href": "lectures/08-tsne-trends-time-series.html#example-statistics-phds-by-year-1",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Example: Statistics PhDs by year",
    "text": "Example: Statistics PhDs by year\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year), \n                     labels = unique(stat_phd_year_summary$year)) + \n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\", title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#add-lines-to-emphasize-order",
    "href": "lectures/08-tsne-trends-time-series.html#add-lines-to-emphasize-order",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Add lines to emphasize order",
    "text": "Add lines to emphasize order\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#drop-points-to-emphasize-trends",
    "href": "lectures/08-tsne-trends-time-series.html#drop-points-to-emphasize-trends",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Drop points to emphasize trends",
    "text": "Drop points to emphasize trends\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#can-fill-the-area-under-the-line",
    "href": "lectures/08-tsne-trends-time-series.html#can-fill-the-area-under-the-line",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Can fill the area under the line",
    "text": "Can fill the area under the line\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_area(fill = \"darkblue\", alpha = 0.5) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#several-time-series-do-not-only-use-points",
    "href": "lectures/08-tsne-trends-time-series.html#several-time-series-do-not-only-use-points",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Several time series? Do NOT only use points",
    "text": "Several time series? Do NOT only use points\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_point() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"bottom\", legend.text = element_text(size = 7)) +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#several-time-series-use-lines",
    "href": "lectures/08-tsne-trends-time-series.html#several-time-series-use-lines",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Several time series? Use lines!",
    "text": "Several time series? Use lines!\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#using-ggrepel-to-directly-label-lines",
    "href": "lectures/08-tsne-trends-time-series.html#using-ggrepel-to-directly-label-lines",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Using ggrepel to directly label lines",
    "text": "Using ggrepel to directly label lines\n\nstats_phds_2017 &lt;- stats_phds |&gt; filter(year == 2017)\n\nlibrary(ggrepel)\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  # Add the labels:\n  geom_text_repel(data = stats_phds_2017, aes(label = field),\n                  size = 3, \n                  # Drop the segment connection:\n                  segment.color = NA, \n                  # Move labels up or down based on overlap\n                  direction = \"y\",\n                  # Try to align the labels horizontally on the left hand side\n                  hjust = \"left\") +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year),\n                     # Update the limits so that there is some padding on the\n                     # x-axis but don't label the new maximum\n                     limits = c(min(stat_phd_year_summary$year),\n                                max(stat_phd_year_summary$year) + 3)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#using-ggrepel-to-directly-label-lines-output",
    "href": "lectures/08-tsne-trends-time-series.html#using-ggrepel-to-directly-label-lines-output",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Using ggrepel to directly label lines",
    "text": "Using ggrepel to directly label lines"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#using-gghighlight-instead",
    "href": "lectures/08-tsne-trends-time-series.html#using-gghighlight-instead",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead\n\nlibrary(gghighlight)\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight()  +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#using-gghighlight-instead-output",
    "href": "lectures/08-tsne-trends-time-series.html#using-gghighlight-instead-output",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#using-gghighlight-instead-1",
    "href": "lectures/08-tsne-trends-time-series.html#using-gghighlight-instead-1",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead\n\nlibrary(gghighlight)\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight(line_label_type = \"sec_axis\")  +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#using-gghighlight-instead-1-output",
    "href": "lectures/08-tsne-trends-time-series.html#using-gghighlight-instead-1-output",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#how-do-we-plot-many-lines-not-like-this",
    "href": "lectures/08-tsne-trends-time-series.html#how-do-we-plot-many-lines-not-like-this",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "How do we plot many lines? NOT LIKE THIS!",
    "text": "How do we plot many lines? NOT LIKE THIS!\n\nphd_field |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#how-do-we-plot-many-lines-not-like-this-output",
    "href": "lectures/08-tsne-trends-time-series.html#how-do-we-plot-many-lines-not-like-this-output",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "How do we plot many lines? NOT LIKE THIS!",
    "text": "How do we plot many lines? NOT LIKE THIS!"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#instead-we-highlight-specific-lines",
    "href": "lectures/08-tsne-trends-time-series.html#instead-we-highlight-specific-lines",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Instead we highlight specific lines",
    "text": "Instead we highlight specific lines\n\nphd_field |&gt;\n  filter(!(field %in% c(\"Biometrics and biostatistics\", \"Statistics (mathematics)\"))) |&gt;\n  ggplot() +\n  # Add the background lines - need to specify the group to be the field\n  geom_line(aes(x = year, y = n_phds, group = field),\n            color = \"gray\", size = .5, alpha = .5) +\n  # Now add the layer with the lines of interest:\n  geom_line(data = filter(phd_field,\n                          # Note this is just the opposite of the above since ! is removed\n                          field %in% c(\"Biometrics and biostatistics\", \n                                       \"Statistics (mathematics)\")),\n            aes(x = year, y = n_phds, color = field),\n            # Make the size larger\n            size = .75, alpha = 1) +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"bottom\", \n        # Drop the panel lines making the gray difficult to see\n        panel.grid = element_blank()) +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#instead-we-highlight-specific-lines-output",
    "href": "lectures/08-tsne-trends-time-series.html#instead-we-highlight-specific-lines-output",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Instead we highlight specific lines",
    "text": "Instead we highlight specific lines"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#or-you-can-use-gghighlight-instead",
    "href": "lectures/08-tsne-trends-time-series.html#or-you-can-use-gghighlight-instead",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Or you can use gghighlight instead",
    "text": "Or you can use gghighlight instead\n\nphd_field |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight(field %in% c(\"Biometrics and biostatistics\", \"Statistics (mathematics)\"),\n              line_label_type = \"sec_axis\") +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#or-you-can-use-gghighlight-instead-output",
    "href": "lectures/08-tsne-trends-time-series.html#or-you-can-use-gghighlight-instead-output",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Or you can use gghighlight instead",
    "text": "Or you can use gghighlight instead"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#what-about-nightingales-rose-diagram",
    "href": "lectures/08-tsne-trends-time-series.html#what-about-nightingales-rose-diagram",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "What about Nightingale’s rose diagram?",
    "text": "What about Nightingale’s rose diagram?"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#what-about-nightingales-rose-diagram-1",
    "href": "lectures/08-tsne-trends-time-series.html#what-about-nightingales-rose-diagram-1",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "What about Nightingale’s rose diagram?",
    "text": "What about Nightingale’s rose diagram?"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#what-about-displaying-lines-instead",
    "href": "lectures/08-tsne-trends-time-series.html#what-about-displaying-lines-instead",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "What about displaying lines instead?",
    "text": "What about displaying lines instead?"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#things-of-interest-for-time-series-data",
    "href": "lectures/08-tsne-trends-time-series.html#things-of-interest-for-time-series-data",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Things of interest for time series data",
    "text": "Things of interest for time series data\nTime series can be characterized by three features:\n\nTrends: Does the variable increase or decrease over time, on average?\nSeasonality: Are there changes in the variable that regularly happen (e.g., every winter, every hour, etc.)? Sometimes called periodicity.\nNoise: Variation in the variable beyond average trends and seasonality.\n\nMoving averages are a starting point for visualizing how a trend changes over time"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#be-responsible-with-your-axes",
    "href": "lectures/08-tsne-trends-time-series.html#be-responsible-with-your-axes",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Be responsible with your axes!",
    "text": "Be responsible with your axes!"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#be-responsible-with-your-axes-1",
    "href": "lectures/08-tsne-trends-time-series.html#be-responsible-with-your-axes-1",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Be responsible with your axes!",
    "text": "Be responsible with your axes!"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#moving-average-plots",
    "href": "lectures/08-tsne-trends-time-series.html#moving-average-plots",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Moving Average Plots",
    "text": "Moving Average Plots\nThe Financial Times COVID-19 plots displayed a moving average (sometimes called a rolling average)\nIntuition\n\nDivide your data into small subsets (“windows”)\nCompute the average within each window\nConnect the averages together to make a trend line\n\n\nSometimes called a simple moving average\nThis is exactly what we did with LOESS… we called this a sliding window, but it’s the same thing"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#how-are-moving-averages-computed",
    "href": "lectures/08-tsne-trends-time-series.html#how-are-moving-averages-computed",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "How are moving averages computed?",
    "text": "How are moving averages computed?\nIntuition\n\nDivide your data into small subsets (windows)\nCompute the average within each window\nConnect the averages together to make a trend line\n\n\nMathematically, a moving average can be written as the following:\n\\[\\mu_k = \\frac{\\sum_{t=k - h + 1}^k X_t}{h}\\]\n\nLarge \\(h\\): Smooth line; captures global trends\nSmall \\(h\\): Jagged/volatile line; captures local trends"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#working-with-time-series",
    "href": "lectures/08-tsne-trends-time-series.html#working-with-time-series",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Working with Time Series",
    "text": "Working with Time Series\nco2: Mauna Loa Atmospheric CO2 Concentration dataset (monthly \\(\\text{CO}^2\\) concentration 1959 to 1997)\n\nco2_tbl |&gt;\n  ggplot(aes(x = obs_i, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Time index\", y = \"CO2 (ppm)\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#formatting-dates",
    "href": "lectures/08-tsne-trends-time-series.html#formatting-dates",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Formatting Dates",
    "text": "Formatting Dates\nCan use as.Date() to create time indexes.\n\nDefault format is Year/Month/Day. For something else, need to specify format in as.Date() (e.g., format = \"%m/%d/%Y\")"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#use-scale_x_date-to-create-interpretable-axis-labels",
    "href": "lectures/08-tsne-trends-time-series.html#use-scale_x_date-to-create-interpretable-axis-labels",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Use scale_x_date() to create interpretable axis labels",
    "text": "Use scale_x_date() to create interpretable axis labels"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#other-moving-averages",
    "href": "lectures/08-tsne-trends-time-series.html#other-moving-averages",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Other Moving Averages",
    "text": "Other Moving Averages\nTwo other common averages: Cumulative moving averages and weighted moving averages.\n\nCumulative moving average: The average at time \\(k\\) is the average of all points at and before \\(k\\). Mathematically:\n\n\\[\\mu_k^{(CMA)} = \\frac{\\sum_{t=1}^k X_t}{k}\\]\n\n\nWeighted moving average: Same as simple moving average, but different measurements get different weights for the average.\n\n\\[\\mu_k^{(WMA)} = \\frac{\\sum_{t=k - h + 1}^k X_t \\cdot w_t}{ \\sum_{t=k - h + 1}^k w_t}\\]"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#working-with-lags",
    "href": "lectures/08-tsne-trends-time-series.html#working-with-lags",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Working with lags",
    "text": "Working with lags\nTime series data is fundamentally different from other data problems we’ve worked with because measurements are not independent\nObvious example: The temperature today is correlated with temperature yesterday. (Maybe not in Pittsburgh?)\n\nImportant term: lags. Used to determine if one time point influences future time points.\nLag 1: Comparing time series at time \\(t\\) with time series at time \\(t - 1\\).\nLag 2: Comparing time series at time \\(t\\) with time series at time \\(t - 2\\).\nAnd so on…\n\n\nLet’s say we have time measurements \\((X_1, X_2, X_3, X_4, X_5)\\).\nThe \\(\\ell = 1\\) lag is \\((X_2, X_3, X_4, X_5)\\) vs \\((X_1, X_2, X_3, X_4)\\).\n\n\nThe \\(\\ell = 2\\) lag is \\((X_3, X_4, X_5)\\) vs \\((X_1, X_2, X_3)\\).\nConsider: Are previous outcomes (lags) predictive of future outcomes?"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#autocorrelation",
    "href": "lectures/08-tsne-trends-time-series.html#autocorrelation",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nAutocorrelation: Correlation between a time series and a lagged version of itself.\nDefine \\(r_{\\ell}\\) as the correlation between a time series and Lag \\(\\ell\\) of that time series.\n\nLag 1: \\(r_1\\) is correlation between \\((X_2, X_3, X_4, X_5)\\) and \\((X_1,X_2,X_3,X_4)\\)\nLag 2: \\(r_2\\) is correlation between \\((X_3, X_4, X_5)\\) and \\((X_1,X_2,X_3)\\)\nAnd so on…\n\n\nCommon diagnostic: Plot \\(\\ell\\) on x-axis, \\(r_{\\ell}\\) on y-axis.\nTells us if correlations are “significantly large” or “significantly small” for certain lags\nTo make an autocorrelation plot, we use the acf() function; the ggplot version uses autoplot()"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#autocorrelation-plots",
    "href": "lectures/08-tsne-trends-time-series.html#autocorrelation-plots",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Autocorrelation plots",
    "text": "Autocorrelation plots\n\nlibrary(ggfortify)\nauto_corr &lt;- acf(co2_tbl$co2_val, plot = FALSE)\nautoplot(auto_corr)"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#autocorrelation-plots-and-seasonality-1",
    "href": "lectures/08-tsne-trends-time-series.html#autocorrelation-plots-and-seasonality-1",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Autocorrelation Plots and Seasonality",
    "text": "Autocorrelation Plots and Seasonality"
  },
  {
    "objectID": "lectures/08-tsne-trends-time-series.html#recap-and-next-steps",
    "href": "lectures/08-tsne-trends-time-series.html#recap-and-next-steps",
    "title": "t-SNE + visualizing trends and time series data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nDiscussed non-linear dimension reduction with t-SNE plots\nDiscussed various aspects of visualizing trends\nWalked through basics of time series data, such as moving averages, autocorrelation, seasonality\n\n\n\nHW4 is due Wednesday Sept 25th\nYou need to email me a draft of your EDA report! (1 per group)\n\n\n\n\nNext time: Visualizing spatial data\nRecommended reading: How to Use t-SNE Effectively, Understanding UMAP, CW CH 13 Visualizing time series and other functions of an independent variable, CW CH 14 Visualizing trends\n\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "lectures/06-into-highdim.html#reminders-previously-and-today",
    "href": "lectures/06-into-highdim.html#reminders-previously-and-today",
    "title": "Into High-Dimensional Data",
    "section": "Reminders, previously, and today…",
    "text": "Reminders, previously, and today…\n\nHW3 is due Wednesday!\nHW4 is posted and due next Wednesday Sept 25th\n\n\n\nWalked through visualiziations with scatterplots (always adjust the alpha!)\nDisplayed 2D joint distributions with contours, heatmaps, and hexagonal binning\nDiscussed approaches for visualizing conditional relationships\n\n\n\nTODAY:\n\nInto high-dimensional data\nWhat type of structure do we want to capture?"
  },
  {
    "objectID": "lectures/06-into-highdim.html#back-to-the-penguins",
    "href": "lectures/06-into-highdim.html#back-to-the-penguins",
    "title": "Into High-Dimensional Data",
    "section": "Back to the penguins…",
    "text": "Back to the penguins…\nPretend I give you this penguins dataset and I ask you to make a plot for every pairwise comparison…\n\npenguins |&gt; slice(1:3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nWe can create a pairs plot to see all pairwise relationships in one plot\nPairs plot can include the various kinds of pairwise plots we’ve seen:\n\nTwo quantitative variables: scatterplot\nOne categorical, one quantitative: side-by-side violins, stacked histograms, overlaid densities\nTwo categorical: stacked bars, side-by-side bars, mosaic plots"
  },
  {
    "objectID": "lectures/06-into-highdim.html#create-pairs-plots-with-ggally",
    "href": "lectures/06-into-highdim.html#create-pairs-plots-with-ggally",
    "title": "Into High-Dimensional Data",
    "section": "Create pairs plots with GGally",
    "text": "Create pairs plots with GGally\n\nlibrary(GGally)\npenguins |&gt; ggpairs(columns = 3:6)"
  },
  {
    "objectID": "lectures/06-into-highdim.html#create-pairs-plots-with-ggally-1",
    "href": "lectures/06-into-highdim.html#create-pairs-plots-with-ggally-1",
    "title": "Into High-Dimensional Data",
    "section": "Create pairs plots with GGally",
    "text": "Create pairs plots with GGally\n\npenguins |&gt; ggpairs(columns = 3:6,\n                    mapping = aes(alpha = 0.5))"
  },
  {
    "objectID": "lectures/06-into-highdim.html#flexibility-in-customization",
    "href": "lectures/06-into-highdim.html#flexibility-in-customization",
    "title": "Into High-Dimensional Data",
    "section": "Flexibility in customization",
    "text": "Flexibility in customization\n\npenguins |&gt; \n  ggpairs(columns = c(\"bill_length_mm\", \"body_mass_g\", \"island\"),\n          mapping = aes(alpha = 0.5, color = species), \n          lower = list(\n            continuous = \"smooth_lm\", \n            combo = \"facetdensitystrip\"\n          ),\n          upper = list(\n            continuous = \"cor\",\n            combo = \"facethist\"\n          )\n  )"
  },
  {
    "objectID": "lectures/06-into-highdim.html#flexibility-in-customization-output",
    "href": "lectures/06-into-highdim.html#flexibility-in-customization-output",
    "title": "Into High-Dimensional Data",
    "section": "Flexibility in customization",
    "text": "Flexibility in customization"
  },
  {
    "objectID": "lectures/06-into-highdim.html#see-demo-03-for-more",
    "href": "lectures/06-into-highdim.html#see-demo-03-for-more",
    "title": "Into High-Dimensional Data",
    "section": "See Demo 03 for more!",
    "text": "See Demo 03 for more!"
  },
  {
    "objectID": "lectures/06-into-highdim.html#what-about-high-dimensional-data",
    "href": "lectures/06-into-highdim.html#what-about-high-dimensional-data",
    "title": "Into High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\nConsider this dataset containing nutritional information about Starbucks drinks:\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nHow do we visualize this dataset? \n\nTedious task: make a series of pairs plots (one giant pairs plot would overwhelming)"
  },
  {
    "objectID": "lectures/06-into-highdim.html#what-about-high-dimensional-data-1",
    "href": "lectures/06-into-highdim.html#what-about-high-dimensional-data-1",
    "title": "Into High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nGoals to keep in mind with visualizing high-dimensional data:\n\nVisualize structure among observations based on distances and projections (next lecture)\nVisualize structure among variables using correlation as “distance”"
  },
  {
    "objectID": "lectures/06-into-highdim.html#correlogram-to-visualize-correlation-matrix",
    "href": "lectures/06-into-highdim.html#correlogram-to-visualize-correlation-matrix",
    "title": "Into High-Dimensional Data",
    "section": "Correlogram to visualize correlation matrix",
    "text": "Correlogram to visualize correlation matrix\nUse the ggcorrplot package:\n\nstarbucks_quant_cor &lt;- cor(dplyr::select(starbucks, serv_size_m_l:caffeine_mg))\n\nlibrary(ggcorrplot)\nggcorrplot(starbucks_quant_cor)"
  },
  {
    "objectID": "lectures/06-into-highdim.html#options-to-customize-correlogram",
    "href": "lectures/06-into-highdim.html#options-to-customize-correlogram",
    "title": "Into High-Dimensional Data",
    "section": "Options to customize correlogram",
    "text": "Options to customize correlogram\n\nggcorrplot(starbucks_quant_cor,\n           type = \"lower\", method = \"circle\")"
  },
  {
    "objectID": "lectures/06-into-highdim.html#reorder-variables-based-on-correlation",
    "href": "lectures/06-into-highdim.html#reorder-variables-based-on-correlation",
    "title": "Into High-Dimensional Data",
    "section": "Reorder variables based on correlation",
    "text": "Reorder variables based on correlation\n\nggcorrplot(starbucks_quant_cor,\n           type = \"lower\", method = \"circle\",\n           hc.order = TRUE)"
  },
  {
    "objectID": "lectures/06-into-highdim.html#heatmap-displays-of-observations",
    "href": "lectures/06-into-highdim.html#heatmap-displays-of-observations",
    "title": "Into High-Dimensional Data",
    "section": "Heatmap displays of observations",
    "text": "Heatmap displays of observations\n\nheatmap(as.matrix(dplyr::select(starbucks, serv_size_m_l:caffeine_mg)),\n        scale = \"column\", \n        labRow = starbucks$product_name,\n        cexRow = .5, cexCol = .75,\n        Rowv = NA, Colv = NA)"
  },
  {
    "objectID": "lectures/06-into-highdim.html#manual-version-of-heatmaps",
    "href": "lectures/06-into-highdim.html#manual-version-of-heatmaps",
    "title": "Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps\n\nstarbucks |&gt;\n  dplyr::select(product_name, serv_size_m_l:caffeine_mg) |&gt;\n  pivot_longer(serv_size_m_l:caffeine_mg,\n               names_to = \"variable\",\n               values_to = \"raw_value\") |&gt;\n  group_by(variable) |&gt;\n  mutate(std_value = (raw_value - mean(raw_value)) / sd(raw_value)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(y = variable, x = product_name, fill = std_value)) +\n  geom_tile() +\n  theme_light() +\n  theme(axis.text.x = element_text(size = 1, angle = 45),\n        legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/06-into-highdim.html#manual-version-of-heatmaps-output",
    "href": "lectures/06-into-highdim.html#manual-version-of-heatmaps-output",
    "title": "Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps"
  },
  {
    "objectID": "lectures/06-into-highdim.html#manual-version-of-heatmaps-1",
    "href": "lectures/06-into-highdim.html#manual-version-of-heatmaps-1",
    "title": "Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps\n\nstarbucks |&gt;\n  dplyr::select(product_name, serv_size_m_l:caffeine_mg) |&gt;\n  mutate(product_name = fct_reorder(product_name, calories)) |&gt;\n  pivot_longer(serv_size_m_l:caffeine_mg,\n               names_to = \"variable\",\n               values_to = \"raw_value\") |&gt;\n  group_by(variable) |&gt;\n  mutate(std_value = (raw_value - mean(raw_value)) / sd(raw_value)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(y = variable, x = product_name, fill = std_value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") +\n  theme_light() +\n  theme(axis.text.x = element_text(size = 1, angle = 45),\n        legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/06-into-highdim.html#manual-version-of-heatmaps-1-output",
    "href": "lectures/06-into-highdim.html#manual-version-of-heatmaps-1-output",
    "title": "Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps"
  },
  {
    "objectID": "lectures/06-into-highdim.html#parallel-coordinates-plot-with-ggparcoord",
    "href": "lectures/06-into-highdim.html#parallel-coordinates-plot-with-ggparcoord",
    "title": "Into High-Dimensional Data",
    "section": "Parallel coordinates plot with ggparcoord",
    "text": "Parallel coordinates plot with ggparcoord\n\nstarbucks |&gt;\n  ggparcoord(columns = 5:15, alphaLines = .1) + #&lt;&lt;\n  theme(axis.text.x = element_text(angle = 90))"
  },
  {
    "objectID": "lectures/06-into-highdim.html#recap-and-next-steps",
    "href": "lectures/06-into-highdim.html#recap-and-next-steps",
    "title": "Into High-Dimensional Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nDiscussed creating pairs plots for initial inspection of several variables\nBegan thinking about ways to displays dataset structure via correlations\nUsed heatmaps and parallel coordinates plot to capture observation and variable structure\n\n\n\nHW3 is due Wednesday!\nHW4 is posted due next Wednesday Sept 25th\n\n\n\n\nNext time: More high-dimensional data\nRecommended reading:\nCW Chapter 12 Visualizing associations among two or more quantitative variables\n\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#reminders-previously-and-today",
    "href": "lectures/03-2dcategorical-1dquant.html#reminders-previously-and-today",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Reminders, previously, and today…",
    "text": "Reminders, previously, and today…\n\nHW1 is due TONIGHT!\nHW2 is posted (due next Wednesday)\n\n\n\nDiscussed data visualization principles and the role of infographics\nVisualized 1D categorical data, i.e., make bar charts!\n\n\n\nTODAY:\n\nPie charts…\nVisualizing 2D categorical data\nBegin 1D quantitative data"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#so-you-want-to-make-pie-charts",
    "href": "lectures/03-2dcategorical-1dquant.html#so-you-want-to-make-pie-charts",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "So you want to make pie charts…",
    "text": "So you want to make pie charts…\n\npenguins |&gt; \n  ggplot(aes(fill = species, x = \"\")) + \n  geom_bar(aes(y = after_stat(count))) +\n  coord_polar(theta = \"y\") +\n  theme_void()"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#friends-dont-let-friends-make-pie-charts",
    "href": "lectures/03-2dcategorical-1dquant.html#friends-dont-let-friends-make-pie-charts",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Friends Don’t Let Friends Make Pie Charts",
    "text": "Friends Don’t Let Friends Make Pie Charts"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#waffle-charts-are-cooler-anyway",
    "href": "lectures/03-2dcategorical-1dquant.html#waffle-charts-are-cooler-anyway",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Waffle charts are cooler anyway…",
    "text": "Waffle charts are cooler anyway…\n\nlibrary(waffle)\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  ggplot(aes(fill = species, values = count)) +\n  geom_waffle(n_rows = 20, color = \"white\", flip = TRUE) +\n  coord_equal() +\n  theme_void()"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#d-categorical-basics-marginal-conditional-distribution",
    "href": "lectures/03-2dcategorical-1dquant.html#d-categorical-basics-marginal-conditional-distribution",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "2D categorical basics: marginal / conditional distribution",
    "text": "2D categorical basics: marginal / conditional distribution\n\naddmargins(table(\"Species\" = penguins$species, \"Island\" = penguins$island))\n\n           Island\nSpecies     Biscoe Dream Torgersen Sum\n  Adelie        44    56        52 152\n  Chinstrap      0    68         0  68\n  Gentoo       124     0         0 124\n  Sum          168   124        52 344\n\n\n\nColumn and row sums: marginal distributions\nValues within rows: conditional distribution for Island given Species\nValues within columns: conditional distribution for Species given Island\nBottom right: total number of observations"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#connecting-distributions-to-visualizations",
    "href": "lectures/03-2dcategorical-1dquant.html#connecting-distributions-to-visualizations",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Connecting distributions to visualizations",
    "text": "Connecting distributions to visualizations\nFive distributions for two categorical variables \\(A\\) and \\(B\\):\n\nMarginals: \\(P(A)\\) and \\(P(B)\\)\nConditionals: \\(P(A | B)\\) and \\(P(B|A)\\)\nJoint: \\(P(A, B)\\)\n\nWe use bar charts to visualize marginal distributions for categorical variables…\n\nAnd we’ll use more bar charts to visualize conditional and joint distributions!"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#stacked-bar-charts---a-bar-chart-of-spine-charts",
    "href": "lectures/03-2dcategorical-1dquant.html#stacked-bar-charts---a-bar-chart-of-spine-charts",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Stacked bar charts - a bar chart of spine charts",
    "text": "Stacked bar charts - a bar chart of spine charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) +\n  geom_bar() + \n  theme_bw()\n\n\n\n\nEasy to see marginal of species, i.e., \\(P(\\) x \\()\\)\nCan see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nHarder to see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#side-by-side-bar-charts",
    "href": "lectures/03-2dcategorical-1dquant.html#side-by-side-bar-charts",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Side-by-side bar charts",
    "text": "Side-by-side bar charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\n\n\n\nEasy to see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nCan see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#side-by-side-bar-charts-1",
    "href": "lectures/03-2dcategorical-1dquant.html#side-by-side-bar-charts-1",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Side-by-side bar charts",
    "text": "Side-by-side bar charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) + \n  geom_bar(position = position_dodge(preserve = \"single\")) +\n  theme_bw()\n\n\n\n\nEasy to see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nCan see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#complete-missing-values-to-preserve-location",
    "href": "lectures/03-2dcategorical-1dquant.html#complete-missing-values-to-preserve-location",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Complete missing values to preserve location",
    "text": "Complete missing values to preserve location\n\npenguins |&gt;\n  count(species, island) |&gt;\n  complete(species = unique(species), island = unique(island), \n           fill = list(n = 0)) |&gt;\n  ggplot(aes(x = species, y = n, fill = island)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#what-do-you-prefer",
    "href": "lectures/03-2dcategorical-1dquant.html#what-do-you-prefer",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "What do you prefer?",
    "text": "What do you prefer?"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#chi-squared-test-for-1d-categorical-data",
    "href": "lectures/03-2dcategorical-1dquant.html#chi-squared-test-for-1d-categorical-data",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Chi-squared test for 1D categorical data:",
    "text": "Chi-squared test for 1D categorical data:\n\n\nNull hypothesis \\(H_0\\): \\(p_1 = p_2 = \\dots = p_K\\), compute the test statistic:\n\n\\[\n\\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\n\\]\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\): expected counts under \\(H_0\\), i.e., each category is equally to occur \\(n / K = p_1 = p_2 = \\dots = p_K\\)\n\n\n\n\nchisq.test(table(penguins$species))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$species)\nX-squared = 31.907, df = 2, p-value = 1.179e-07"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#hypothesis-testing-review",
    "href": "lectures/03-2dcategorical-1dquant.html#hypothesis-testing-review",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Hypothesis testing review",
    "text": "Hypothesis testing review\n\n\n\nComputing \\(p\\)-values works like this:\n\nChoose a test statistic.\nCompute the test statistic in your dataset.\nIs test statistic “unusual” compared to what I would expect under \\(H_0\\)?\nCompare \\(p\\)-value to target error rate \\(\\alpha\\) (typically referred to as target level \\(\\alpha\\) )\nTypically choose \\(\\alpha = 0.05\\)\n\ni.e., if we reject null hypothesis at \\(\\alpha = 0.05\\) then, assuming \\(H_0\\) is true, there is a 5% chance it is a false positive (aka Type 1 error)"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#inference-for-2d-categorical-data",
    "href": "lectures/03-2dcategorical-1dquant.html#inference-for-2d-categorical-data",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Inference for 2D categorical data",
    "text": "Inference for 2D categorical data\n\nAgain we use the chi-squared test:\n\nNull hypothesis \\(H_0\\): variables \\(A\\) and \\(B\\) are independent, compute the test statistic:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\\(O_{ij}\\): observed counts in contingency table\n\\(E_{ij}\\): expected counts under \\(H_0\\)\n\n\\[\n\\begin{aligned}\nE_{ij} &= n \\cdot P(A = a_i, B = b_j) \\\\\n&= n \\cdot P(A = a_i) P(B = b_j) \\\\\n&= n \\cdot \\left( \\frac{n_{i \\cdot}}{n} \\right) \\left( \\frac{ n_{\\cdot j}}{n} \\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#inference-for-2d-categorical-data-1",
    "href": "lectures/03-2dcategorical-1dquant.html#inference-for-2d-categorical-data-1",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Inference for 2D categorical data",
    "text": "Inference for 2D categorical data\n\nAgain we use the chi-squared test:\n\nNull hypothesis \\(H_0\\): variables \\(A\\) and \\(B\\) are independent, compute the test statistic:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\\(O_{ij}\\): observed counts in contingency table\n\\(E_{ij}\\): expected counts under \\(H_0\\)\n\n\n\nchisq.test(table(penguins$species, penguins$island))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(penguins$species, penguins$island)\nX-squared = 299.55, df = 4, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#visualize-independence-test-with-mosaic-plots",
    "href": "lectures/03-2dcategorical-1dquant.html#visualize-independence-test-with-mosaic-plots",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Visualize independence test with mosaic plots",
    "text": "Visualize independence test with mosaic plots\n\nTwo variables are independent if knowing the level of one tells us nothing about the other\n\ni.e. \\(P(A | B) = P(A)\\), and that \\(P(A, B) = P(A) \\times P(B)\\)\n\nCreate a mosaic plot using base R\n\n\nmosaicplot(table(penguins$species, penguins$island))"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#shade-by-pearson-residuals",
    "href": "lectures/03-2dcategorical-1dquant.html#shade-by-pearson-residuals",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Shade by Pearson residuals",
    "text": "Shade by Pearson residuals\n\n\nThe test statistic is:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\nDefine the Pearson residuals as:\n\n\\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}}}\\]\n\nSide-note: In general, Pearson residuals are \\(\\frac{\\text{residuals}}{\\sqrt{\\text{variance}}}\\)\n\n\n\n\n\n\\(r_{ij} \\approx 0 \\rightarrow\\) observed counts are close to expected counts\n\\(|r_{ij}| &gt; 2 \\rightarrow\\) “significant” at level \\(\\alpha = 0.05\\).\nVery positive \\(r_{ij} \\rightarrow\\) more than expected, while very negative \\(r_{ij} \\rightarrow\\) fewer than expected\nColor by Pearson residuals to tell us which combos are much bigger/smaller than expected."
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#bonus-treemaps-do-not-require-same-categorical-levels-across-subgroups",
    "href": "lectures/03-2dcategorical-1dquant.html#bonus-treemaps-do-not-require-same-categorical-levels-across-subgroups",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Bonus: Treemaps do not require same categorical levels across subgroups",
    "text": "Bonus: Treemaps do not require same categorical levels across subgroups\n\nlibrary(treemapify)\npenguins |&gt;\n  group_by(species, island) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  ggplot(aes(area = count, subgroup = island,\n             label = species,\n             fill = interaction(species, island))) +\n  # 1. Draw species borders and fill colors\n  geom_treemap() +\n  # 2. Draw island borders\n  geom_treemap_subgroup_border() +\n  # 3. Print island text\n  geom_treemap_subgroup_text(place = \"centre\", grow = T, \n                             alpha = 0.5, colour = \"black\",\n                             fontface = \"italic\", min.size = 0) +\n  # 4. Print species text\n  geom_treemap_text(colour = \"white\", place = \"topleft\", \n                    reflow = T) +\n  guides(colour = \"none\", fill = \"none\")"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#bonus-treemaps-do-not-require-same-categorical-levels-across-subgroups-output",
    "href": "lectures/03-2dcategorical-1dquant.html#bonus-treemaps-do-not-require-same-categorical-levels-across-subgroups-output",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Bonus: Treemaps do not require same categorical levels across subgroups",
    "text": "Bonus: Treemaps do not require same categorical levels across subgroups"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#d-quantitative-data",
    "href": "lectures/03-2dcategorical-1dquant.html#d-quantitative-data",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "1D Quantitative Data",
    "text": "1D Quantitative Data\nObservations are collected into a vector \\((x_1, \\dots, x_n)\\), \\(x_i \\in \\mathbb{R}\\) (or \\(\\mathbb{R}^+\\), \\(\\mathbb{Z}\\))\nCommon summary statistics for 1D quantitative data:\n\n\nCenter: Mean, median, weighted mean, mode\n\nRelated to the first moment, i.e., \\(\\mathbb{E}[X]\\)\n\n\n\n\n\nSpread: Variance, range, min/max, quantiles, IQR\n\nRelated to the second moment, i.e., \\(\\mathbb{E}[X^2]\\)\n\n\n\n\n\nShape: symmetry, skew, kurtosis (“peakedness”)\n\nRelated to higher order moments, i.e., skewness is \\(\\mathbb{E}[X^3]\\), kurtosis is \\(\\mathbb{E}[X^4]\\)\n\n\nCompute various statistics with summary(), mean(), median(), quantile(), range(), sd(), var(), etc."
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#box-plots-visualize-summary-statistics",
    "href": "lectures/03-2dcategorical-1dquant.html#box-plots-visualize-summary-statistics",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Box plots visualize summary statistics",
    "text": "Box plots visualize summary statistics\n\npenguins |&gt;\n  ggplot(aes(y = flipper_length_mm)) +\n  geom_boxplot(aes(x = \"\")) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#histograms-display-1d-continuous-distributions",
    "href": "lectures/03-2dcategorical-1dquant.html#histograms-display-1d-continuous-distributions",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Histograms display 1D continuous distributions",
    "text": "Histograms display 1D continuous distributions\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) +\n  geom_histogram()"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#do-not-rely-on-box-plots",
    "href": "lectures/03-2dcategorical-1dquant.html#do-not-rely-on-box-plots",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Do NOT rely on box plots…",
    "text": "Do NOT rely on box plots…"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#what-do-visualizations-of-continuous-distributions-display",
    "href": "lectures/03-2dcategorical-1dquant.html#what-do-visualizations-of-continuous-distributions-display",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "What do visualizations of continuous distributions display?",
    "text": "What do visualizations of continuous distributions display?\nProbability that continuous variable \\(X\\) takes a particular value is 0\ne.g., \\(P\\) (flipper_length_mm \\(= 200\\)) \\(= 0\\), why?\n\nInstead we use the probability density function (PDF) to provide a relative likelihood\nFor continuous variables we can use the cumulative distribution function (CDF),\n\\[\nF(x) = P(X \\leq x)\n\\]\n\n\nFor \\(n\\) observations we can easily compute the Empirical CDF (ECDF):\n\\[\\hat{F}_n(x)  = \\frac{\\text{# obs. with variable} \\leq x}{n} = \\frac{1}{n} \\sum_{i=1}^{n}1(x_i \\leq x)\\]\n\nwhere \\(1()\\) is the indicator function, i.e. ifelse(x_i &lt;= x, 1, 0)"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#display-full-distribution-with-ecdf-plot",
    "href": "lectures/03-2dcategorical-1dquant.html#display-full-distribution-with-ecdf-plot",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Display full distribution with ECDF plot",
    "text": "Display full distribution with ECDF plot\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  stat_ecdf() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#whats-the-relationship-between-these-two-figures",
    "href": "lectures/03-2dcategorical-1dquant.html#whats-the-relationship-between-these-two-figures",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "What’s the relationship between these two figures?",
    "text": "What’s the relationship between these two figures?"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#what-about-comparing-to-theoretical-distributions",
    "href": "lectures/03-2dcategorical-1dquant.html#what-about-comparing-to-theoretical-distributions",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "What about comparing to theoretical distributions?",
    "text": "What about comparing to theoretical distributions?"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#one-sample-kolmogorov-smirnov-test",
    "href": "lectures/03-2dcategorical-1dquant.html#one-sample-kolmogorov-smirnov-test",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "One-Sample Kolmogorov-Smirnov Test",
    "text": "One-Sample Kolmogorov-Smirnov Test\n\nWe compare the ECDF \\(\\hat{F}(x)\\) to a theoretical distribution’s CDF \\(F(x)\\)\nThe one sample KS test statistic is: \\(\\text{max}_x |\\hat{F}(x) - F(x)|\\)"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#flipper-length-example",
    "href": "lectures/03-2dcategorical-1dquant.html#flipper-length-example",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Flipper length example",
    "text": "Flipper length example\nWhat if we assume flipper_length_mm follows Normal distribution?\n\ni.e., flipper_length_mm \\(\\sim N(\\mu, \\sigma^2)\\)\n\nNeed estimates for mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\nflipper_length_mean &lt;- mean(penguins$flipper_length_mm, na.rm = TRUE)\nflipper_length_sd &lt;- sd(penguins$flipper_length_mm, na.rm = TRUE)\n\n\nPerform one-sample KS test using ks.test():\n\nks.test(x = penguins$flipper_length_mm, y = \"pnorm\",\n        mean = flipper_length_mean, sd = flipper_length_sd)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  penguins$flipper_length_mm\nD = 0.12428, p-value = 5.163e-05\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#flipper-length-example-1",
    "href": "lectures/03-2dcategorical-1dquant.html#flipper-length-example-1",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Flipper length example",
    "text": "Flipper length example"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#recap-and-next-steps",
    "href": "lectures/03-2dcategorical-1dquant.html#recap-and-next-steps",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nVisualize categorical data with bars! Regular, stacked, side-by-side, mosaic\nDisplay uncertainty: (1D) standard errors, (2D) Pearson residuals\nVisualize 1D quantitative data with histograms, ECDFs, but never use a box plot by itself\n\n\n\nHW1 is due TONIGHT!\nHW2 is posted (due next Wednesday)\n\n\n\n\nNext time: Density Estimation\nRecommended reading:\nCW Chapter 11 Visualizing nested proportions, CW Chapter 7 Visualizing distributions: Histograms and density plots, CW Chapter 8 Visualizing distributions: Empirical cumulative distribution functions and q-q plots"
  },
  {
    "objectID": "lectures/03-2dcategorical-1dquant.html#bonus-visualizing-the-ks-test-statistic",
    "href": "lectures/03-2dcategorical-1dquant.html#bonus-visualizing-the-ks-test-statistic",
    "title": "Visualizations for 2D Categorical and 1D Quantitative Data",
    "section": "BONUS: Visualizing the KS test statistic",
    "text": "BONUS: Visualizing the KS test statistic\n\n# First create the ECDF function for the variable:\nfl_ecdf &lt;- ecdf(penguins$flipper_length_mm)\n# Compute the absolute value of the differences between the ECDF for the values\n# and the theoretical values with assumed Normal distribution:\nabs_ecdf_diffs &lt;- abs(fl_ecdf(penguins$flipper_length_mm) - pnorm(penguins$flipper_length_mm,\n                                                                  mean = flipper_length_mean, sd = flipper_length_sd))\n# Now find where the maximum difference is:\nmax_abs_ecdf_diff_i &lt;- which.max(abs_ecdf_diffs)\n# Get this flipper length value:\nmax_fl_diff_value &lt;- penguins$flipper_length_mm[max_abs_ecdf_diff_i]\n# Plot the ECDF with the theoretical Normal and KS test info:\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) +\n  stat_ecdf(color = \"darkblue\") +\n  # Use stat_function to draw the Normal ECDF\n  stat_function(fun = pnorm, args = list(mean = flipper_length_mean, sd = flipper_length_sd), color = \"black\", linetype = \"dashed\") +\n  # Draw KS test line:\n  geom_vline(xintercept = max_fl_diff_value, color = \"red\") +\n  # Add text with the test results (x and y are manually entered locations)\n  annotate(geom = \"text\", x = 215, y = .25, label = \"KS test stat = 0.12428\\np-value = 5.163e-05\") + \n  labs(x = \"Flipper length (mm)\", y = \"Fn(x)\") + theme_bw()\n\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "demos.html",
    "href": "demos.html",
    "title": "Demos",
    "section": "",
    "text": "Demo\nDate\nTitle\nDemo file\n\n\n\n\n1\nAug 26\nInto the tidyverse\nHTML\n\n\n2\nSept 11\nScatterplots and regression with categorical variables\nHTML\n\n\n3\nSept 16\nSimple visuals for high-dimensional data\nHTML\n\n\n4\nSept 18\nMulti-Dimensional Scaling\nHTML\n\n\n5\nSept 18\nPrincipal Component Analysis\nHTML\n\n\n6\nSept 18\nIntroduction to t-SNE and UMAP\nHTML\n\n\n7\nSept 23\nVisualizing trends and time series data\nHTML\n\n\n8\nSept 25\nVisualizations and inference for spatial data\nHTML\n\n\n9\nSept 30\nVisualizations for areal data\nHTML\n\n\n10\nSept 30\nModifying colors and themes\nHTML\n\n\n11\nOct 7\nVisualizing text data\nHTML",
    "crumbs": [
      "Demos"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MADS Data Visualization 36-613",
    "section": "",
    "text": "This is the companion website for MADS Data Visualization course 36-613. While all of the assignments will be posted on Canvas, this website provides an alternative way to access lecture materials and additional demo files (see the see side-bar).\nLectures are on Mondays and Wednesdays from 9:30 - 10:50 AM ET, located in WEH 4709.\nOffice hours schedule:\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nProf Yurko\nM/W 11 AM - 12 PM\nBaker Hall 132D\n\n\nAnna Rosengart\nTuesday 1 - 2 PM\nZoom\n\n\n\nThere are no required textbooks for this course, but the following are useful free resources that I will sometimes refer to as recommended reading:\n\nR for Data Science\nData Visualization: A Practical Introduction\nFundamentals of Data Visualization\nggplot2: Elegant Graphics for Data Analysis\nMastering Shiny\n\nAnd the following are interesting data visualization and infographics websites:\n\nFlowingData\nHistory of Data Visualization\nAlberto Cairo’s substack\nFriends Don’t Let Friends Make Bad Graphs",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lectures/01-intro.html#who-am-i",
    "href": "lectures/01-intro.html#who-am-i",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Who am I?",
    "text": "Who am I?\n\n\n\n\nAssistant Teaching Professor\nFinished Phd in Statistics @ CMU in May 2022\nPreviously BS in Statistics @ CMU in 2015\nResearch interests: sports analytics, natural language processing, clustering, selective inference\n\n\n\n\n\nIndustry experience: finance before returning to grad school and also as data scientist in professional sports"
  },
  {
    "objectID": "lectures/01-intro.html#why-do-we-visualize-data",
    "href": "lectures/01-intro.html#why-do-we-visualize-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Why do we visualize data?",
    "text": "Why do we visualize data?"
  },
  {
    "objectID": "lectures/01-intro.html#course-structure",
    "href": "lectures/01-intro.html#course-structure",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nLectures on Mondays/Wednesdays\n\nWill include example code, all slides and additional R demos posted on https://ryurko.github.io/mads-36613-fall24/\nWant class discussion - so participate and ask questions!\n\nFour homework assignments due Wednesdays by 11:59 PM ET\n\nPosted Monday mornings and due Wednesday of the following week\n\n\n\n\n\nGroup EDA Report due Friday October 4th by 5:00 PM ET\n\nEach group will write an IMRD report and present their work in 36-611\n\nIndividual Infographics due Friday October 11th by 11:59 PM ET\n\nYou will create a high-quality, single page infographic with dataset of your choice\nFirst rough draft for peer feedback due Wednesday Oct 2nd"
  },
  {
    "objectID": "lectures/01-intro.html#important-hw0-and-genai-module-in-hw1",
    "href": "lectures/01-intro.html#important-hw0-and-genai-module-in-hw1",
    "title": "Introduction and the Grammar of Graphics",
    "section": "IMPORTANT! HW0 and GenAI module in HW1",
    "text": "IMPORTANT! HW0 and GenAI module in HW1\n\nAs seen in today’s Canvas announcement - you must submit HW0 by Thursday night!\n\nThis is just to make sure you have everything installed correctly and can render .qmd files to PDF\n\n\n\n\nHW1 is posted already, since you will complete a Generative AI Learning Module Assignment\nAll you need to do is follow the steps in the Fostering GenAI Literacy Canvas Module: Student Information by completing the tasks in order before their respective deadlines in order to receive full credit:\n\nKnowledge Check: Opens on Tuesday August 27 at 12:00 AM and is due Wednesday August 28 by 11:59 PM. This must be completed in one sitting (open for 2 hours in total, but should only take 10-20 minutes).\nLearning Modules: Opens on Thursday August 29 at 12:00 AM and is due Friday August 30 by 11:59 PM. This can be completed over multiple sessions.\nKnowledge Review: Opens on Saturday August 31 at 12:00 AM and is due Sunday September 1 by 11:59 PM. This must be completed in one sitting (open for 2 hours in total, but should only take 10-20 minutes)."
  },
  {
    "objectID": "lectures/01-intro.html#course-objectives",
    "href": "lectures/01-intro.html#course-objectives",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course Objectives",
    "text": "Course Objectives\nPractice the Fundamentals of Tidy Data Wrangling and Reproducible Workflows.\n\nPractice tidy data manipulation in R using the tidyverse with consistent code style\n\nCreate High-Quality Data Visualizations and Infographics.\n\nMaster the use of R and ggplot2 to create data visualizations and infographics that are easily readable and understandable for technical and non-technical audiences\n\nCritique and Write About Data Visualizations and Infographics.\n\nGive useful critiques, feedback, and suggestions for improvement on others’ graphics"
  },
  {
    "objectID": "lectures/01-intro.html#what-do-i-mean-by-tidy-data",
    "href": "lectures/01-intro.html#what-do-i-mean-by-tidy-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "What do I mean by tidy data?",
    "text": "What do I mean by tidy data?\nData are often stored in tabular (or matrix) form:\n\nlibrary(palmerpenguins)\npenguins |&gt; slice(1:5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-intro.html#the-grammar-of-graphics",
    "href": "lectures/01-intro.html#the-grammar-of-graphics",
    "title": "Introduction and the Grammar of Graphics",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\n\nOriginally defined by Leland Wilkinson\n\n\ndata\ngeometries: type of geometric objects to represent data, e.g., points, lines\naesthetics: visual characteristics of geometric objects to represent data, e.g., position, size\nscales: how each aesthetic is converted into values on the graph, e.g., color scales\nstats: statistical transformations to summarize data, e.g., counts, means, regression lines\nfacets: split data and view as multiple graphs\ncoordinate system: 2D space the data are projected onto, e.g., Cartesian coordinates\n\n\n\n\nHadley Wickham created ggplot2\n\n\ndata\ngeom\naes: mappings of columns to geometric objects\nscale: one scale for each aes variable\nstat\nfacet\ncoord\nlabs: labels/guides for each variable and other parts of the plot, e.g., title, subtitle, caption\ntheme: customization of plot layout"
  },
  {
    "objectID": "lectures/01-intro.html#start-with-the-data",
    "href": "lectures/01-intro.html#start-with-the-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Start with the data",
    "text": "Start with the data\n\n\nAccess ggplot2 from the tidyverse:\n\nlibrary(tidyverse)\nggplot(data = penguins)\n\n\nOr equivalently using |&gt;:\n\npenguins |&gt;\n  ggplot()"
  },
  {
    "objectID": "lectures/01-intro.html#need-to-add-geometric-objects",
    "href": "lectures/01-intro.html#need-to-add-geometric-objects",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Need to add geometric objects!",
    "text": "Need to add geometric objects!\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, \n             y = bill_depth_mm)) + \n  geom_point()\n\n\n\npenguins %&gt;%\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm)) + \n  geom_point()"
  },
  {
    "objectID": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on",
    "href": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Modify scale, add statistical summary, and so on…",
    "text": "Modify scale, add statistical summary, and so on…\n\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm,\n             y = bill_depth_mm)) + \n  # Adjust alpha of points\n  geom_point(alpha = 0.5) +\n  # Add smooth regression line\n  stat_smooth(method = \"lm\") + \n  # Flip the x-axis scale\n  scale_x_reverse() + \n  # Change title & axes labels \n  labs(x = \"Bill length (mm)\", \n       y = \"Bill depth (mm)\", \n       title = \"Clustering of penguins bills\") + \n  # Change the theme:\n  theme_bw() +\n  # Update font size of text:\n  theme(axis.title = element_text(size = 12),\n        plot.title = element_text(size = 16))"
  },
  {
    "objectID": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on-output",
    "href": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on-output",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Modify scale, add statistical summary, and so on…",
    "text": "Modify scale, add statistical summary, and so on…"
  },
  {
    "objectID": "lectures/01-intro.html#in-the-beginning",
    "href": "lectures/01-intro.html#in-the-beginning",
    "title": "Introduction and the Grammar of Graphics",
    "section": "In the beginning…",
    "text": "In the beginning…\n\nMichael Florent van Langren published the first (known) statistical graphic in 1644\n\n\n\n\n\n\nPlots different estimates of the longitudinal distance between Toledo, Spain and Rome, Italy\ni.e., visualization of collected data to aid in estimation of parameter"
  },
  {
    "objectID": "lectures/01-intro.html#john-snow-knows-something-about-cholera",
    "href": "lectures/01-intro.html#john-snow-knows-something-about-cholera",
    "title": "Introduction and the Grammar of Graphics",
    "section": "John Snow Knows Something About Cholera",
    "text": "John Snow Knows Something About Cholera"
  },
  {
    "objectID": "lectures/01-intro.html#charles-minards-map-of-napoleons-russian-disaster",
    "href": "lectures/01-intro.html#charles-minards-map-of-napoleons-russian-disaster",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Charles Minard’s Map of Napoleon’s Russian Disaster",
    "text": "Charles Minard’s Map of Napoleon’s Russian Disaster"
  },
  {
    "objectID": "lectures/01-intro.html#florence-nightingales-rose-diagram",
    "href": "lectures/01-intro.html#florence-nightingales-rose-diagram",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Florence Nightingale’s Rose Diagram",
    "text": "Florence Nightingale’s Rose Diagram"
  },
  {
    "objectID": "lectures/01-intro.html#milestones-in-data-visualization-history",
    "href": "lectures/01-intro.html#milestones-in-data-visualization-history",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Milestones in Data Visualization History",
    "text": "Milestones in Data Visualization History"
  },
  {
    "objectID": "lectures/01-intro.html#edward-tuftes-principles-of-data-visualization",
    "href": "lectures/01-intro.html#edward-tuftes-principles-of-data-visualization",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Edward Tufte’s Principles of Data Visualization",
    "text": "Edward Tufte’s Principles of Data Visualization\nGraphics: visually display measured quantities by combining points, lines, coordinate systems, numbers, symbols, words, shading, color\nOften our goal is to show data and/or communicate a story\n\n\nInduce viewer to think about substance, not graphical methodology\nMake large, complex datasets more coherent\nEncourage comparison of different pieces of data\nDescribe, explore, and identify relationships\nAvoid data distortion and data decoration\nUse consistent graph design\n\n\n\nAvoid graphs that lead to misleading conclusions!"
  },
  {
    "objectID": "lectures/01-intro.html#how-to-fail-this-class",
    "href": "lectures/01-intro.html#how-to-fail-this-class",
    "title": "Introduction and the Grammar of Graphics",
    "section": "How to Fail this Class:",
    "text": "How to Fail this Class:"
  },
  {
    "objectID": "lectures/01-intro.html#what-about-this-spiral",
    "href": "lectures/01-intro.html#what-about-this-spiral",
    "title": "Introduction and the Grammar of Graphics",
    "section": "What about this spiral?",
    "text": "What about this spiral?\n\n\nRequires distortion"
  },
  {
    "objectID": "lectures/01-intro.html#infographics-to-communicate-a-story-check-out-flowingdata-for-more-examples",
    "href": "lectures/01-intro.html#infographics-to-communicate-a-story-check-out-flowingdata-for-more-examples",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Infographics to communicate a story (check out FlowingData for more examples)",
    "text": "Infographics to communicate a story (check out FlowingData for more examples)"
  },
  {
    "objectID": "lectures/01-intro.html#alberto-cairo-and-the-art-of-insight",
    "href": "lectures/01-intro.html#alberto-cairo-and-the-art-of-insight",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Alberto Cairo and the art of insight",
    "text": "Alberto Cairo and the art of insight"
  },
  {
    "objectID": "lectures/01-intro.html#recap-and-next-steps",
    "href": "lectures/01-intro.html#recap-and-next-steps",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWalked through course logistics (READ THE SYLLABUS)\nIntroduced the Grammar of Graphics and ggplot2 basics\nDiscussed data visualization principles and the role of infographics\n\n\n\nHW1 is posted - complete the GenAI Literacy module ON TIME!\nComplete HW0 by Thursday night! Confirms you have everything installed and can render .qmd files to PDF via tinytex\n\n\n\n\nNext time: Visualizing categorical data\nRecommended reading:\n\nCW Chapter 2 Visualizing data: Mapping data onto aesthetics, CW Chapter 17 The principle of proportional ink\nKH Chapter 1 Look at data, KH Chapter 3 Make a plot\n\n\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#reminders-previously-and-today",
    "href": "lectures/04-quant-distributions.html#reminders-previously-and-today",
    "title": "Visualizing Quantitative Distributions",
    "section": "Reminders, previously, and today…",
    "text": "Reminders, previously, and today…\n\nHW2 is due Wednesday Sept 11th!\nHW3 is posted and due next Wednesday Sept 18th\n\n\n\nVisualized 2D categorical data with more bar charts, include mosaic plots\nWalked through different approaches for 1D quantitative data visualization\n\n\n\nTODAY:\n\nThinking carefully about histograms\nIntroduction to density estimation\nVisualization 1D quantitative by 1D categorical distributions"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#simulate-data-from-mixture-of-normal-distributions",
    "href": "lectures/04-quant-distributions.html#simulate-data-from-mixture-of-normal-distributions",
    "title": "Visualizing Quantitative Distributions",
    "section": "Simulate data from mixture of Normal distributions",
    "text": "Simulate data from mixture of Normal distributions\nWill sample 100 draws from \\(N(-1.5, 1)\\) and 100 draws from \\(N(1.5, 1)\\)"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#revisit-histograms",
    "href": "lectures/04-quant-distributions.html#revisit-histograms",
    "title": "Visualizing Quantitative Distributions",
    "section": "Revisit histograms",
    "text": "Revisit histograms\n\nset.seed(2024)\nfake_data &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-happens-as-we-change-the-number-of-bins",
    "href": "lectures/04-quant-distributions.html#what-happens-as-we-change-the-number-of-bins",
    "title": "Visualizing Quantitative Distributions",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 15) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-happens-as-we-change-the-number-of-bins-1",
    "href": "lectures/04-quant-distributions.html#what-happens-as-we-change-the-number-of-bins-1",
    "title": "Visualizing Quantitative Distributions",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 60) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-happens-as-we-change-the-number-of-bins-2",
    "href": "lectures/04-quant-distributions.html#what-happens-as-we-change-the-number-of-bins-2",
    "title": "Visualizing Quantitative Distributions",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 5) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-happens-as-we-change-the-number-of-bins-3",
    "href": "lectures/04-quant-distributions.html#what-happens-as-we-change-the-number-of-bins-3",
    "title": "Visualizing Quantitative Distributions",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 100) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#variability-of-graphs---30-bins",
    "href": "lectures/04-quant-distributions.html#variability-of-graphs---30-bins",
    "title": "Visualizing Quantitative Distributions",
    "section": "Variability of graphs - 30 bins",
    "text": "Variability of graphs - 30 bins\n\nset.seed(2024)\nfake_data &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-happens-with-a-different-sample",
    "href": "lectures/04-quant-distributions.html#what-happens-with-a-different-sample",
    "title": "Visualizing Quantitative Distributions",
    "section": "What happens with a different sample?",
    "text": "What happens with a different sample?\n\nset.seed(1979)\nfake_data2 &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data2 |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#variability-of-graphs---15-bins",
    "href": "lectures/04-quant-distributions.html#variability-of-graphs---15-bins",
    "title": "Visualizing Quantitative Distributions",
    "section": "Variability of graphs - 15 bins",
    "text": "Variability of graphs - 15 bins"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#variability-of-graphs---5-bins",
    "href": "lectures/04-quant-distributions.html#variability-of-graphs---5-bins",
    "title": "Visualizing Quantitative Distributions",
    "section": "Variability of graphs - 5 bins",
    "text": "Variability of graphs - 5 bins"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#variability-of-graphs---100-bins",
    "href": "lectures/04-quant-distributions.html#variability-of-graphs---100-bins",
    "title": "Visualizing Quantitative Distributions",
    "section": "Variability of graphs - 100 bins",
    "text": "Variability of graphs - 100 bins"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-do-visualizations-of-continuous-distributions-display",
    "href": "lectures/04-quant-distributions.html#what-do-visualizations-of-continuous-distributions-display",
    "title": "Visualizing Quantitative Distributions",
    "section": "What do visualizations of continuous distributions display?",
    "text": "What do visualizations of continuous distributions display?\nProbability that continuous variable \\(X\\) takes a particular value is 0\ne.g., \\(P\\) (flipper_length_mm \\(= 200\\)) \\(= 0\\), why?\n\nInstead we use the probability density function (PDF) to provide a relative likelihood\nFor continuous variables we can use the cumulative distribution function (CDF),\n\\[\nF(x) = P(X \\leq x)\n\\]\n\n\nFor \\(n\\) observations we can easily compute the Empirical CDF (ECDF):\n\\[\\hat{F}_n(x)  = \\frac{\\text{# obs. with variable} \\leq x}{n} = \\frac{1}{n} \\sum_{i=1}^{n}1(x_i \\leq x)\\]\n\nwhere \\(1()\\) is the indicator function, i.e. ifelse(x_i &lt;= x, 1, 0)"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#display-full-distribution-with-ecdf-plot",
    "href": "lectures/04-quant-distributions.html#display-full-distribution-with-ecdf-plot",
    "title": "Visualizing Quantitative Distributions",
    "section": "Display full distribution with ECDF plot",
    "text": "Display full distribution with ECDF plot\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  stat_ecdf() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-about-comparing-to-theoretical-distributions",
    "href": "lectures/04-quant-distributions.html#what-about-comparing-to-theoretical-distributions",
    "title": "Visualizing Quantitative Distributions",
    "section": "What about comparing to theoretical distributions?",
    "text": "What about comparing to theoretical distributions?"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#one-sample-kolmogorov-smirnov-test",
    "href": "lectures/04-quant-distributions.html#one-sample-kolmogorov-smirnov-test",
    "title": "Visualizing Quantitative Distributions",
    "section": "One-Sample Kolmogorov-Smirnov Test",
    "text": "One-Sample Kolmogorov-Smirnov Test\n\nWe compare the ECDF \\(\\hat{F}(x)\\) to a theoretical distribution’s CDF \\(F(x)\\)\nThe one sample KS test statistic is: \\(\\text{max}_x |\\hat{F}(x) - F(x)|\\)"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#flipper-length-example",
    "href": "lectures/04-quant-distributions.html#flipper-length-example",
    "title": "Visualizing Quantitative Distributions",
    "section": "Flipper length example",
    "text": "Flipper length example\nWhat if we assume flipper_length_mm follows Normal distribution?\n\ni.e., flipper_length_mm \\(\\sim N(\\mu, \\sigma^2)\\)\n\nNeed estimates for mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\nflipper_length_mean &lt;- mean(penguins$flipper_length_mm, na.rm = TRUE)\nflipper_length_sd &lt;- sd(penguins$flipper_length_mm, na.rm = TRUE)\n\n\nPerform one-sample KS test using ks.test():\n\nks.test(x = penguins$flipper_length_mm, y = \"pnorm\",\n        mean = flipper_length_mean, sd = flipper_length_sd)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  penguins$flipper_length_mm\nD = 0.12428, p-value = 5.163e-05\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#flipper-length-example-1",
    "href": "lectures/04-quant-distributions.html#flipper-length-example-1",
    "title": "Visualizing Quantitative Distributions",
    "section": "Flipper length example",
    "text": "Flipper length example"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-about-displaying-conditional-distributions",
    "href": "lectures/04-quant-distributions.html#what-about-displaying-conditional-distributions",
    "title": "Visualizing Quantitative Distributions",
    "section": "What about displaying conditional distributions?",
    "text": "What about displaying conditional distributions?\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(fill = species))"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-about-displaying-conditional-distributions-1",
    "href": "lectures/04-quant-distributions.html#what-about-displaying-conditional-distributions-1",
    "title": "Visualizing Quantitative Distributions",
    "section": "What about displaying conditional distributions?",
    "text": "What about displaying conditional distributions?\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#normalize-histogram-frequencies-with-density-values",
    "href": "lectures/04-quant-distributions.html#normalize-histogram-frequencies-with-density-values",
    "title": "Visualizing Quantitative Distributions",
    "section": "Normalize histogram frequencies with density values",
    "text": "Normalize histogram frequencies with density values\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(y = after_stat(density), fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#use-density-curves-instead-for-comparison",
    "href": "lectures/04-quant-distributions.html#use-density-curves-instead-for-comparison",
    "title": "Visualizing Quantitative Distributions",
    "section": "Use density curves instead for comparison",
    "text": "Use density curves instead for comparison\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(color = species))"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#we-should-not-fill-the-density-curves",
    "href": "lectures/04-quant-distributions.html#we-should-not-fill-the-density-curves",
    "title": "Visualizing Quantitative Distributions",
    "section": "We should NOT fill the density curves",
    "text": "We should NOT fill the density curves\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(fill = species), alpha = .3)"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#whats-the-relationship-between-these-two-figures",
    "href": "lectures/04-quant-distributions.html#whats-the-relationship-between-these-two-figures",
    "title": "Visualizing Quantitative Distributions",
    "section": "What’s the relationship between these two figures?",
    "text": "What’s the relationship between these two figures?"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#kernel-density-estimation",
    "href": "lectures/04-quant-distributions.html#kernel-density-estimation",
    "title": "Visualizing Quantitative Distributions",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nGoal: estimate the PDF \\(f(x)\\) for all possible values (assuming it is continuous / smooth)\n\n\\[\n\\text{Kernel density estimate: } \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\n\n\n\n\\(n =\\) sample size, \\(x =\\) new point to estimate \\(f(x)\\) (does NOT have to be in dataset!)\n\n\n\n\n\n\n\\(h =\\) bandwidth, analogous to histogram bin width, ensures \\(\\hat{f}(x)\\) integrates to 1\n\\(x_i =\\) \\(i\\)th observation in dataset\n\n\n\n\n\n\n\\(K_h(x - x_i)\\) is the Kernel function, creates weight given distance of \\(i\\)th observation from new point\n\nas \\(|x - x_i| \\rightarrow \\infty\\) then \\(K_h(x - x_i) \\rightarrow 0\\), i.e. further apart \\(i\\)th row is from \\(x\\), smaller the weight\nas bandwidth \\(h \\uparrow\\) weights are more evenly spread out (as \\(h \\downarrow\\) more concentrated around \\(x\\))\ntypically use Gaussian / Normal kernel: \\(\\propto e^{-(x - x_i)^2 / 2h^2}\\)\n\\(K_h(x - x_i)\\) is large when \\(x_i\\) is close to \\(x\\)"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#wikipedia-example",
    "href": "lectures/04-quant-distributions.html#wikipedia-example",
    "title": "Visualizing Quantitative Distributions",
    "section": "Wikipedia example",
    "text": "Wikipedia example"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#we-display-kernel-density-estimates-with-geom_density",
    "href": "lectures/04-quant-distributions.html#we-display-kernel-density-estimates-with-geom_density",
    "title": "Visualizing Quantitative Distributions",
    "section": "We display kernel density estimates with geom_density()",
    "text": "We display kernel density estimates with geom_density()\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#choice-of-kernel",
    "href": "lectures/04-quant-distributions.html#choice-of-kernel",
    "title": "Visualizing Quantitative Distributions",
    "section": "Choice of kernel?",
    "text": "Choice of kernel?"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-about-the-bandwidth",
    "href": "lectures/04-quant-distributions.html#what-about-the-bandwidth",
    "title": "Visualizing Quantitative Distributions",
    "section": "What about the bandwidth?",
    "text": "What about the bandwidth?\nUse Gaussian reference rule (rule-of-thumb) \\(\\approx 1.06 \\cdot \\sigma \\cdot n^{-1/5}\\), where \\(\\sigma\\) is the observed standard deviation\nModify the bandwidth using the adjust argument - value to multiply default bandwidth by\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(adjust = 0.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#what-about-the-bandwidth-1",
    "href": "lectures/04-quant-distributions.html#what-about-the-bandwidth-1",
    "title": "Visualizing Quantitative Distributions",
    "section": "What about the bandwidth?",
    "text": "What about the bandwidth?\nUse Gaussian reference rule (rule-of-thumb) \\(\\approx 1.06 \\cdot \\sigma \\cdot n^{-1/5}\\), where \\(\\sigma\\) is the observed standard deviation\nModify the bandwidth using the adjust argument - value to multiply default bandwidth by\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(adjust = 2) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#caution-dealing-with-bounded-data",
    "href": "lectures/04-quant-distributions.html#caution-dealing-with-bounded-data",
    "title": "Visualizing Quantitative Distributions",
    "section": "CAUTION: dealing with bounded data…",
    "text": "CAUTION: dealing with bounded data…\n\nset.seed(101)\nbound_data &lt;- tibble(fake_x = runif(100))\n\nbound_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_density() +\n  geom_rug(alpha = 0.5) + #&lt;&lt;\n  stat_function(data = \n                  tibble(fake_x = c(0, 1)),\n                fun = dunif, color = \"red\") +\n  scale_x_continuous(limits = c(-.5, 1.5))"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#visualizing-conditional-distributions-with-violin-plots",
    "href": "lectures/04-quant-distributions.html#visualizing-conditional-distributions-with-violin-plots",
    "title": "Visualizing Quantitative Distributions",
    "section": "Visualizing conditional distributions with violin plots",
    "text": "Visualizing conditional distributions with violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#visualizing-conditional-distributions-with-violin-plots-1",
    "href": "lectures/04-quant-distributions.html#visualizing-conditional-distributions-with-violin-plots-1",
    "title": "Visualizing Quantitative Distributions",
    "section": "Visualizing conditional distributions with violin plots",
    "text": "Visualizing conditional distributions with violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() + \n  geom_boxplot(width = .2) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#visualizing-conditional-distributions-with-ggridges-package",
    "href": "lectures/04-quant-distributions.html#visualizing-conditional-distributions-with-ggridges-package",
    "title": "Visualizing Quantitative Distributions",
    "section": "Visualizing conditional distributions with ggridges package",
    "text": "Visualizing conditional distributions with ggridges package\n\nlibrary(ggridges)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_density_ridges(rel_min_height = 0.01)"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#visualizing-conditional-distributions-with-ggbeeswarm-package",
    "href": "lectures/04-quant-distributions.html#visualizing-conditional-distributions-with-ggbeeswarm-package",
    "title": "Visualizing Quantitative Distributions",
    "section": "Visualizing conditional distributions with ggbeeswarm package",
    "text": "Visualizing conditional distributions with ggbeeswarm package\n\nlibrary(ggbeeswarm)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_beeswarm(cex = 1.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#recap-and-next-steps",
    "href": "lectures/04-quant-distributions.html#recap-and-next-steps",
    "title": "Visualizing Quantitative Distributions",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nDiscussed impact of bins on histograms\nCovered ECDFs and connection to KS-tests\nWalked through density estimation\nDiscussed ways of visualizing conditional distributions\n\n\n\nHW2 is due Wednesday Sept 11th!\nHW3 is posted and due next Wednesday Sept 18th\n\n\n\n\nNext time: 2D quantitative data\nRecommended reading:\nCW Chapter 7 Visualizing distributions: Histograms and density plots, CW Chapter 8 Visualizing distributions: Empirical cumulative distribution functions and q-q plots"
  },
  {
    "objectID": "lectures/04-quant-distributions.html#bonus-visualizing-the-ks-test-statistic",
    "href": "lectures/04-quant-distributions.html#bonus-visualizing-the-ks-test-statistic",
    "title": "Visualizing Quantitative Distributions",
    "section": "BONUS: Visualizing the KS test statistic",
    "text": "BONUS: Visualizing the KS test statistic\n\n# First create the ECDF function for the variable:\nfl_ecdf &lt;- ecdf(penguins$flipper_length_mm)\n# Compute the absolute value of the differences between the ECDF for the values\n# and the theoretical values with assumed Normal distribution:\nabs_ecdf_diffs &lt;- abs(fl_ecdf(penguins$flipper_length_mm) - pnorm(penguins$flipper_length_mm,\n                                                                  mean = flipper_length_mean, sd = flipper_length_sd))\n# Now find where the maximum difference is:\nmax_abs_ecdf_diff_i &lt;- which.max(abs_ecdf_diffs)\n# Get this flipper length value:\nmax_fl_diff_value &lt;- penguins$flipper_length_mm[max_abs_ecdf_diff_i]\n# Plot the ECDF with the theoretical Normal and KS test info:\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) +\n  stat_ecdf(color = \"darkblue\") +\n  # Use stat_function to draw the Normal ECDF\n  stat_function(fun = pnorm, args = list(mean = flipper_length_mean, sd = flipper_length_sd), color = \"black\", linetype = \"dashed\") +\n  # Draw KS test line:\n  geom_vline(xintercept = max_fl_diff_value, color = \"red\") +\n  # Add text with the test results (x and y are manually entered locations)\n  annotate(geom = \"text\", x = 215, y = .25, label = \"KS test stat = 0.12428\\np-value = 5.163e-05\") + \n  labs(x = \"Flipper length (mm)\", y = \"Fn(x)\") + theme_bw()\n\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#reminders-previously-and-today",
    "href": "lectures/09-time-series-intro-spatial.html#reminders-previously-and-today",
    "title": "Time series and intro to spatial data",
    "section": "Reminders, previously, and today…",
    "text": "Reminders, previously, and today…\n\nHW4 is due TONIGHT Sept 25th\nYou need to email me a draft of your EDA report! (1 per group)\n\n\n\nWalked through non-linear dimension reduction with t-SNE\nDiscussed visualizing trends, highglighting points of emphasis\n\n\n\nTODAY:\n\nWalk through the basics of time series data techniques\nIntroduce visualizations and inference with spatial data"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#things-of-interest-for-time-series-data",
    "href": "lectures/09-time-series-intro-spatial.html#things-of-interest-for-time-series-data",
    "title": "Time series and intro to spatial data",
    "section": "Things of interest for time series data",
    "text": "Things of interest for time series data\nTime series can be characterized by three features:\n\nTrends: Does the variable increase or decrease over time, on average?\nSeasonality: Are there changes in the variable that regularly happen (e.g., every winter, every hour, etc.)? Sometimes called periodicity.\nNoise: Variation in the variable beyond average trends and seasonality.\n\nMoving averages are a starting point for visualizing how a trend changes over time"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#be-responsible-with-your-axes",
    "href": "lectures/09-time-series-intro-spatial.html#be-responsible-with-your-axes",
    "title": "Time series and intro to spatial data",
    "section": "Be responsible with your axes!",
    "text": "Be responsible with your axes!"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#be-responsible-with-your-axes-1",
    "href": "lectures/09-time-series-intro-spatial.html#be-responsible-with-your-axes-1",
    "title": "Time series and intro to spatial data",
    "section": "Be responsible with your axes!",
    "text": "Be responsible with your axes!"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#moving-average-plots",
    "href": "lectures/09-time-series-intro-spatial.html#moving-average-plots",
    "title": "Time series and intro to spatial data",
    "section": "Moving Average Plots",
    "text": "Moving Average Plots\nThe Financial Times COVID-19 plots displayed a moving average (sometimes called a rolling average)\nIntuition\n\nDivide your data into small subsets (“windows”)\nCompute the average within each window\nConnect the averages together to make a trend line\n\n\nSometimes called a simple moving average\nThis is exactly what we did with LOESS… we called this a sliding window, but it’s the same thing"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#how-are-moving-averages-computed",
    "href": "lectures/09-time-series-intro-spatial.html#how-are-moving-averages-computed",
    "title": "Time series and intro to spatial data",
    "section": "How are moving averages computed?",
    "text": "How are moving averages computed?\nIntuition\n\nDivide your data into small subsets (windows)\nCompute the average within each window\nConnect the averages together to make a trend line\n\n\nMathematically, a moving average can be written as the following:\n\\[\\mu_k = \\frac{\\sum_{t=k - h + 1}^k X_t}{h}\\]\n\nLarge \\(h\\): Smooth line; captures global trends\nSmall \\(h\\): Jagged/volatile line; captures local trends"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#working-with-time-series",
    "href": "lectures/09-time-series-intro-spatial.html#working-with-time-series",
    "title": "Time series and intro to spatial data",
    "section": "Working with Time Series",
    "text": "Working with Time Series\nco2: Mauna Loa Atmospheric CO2 Concentration dataset (monthly \\(\\text{CO}^2\\) concentration 1959 to 1997)\n\nco2_tbl |&gt;\n  ggplot(aes(x = obs_i, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Time index\", y = \"CO2 (ppm)\")"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#formatting-dates",
    "href": "lectures/09-time-series-intro-spatial.html#formatting-dates",
    "title": "Time series and intro to spatial data",
    "section": "Formatting Dates",
    "text": "Formatting Dates\nCan use as.Date() to create time indexes.\n\nDefault format is Year/Month/Day. For something else, need to specify format in as.Date() (e.g., format = \"%m/%d/%Y\")"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#use-scale_x_date-to-create-interpretable-axis-labels",
    "href": "lectures/09-time-series-intro-spatial.html#use-scale_x_date-to-create-interpretable-axis-labels",
    "title": "Time series and intro to spatial data",
    "section": "Use scale_x_date() to create interpretable axis labels",
    "text": "Use scale_x_date() to create interpretable axis labels"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#other-moving-averages",
    "href": "lectures/09-time-series-intro-spatial.html#other-moving-averages",
    "title": "Time series and intro to spatial data",
    "section": "Other Moving Averages",
    "text": "Other Moving Averages\nTwo other common averages: Cumulative moving averages and weighted moving averages.\n\nCumulative moving average: The average at time \\(k\\) is the average of all points at and before \\(k\\). Mathematically:\n\n\\[\\mu_k^{(CMA)} = \\frac{\\sum_{t=1}^k X_t}{k}\\]\n\n\nWeighted moving average: Same as simple moving average, but different measurements get different weights for the average.\n\n\\[\\mu_k^{(WMA)} = \\frac{\\sum_{t=k - h + 1}^k X_t \\cdot w_t}{ \\sum_{t=k - h + 1}^k w_t}\\]"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#working-with-lags",
    "href": "lectures/09-time-series-intro-spatial.html#working-with-lags",
    "title": "Time series and intro to spatial data",
    "section": "Working with lags",
    "text": "Working with lags\nTime series data is fundamentally different from other data problems we’ve worked with because measurements are not independent\nObvious example: The temperature today is correlated with temperature yesterday. (Maybe not in Pittsburgh?)\n\nImportant term: lags. Used to determine if one time point influences future time points.\nLag 1: Comparing time series at time \\(t\\) with time series at time \\(t - 1\\).\nLag 2: Comparing time series at time \\(t\\) with time series at time \\(t - 2\\).\nAnd so on…\n\n\nLet’s say we have time measurements \\((X_1, X_2, X_3, X_4, X_5)\\).\nThe \\(\\ell = 1\\) lag is \\((X_2, X_3, X_4, X_5)\\) vs \\((X_1, X_2, X_3, X_4)\\).\n\n\nThe \\(\\ell = 2\\) lag is \\((X_3, X_4, X_5)\\) vs \\((X_1, X_2, X_3)\\).\nConsider: Are previous outcomes (lags) predictive of future outcomes?"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#autocorrelation",
    "href": "lectures/09-time-series-intro-spatial.html#autocorrelation",
    "title": "Time series and intro to spatial data",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nAutocorrelation: Correlation between a time series and a lagged version of itself.\nDefine \\(r_{\\ell}\\) as the correlation between a time series and Lag \\(\\ell\\) of that time series.\n\nLag 1: \\(r_1\\) is correlation between \\((X_2, X_3, X_4, X_5)\\) and \\((X_1,X_2,X_3,X_4)\\)\nLag 2: \\(r_2\\) is correlation between \\((X_3, X_4, X_5)\\) and \\((X_1,X_2,X_3)\\)\nAnd so on…\n\n\nCommon diagnostic: Plot \\(\\ell\\) on x-axis, \\(r_{\\ell}\\) on y-axis.\nTells us if correlations are “significantly large” or “significantly small” for certain lags\nTo make an autocorrelation plot, we use the acf() function; the ggplot version uses autoplot()"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#autocorrelation-plots",
    "href": "lectures/09-time-series-intro-spatial.html#autocorrelation-plots",
    "title": "Time series and intro to spatial data",
    "section": "Autocorrelation plots",
    "text": "Autocorrelation plots\n\nlibrary(ggfortify)\nauto_corr &lt;- acf(co2_tbl$co2_val, plot = FALSE)\nautoplot(auto_corr)"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#autocorrelation-plots-and-seasonality-1",
    "href": "lectures/09-time-series-intro-spatial.html#autocorrelation-plots-and-seasonality-1",
    "title": "Time series and intro to spatial data",
    "section": "Autocorrelation Plots and Seasonality",
    "text": "Autocorrelation Plots and Seasonality"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#how-should-we-think-about-spatial-data",
    "href": "lectures/09-time-series-intro-spatial.html#how-should-we-think-about-spatial-data",
    "title": "Time series and intro to spatial data",
    "section": "How should we think about spatial data?",
    "text": "How should we think about spatial data?\nTypically location is measured with latitude / longitude (2D)\n\n\n\nLatitude: Measures North / South (the “y-axis”)\n\nRange is \\((-90^{\\circ}, 90^{\\circ})\\)\nMeasures degrees from the equator \\((0^{\\circ})\\)\n\\((-90^{\\circ}, 0^{\\circ})\\) = southern hemisphere\n\\((0^{\\circ}, 90^{\\circ})\\) = northern hemisphere\n\n\n\n\nLongitude: Measures East/West (the “x-axis”)\n\nRange is \\((-180^{\\circ}, 180^{\\circ})\\)\nMeasures degrees from the prime meridian \\((0^{\\circ})\\) in Greenwich, England\n\\((-180^{\\circ}, 0^{\\circ})\\) = eastern hemisphere\n\\((0^{\\circ}, 180^{\\circ})\\) = western hemisphere"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#latitude-and-longitude",
    "href": "lectures/09-time-series-intro-spatial.html#latitude-and-longitude",
    "title": "Time series and intro to spatial data",
    "section": "Latitude and Longitude",
    "text": "Latitude and Longitude"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#map-projections",
    "href": "lectures/09-time-series-intro-spatial.html#map-projections",
    "title": "Time series and intro to spatial data",
    "section": "Map Projections",
    "text": "Map Projections\nMap projections: Transformation of the lat / long coordinates on a sphere (the earth) to a 2D plane\n\nThere are many different projections - each will distort the map in different ways.\nThe most common projections are:\n\nMercator\nRobinson\nConic\nCylindrical\nPlanar\nInterrupted projections"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#mercator-projection-1500s",
    "href": "lectures/09-time-series-intro-spatial.html#mercator-projection-1500s",
    "title": "Time series and intro to spatial data",
    "section": "Mercator Projection (1500s)",
    "text": "Mercator Projection (1500s)"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#mercator-projection-tissot-indicatrix",
    "href": "lectures/09-time-series-intro-spatial.html#mercator-projection-tissot-indicatrix",
    "title": "Time series and intro to spatial data",
    "section": "Mercator Projection (Tissot indicatrix)",
    "text": "Mercator Projection (Tissot indicatrix)"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#robinson-projection-standard-from-1963-1998",
    "href": "lectures/09-time-series-intro-spatial.html#robinson-projection-standard-from-1963-1998",
    "title": "Time series and intro to spatial data",
    "section": "Robinson Projection (Standard from 1963-1998)",
    "text": "Robinson Projection (Standard from 1963-1998)"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#robinson-projection-tissot-indicatrix",
    "href": "lectures/09-time-series-intro-spatial.html#robinson-projection-tissot-indicatrix",
    "title": "Time series and intro to spatial data",
    "section": "Robinson Projection (Tissot indicatrix)",
    "text": "Robinson Projection (Tissot indicatrix)"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#winkel-tripel-projection-proposed-1921-now-the-standard",
    "href": "lectures/09-time-series-intro-spatial.html#winkel-tripel-projection-proposed-1921-now-the-standard",
    "title": "Time series and intro to spatial data",
    "section": "Winkel Tripel Projection (proposed 1921, now the standard)",
    "text": "Winkel Tripel Projection (proposed 1921, now the standard)"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#winkel-tripel-projection-tissot-indicatrix",
    "href": "lectures/09-time-series-intro-spatial.html#winkel-tripel-projection-tissot-indicatrix",
    "title": "Time series and intro to spatial data",
    "section": "Winkel Tripel Projection (Tissot indicatrix)",
    "text": "Winkel Tripel Projection (Tissot indicatrix)"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#and-many-more-see-xkcd-comic",
    "href": "lectures/09-time-series-intro-spatial.html#and-many-more-see-xkcd-comic",
    "title": "Time series and intro to spatial data",
    "section": "And many more… (see xkcd comic)",
    "text": "And many more… (see xkcd comic)"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#visualizing-spatial-data-on-maps-using-ggmap",
    "href": "lectures/09-time-series-intro-spatial.html#visualizing-spatial-data-on-maps-using-ggmap",
    "title": "Time series and intro to spatial data",
    "section": "Visualizing spatial data on maps using ggmap",
    "text": "Visualizing spatial data on maps using ggmap\n\nlibrary(ggmap)\n# First, we'll draw a \"box\" around the US (in terms of latitude and longitude)\nUS &lt;- c(left = -125, bottom = 10, right = -67, top = 49)\nmap &lt;- get_stadiamap(US, zoom = 5, maptype = \"stamen_toner_lite\")\n\n# Visualize the basic map\nggmap(map)\n\n\n\nDraw map based on lat / lon coordinates\nPut the box into get_stadiamap() to access Stamen Maps (you need an API key!)\nDraw the map using ggmap() to serve as base"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#visualizing-spatial-data-on-maps-using-ggmap-output",
    "href": "lectures/09-time-series-intro-spatial.html#visualizing-spatial-data-on-maps-using-ggmap-output",
    "title": "Time series and intro to spatial data",
    "section": "Visualizing spatial data on maps using ggmap",
    "text": "Visualizing spatial data on maps using ggmap"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#three-main-types-of-spatial-data",
    "href": "lectures/09-time-series-intro-spatial.html#three-main-types-of-spatial-data",
    "title": "Time series and intro to spatial data",
    "section": "Three main types of spatial data",
    "text": "Three main types of spatial data\n\nPoint Pattern Data: lat-long coordinates where events have occurred\nPoint-Referenced data: Latitude-longitude (lat-long) coordinates as well as one or more variables specific to those coordinates.\nAreal Data: Geographic regions with one or more variables associated with those regions.\n\n\n\nEach type is structured differently within a dataset\nEach type requires a different kind of graph(s)\n\n\n\nWe’re going to review each type of data. Then, we’re going to demonstrate how to plot these different data types\n\nToday: Point-referenced and point pattern\nMonday: Areal data"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#point-pattern-data",
    "href": "lectures/09-time-series-intro-spatial.html#point-pattern-data",
    "title": "Time series and intro to spatial data",
    "section": "Point-Pattern data",
    "text": "Point-Pattern data\n\nPoint Pattern Data: lat-long coordinates where events have occurred\nPoint pattern data simply records the lat-long of events; thus, there are only two columns\nAgain, latitude and longitude are represented with dots, sometimes called a dot or bubble map.\n\n\n\nThe goal is to understand how the density of events varies across space\nThe density of the dots can also be visualized (e.g., with contours)\n\nUse methods we’ve discussed before for visualizing 2D joint distribution"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#point-referenced-data",
    "href": "lectures/09-time-series-intro-spatial.html#point-referenced-data",
    "title": "Time series and intro to spatial data",
    "section": "Point-Referenced data",
    "text": "Point-Referenced data\n\nPoint-Referenced data: Latitude-longitude (lat-long) coordinates as well as one or more variables specific to those coordinates\nPoint-referenced data will have the following form:\n\n\nairports |&gt; dplyr::select(lat, lon, altitude, n_depart, n_arrive, name) |&gt; slice(1:3)\n\n# A tibble: 3 × 6\n    lat   lon altitude n_depart n_arrive name                        \n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt; &lt;chr&gt;                       \n1 -6.08  145.     5282        5        5 Goroka Airport              \n2 -5.21  146.       20        8        8 Madang Airport              \n3 -5.83  144.     5388       10       12 Mount Hagen Kagamuga Airport\n\n\n\n\nThe goal is to understand how the variable(s) (e.g., altitude) vary across different spatial locations\nTypically, the latitude and longitude are represented with dots, and the variable(s) are represented with size and/or colors"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#adding-points-to-the-map-as-usual",
    "href": "lectures/09-time-series-intro-spatial.html#adding-points-to-the-map-as-usual",
    "title": "Time series and intro to spatial data",
    "section": "Adding points to the map as usual",
    "text": "Adding points to the map as usual\n\nggmap(map) +\n  geom_point(data = airports, aes(x = lon, y = lat), alpha = 0.25)"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#altering-points-on-the-map-in-the-usual-way",
    "href": "lectures/09-time-series-intro-spatial.html#altering-points-on-the-map-in-the-usual-way",
    "title": "Time series and intro to spatial data",
    "section": "Altering points on the map (in the usual way)",
    "text": "Altering points on the map (in the usual way)\n\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, \n                 size = sqrt(n_depart), color = sqrt(n_arrive)),\n             alpha = .5) +\n  scale_size_area(breaks = sqrt(c(1, 5, 10, 50, 100, 500)), \n                  labels = c(1, 5, 10, 50, 100, 500), \n                  name = \"# departures\") +\n  scale_color_distiller(palette = \"Spectral\") +\n  labs(color = \"sqrt(# arrivals)\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#altering-points-on-the-map-in-the-usual-way-output",
    "href": "lectures/09-time-series-intro-spatial.html#altering-points-on-the-map-in-the-usual-way-output",
    "title": "Time series and intro to spatial data",
    "section": "Altering points on the map (in the usual way)",
    "text": "Altering points on the map (in the usual way)"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#inference-for-spatial-data",
    "href": "lectures/09-time-series-intro-spatial.html#inference-for-spatial-data",
    "title": "Time series and intro to spatial data",
    "section": "Inference for Spatial Data",
    "text": "Inference for Spatial Data\nThere are whole courses, textbooks, and careers dedicated to this. We’re not going to cover everything!\nHowever, there are some straightforward analyses that can be done for spatial data.\nPoint-Referenced Data:\n\nDivide geography into groups (e.g., north/south/east/west) and use regression to test if there are significant differences.\nRegression of \\(\\text{outcome} \\sim \\text{latitude} + \\text{longitude}\\). Smoothing regression (e.g., loess) is particularly useful here."
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#visualizing-inference-for-point-reference-data",
    "href": "lectures/09-time-series-intro-spatial.html#visualizing-inference-for-point-reference-data",
    "title": "Time series and intro to spatial data",
    "section": "Visualizing Inference for Point-Reference Data",
    "text": "Visualizing Inference for Point-Reference Data\nFor basic linear regression:\n\nPlot \\((x, y)\\) as points\nFit the regression model \\(y \\sim x\\), to give us \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x\\)\nPlot \\((x, \\hat{y})\\) as a line\n\n\nFor point reference data, we have the following variables:\n\nInputs are longitude \\(x\\) and latitude \\(y\\), and outcome variable is \\(z\\)\n\nConsider the following linear regression model: \\(z \\sim \\text{lat} + \\text{long}\\)\nGoal: Make a visual involving \\((\\text{long}, \\text{lat}, \\hat{z})\\), and possibly \\(z\\)."
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#kriging",
    "href": "lectures/09-time-series-intro-spatial.html#kriging",
    "title": "Time series and intro to spatial data",
    "section": "Kriging",
    "text": "Kriging\nGoal: Make a visual involving (long, lat, \\(\\hat{z}\\)) and possibly \\(z\\)\nWant \\(\\hat{z}\\) for many (long, lat) combos (not just the observed one!)\nTo do this, follow this procedure:\n\nFit the model \\(z \\sim \\text{lat} + \\text{long}\\)\nCreate a grid of \\((\\text{long}, \\text{lat})_{ij}\\)\nGenerate \\(\\hat{z}_{ij}\\) for each \\((\\text{long}, \\text{lat})_{ij}\\)\nPlot a heat map or contour plot of (long, lat, \\(\\hat{z}\\))\n\n\nYou can also add the actual \\(z\\) values (e.g., via size) on the heat map\n\nThis is known as kriging, or spatial interpolation"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#kriging-airline-data-example",
    "href": "lectures/09-time-series-intro-spatial.html#kriging-airline-data-example",
    "title": "Time series and intro to spatial data",
    "section": "Kriging: airline data example",
    "text": "Kriging: airline data example"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#kriging-creating-the-map",
    "href": "lectures/09-time-series-intro-spatial.html#kriging-creating-the-map",
    "title": "Time series and intro to spatial data",
    "section": "Kriging: creating the map",
    "text": "Kriging: creating the map"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#kriging-generating-the-grid",
    "href": "lectures/09-time-series-intro-spatial.html#kriging-generating-the-grid",
    "title": "Time series and intro to spatial data",
    "section": "Kriging: generating the grid",
    "text": "Kriging: generating the grid"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#kriging-generating-predicted-values",
    "href": "lectures/09-time-series-intro-spatial.html#kriging-generating-predicted-values",
    "title": "Time series and intro to spatial data",
    "section": "Kriging: generating predicted values",
    "text": "Kriging: generating predicted values"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#kriging-plotting-heat-map-of-predicted-values",
    "href": "lectures/09-time-series-intro-spatial.html#kriging-plotting-heat-map-of-predicted-values",
    "title": "Time series and intro to spatial data",
    "section": "Kriging: plotting heat map of predicted values",
    "text": "Kriging: plotting heat map of predicted values"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#kriging-overview",
    "href": "lectures/09-time-series-intro-spatial.html#kriging-overview",
    "title": "Time series and intro to spatial data",
    "section": "Kriging overview",
    "text": "Kriging overview\nThe steps used to create this map are…\n\nFit an interactive regression model using loess()\nMake a grid of lat/long coordinates, using seq() and expand.grid()\nGet estimated outcomes across the grid using predict()\nUse geom_contour_filled() to color map by estimated outcomes"
  },
  {
    "objectID": "lectures/09-time-series-intro-spatial.html#recap-and-next-steps",
    "href": "lectures/09-time-series-intro-spatial.html#recap-and-next-steps",
    "title": "Time series and intro to spatial data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWalked through basics of time series data, such as moving averages, autocorrelation, seasonality\nVisualized spatial data in a 2D plane (latitude/longitude), i.e., maps\n\nPoint pattern: Scatterplots with density contours\nPoint-referenced: Scatterplots with color/size, use regression/loess for inference\n\n\n\n\nHW4 is due TONIGHT! Email me a draft of your EDA report! (1 per group)\nNext time: Visualizing areal data and creating high-quality graphics\nCW CH 13 Visualizing time series and other functions of an independent variable, CW CH 14 Visualizing trends, CW Chapter 15 Visualizing geospatial data, KH Chapter 7 Draw Maps\n\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#reminders-previously-and-today",
    "href": "lectures/10-areal-data-creating-graphics.html#reminders-previously-and-today",
    "title": "Areal data and creating high-quality graphics",
    "section": "Reminders, previously, and today…",
    "text": "Reminders, previously, and today…\n\nInfographic draft is due Wednesday night! (more details later today…)\nYour EDA report is due Friday Oct 4th by 11:59 PM ET (1 per group)\nNo lecture on Wednesday! But I will have virtual office hours during class time\n\n\n\nWrapped up basics of time series data\nIntroduction to spatial data and the different types\nWalked through visualizing point-reference data\n\n\n\nTODAY:\n\nVisualizations for areal data\nDiscuss making high-quality graphics"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#thinking-about-areal-data",
    "href": "lectures/10-areal-data-creating-graphics.html#thinking-about-areal-data",
    "title": "Areal data and creating high-quality graphics",
    "section": "Thinking about areal data",
    "text": "Thinking about areal data\n\nAreal Data: Geographic regions associated with one or more variables specific to those regions\nAreal data will have the following form (example US states data from 1970s):\n\n\nstate_data |&gt; dplyr::slice(1:3)\n\n# A tibble: 3 × 9\n  Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area state  \n       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n1       3615   3624        2.1       69.0   15.1      41.3    20  50708 alabama\n2        365   6315        1.5       69.3   11.3      66.7   152 566432 alaska \n3       2212   4530        1.8       70.6    7.8      58.1    15 113417 arizona"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#high-level-overview-of-steps",
    "href": "lectures/10-areal-data-creating-graphics.html#high-level-overview-of-steps",
    "title": "Areal data and creating high-quality graphics",
    "section": "High-level overview of steps",
    "text": "High-level overview of steps\n\nNeed to match the region with the actual geographic boundaries\nMany geographic boundaries/features are stored as “shapefiles”\n\ni.e., complicated polygons\n\nCan contain the lines, points, etc. to represent any geographic feature\nShapefiles are readily available for countries, states, counties, etc."
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#access-shapefiles-using-map_data",
    "href": "lectures/10-areal-data-creating-graphics.html#access-shapefiles-using-map_data",
    "title": "Areal data and creating high-quality graphics",
    "section": "Access shapefiles using map_data()",
    "text": "Access shapefiles using map_data()\n\nlibrary(maps)\nstate_borders &lt;- map_data(\"state\") \nhead(state_borders)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\n\nFor example: map_data(\"world\"), map_data(\"state\"), map_data(\"county\") (need to install maps package)\nContains lat/lon coordinates to draw geographic boundaries"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#typica-workflow-for-plotting-areal-data",
    "href": "lectures/10-areal-data-creating-graphics.html#typica-workflow-for-plotting-areal-data",
    "title": "Areal data and creating high-quality graphics",
    "section": "Typica workflow for plotting areal data",
    "text": "Typica workflow for plotting areal data\n\nGet state-specific data\nGet state boundaries\nMerge state-specific data with state boundaries (using left_join())\n\n\nstate_plot_data &lt;- state_borders |&gt;\n  left_join(state_data, by = c(\"region\" = \"state\"))\nhead(state_plot_data)\n\n       long      lat group order  region subregion Population Income Illiteracy\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;       3615   3624        2.1\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;       3615   3624        2.1\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;       3615   3624        2.1\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;       3615   3624        2.1\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;       3615   3624        2.1\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;       3615   3624        2.1\n  Life Exp Murder HS Grad Frost  Area\n1    69.05   15.1    41.3    20 50708\n2    69.05   15.1    41.3    20 50708\n3    69.05   15.1    41.3    20 50708\n4    69.05   15.1    41.3    20 50708\n5    69.05   15.1    41.3    20 50708\n6    69.05   15.1    41.3    20 50708\n\n\n\nPlot the data"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#create-a-choropleth-map-with-geom_polygon",
    "href": "lectures/10-areal-data-creating-graphics.html#create-a-choropleth-map-with-geom_polygon",
    "title": "Areal data and creating high-quality graphics",
    "section": "Create a choropleth map with geom_polygon()",
    "text": "Create a choropleth map with geom_polygon()\n\nstate_plot_data |&gt;\n  ggplot() + \n  geom_polygon(aes(x = long, y = lat, group = group, fill = Illiteracy), \n               color = \"black\") + \n  scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                       high = \"darkorchid4\", midpoint = 0.95) +\n  theme_void() +\n  coord_map(\"polyconic\") + \n  labs(fill = \"Illiteracy %\") + \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#create-a-choropleth-map-with-geom_polygon-output",
    "href": "lectures/10-areal-data-creating-graphics.html#create-a-choropleth-map-with-geom_polygon-output",
    "title": "Areal data and creating high-quality graphics",
    "section": "Create a choropleth map with geom_polygon()",
    "text": "Create a choropleth map with geom_polygon()"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#uniform-size-with-statebins",
    "href": "lectures/10-areal-data-creating-graphics.html#uniform-size-with-statebins",
    "title": "Areal data and creating high-quality graphics",
    "section": "Uniform size with statebins",
    "text": "Uniform size with statebins\n\nlibrary(statebins)\nstate_data$new_state &lt;- str_to_title(state_data$state)\nstatebins(state_data = state_data, \n          state_col = \"new_state\", value_col = \"Illiteracy\") +\n  theme_statebins()"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#many-choices-for-displaying-maps",
    "href": "lectures/10-areal-data-creating-graphics.html#many-choices-for-displaying-maps",
    "title": "Areal data and creating high-quality graphics",
    "section": "Many choices for displaying maps…",
    "text": "Many choices for displaying maps…"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#visual-randomization-test",
    "href": "lectures/10-areal-data-creating-graphics.html#visual-randomization-test",
    "title": "Areal data and creating high-quality graphics",
    "section": "Visual randomization test",
    "text": "Visual randomization test"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#visual-randomization-test-1",
    "href": "lectures/10-areal-data-creating-graphics.html#visual-randomization-test-1",
    "title": "Areal data and creating high-quality graphics",
    "section": "Visual randomization test",
    "text": "Visual randomization test"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#infographics-draft-and-feedback-assignment",
    "href": "lectures/10-areal-data-creating-graphics.html#infographics-draft-and-feedback-assignment",
    "title": "Areal data and creating high-quality graphics",
    "section": "Infographics draft and feedback assignment",
    "text": "Infographics draft and feedback assignment\n\nYou turn in via Gradescope and email a single page PDF draft of your infographic to your assigned partner with myself cc’ed by 11:59 PM Wednesday night (no code is necessary for this draft)\nFor only this draft submission, you are allowed to use something like google slides or powerpoint to create your draft PDF\nDetailed grading rubric for your final submission (due Oct 11th by 11:59 PM ET) is posted on Canvas\n\n\n\nYou must provide feedback to your assigned infographics partner via email (see emails I sent this morning) by Saturday night 11:59 PM ET and turn in via Gradescope as well\nFeedback template is available on Canvas (and is 10% of your grade!)"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#creating-compound-figures",
    "href": "lectures/10-areal-data-creating-graphics.html#creating-compound-figures",
    "title": "Areal data and creating high-quality graphics",
    "section": "Creating compound figures",
    "text": "Creating compound figures\nTwo different scenarios we may face:\n\nCreating the same type of plot many times\n\n\ne.g., using facet_wrap() or facet_grid()\n\n\nCombining several distinct plots into one cohesive display\n\n\ne.g., using flexible arrangement packages like cowplot or patchwork"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#creating-the-same-type-of-plot-many-times",
    "href": "lectures/10-areal-data-creating-graphics.html#creating-the-same-type-of-plot-many-times",
    "title": "Areal data and creating high-quality graphics",
    "section": "Creating the same type of plot many times",
    "text": "Creating the same type of plot many times\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species) +\n  theme_light()"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#creating-the-same-type-of-plot-many-times-1",
    "href": "lectures/10-areal-data-creating-graphics.html#creating-the-same-type-of-plot-many-times-1",
    "title": "Areal data and creating high-quality graphics",
    "section": "Creating the same type of plot many times",
    "text": "Creating the same type of plot many times\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_grid(island ~ species) +\n  theme_light()"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#creating-a-single-cohesive-display-of-multiple-plots",
    "href": "lectures/10-areal-data-creating-graphics.html#creating-a-single-cohesive-display-of-multiple-plots",
    "title": "Areal data and creating high-quality graphics",
    "section": "Creating a single cohesive display of multiple plots",
    "text": "Creating a single cohesive display of multiple plots\n\nplot1 &lt;- penguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5)\nplot1"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#creating-a-single-cohesive-display-of-multiple-plots-1",
    "href": "lectures/10-areal-data-creating-graphics.html#creating-a-single-cohesive-display-of-multiple-plots-1",
    "title": "Areal data and creating high-quality graphics",
    "section": "Creating a single cohesive display of multiple plots",
    "text": "Creating a single cohesive display of multiple plots\n\nplot2 &lt;- penguins |&gt;\n  ggplot(aes(x = species, y = bill_depth_mm)) +\n  geom_violin(alpha = 0.5)\nplot2"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#using-cowplot-to-arrange-plots-together",
    "href": "lectures/10-areal-data-creating-graphics.html#using-cowplot-to-arrange-plots-together",
    "title": "Areal data and creating high-quality graphics",
    "section": "Using cowplot to arrange plots together",
    "text": "Using cowplot to arrange plots together\n\nlibrary(cowplot)\nplot_grid(plot1, plot2)"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#using-cowplot-to-arrange-plots-together-1",
    "href": "lectures/10-areal-data-creating-graphics.html#using-cowplot-to-arrange-plots-together-1",
    "title": "Areal data and creating high-quality graphics",
    "section": "Using cowplot to arrange plots together",
    "text": "Using cowplot to arrange plots together\n\nlibrary(cowplot)\nplot_grid(plot1, plot2, labels = c('A', 'B'), label_size = 12)"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together",
    "href": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together",
    "title": "Areal data and creating high-quality graphics",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\nlibrary(patchwork)\nplot1 + plot2"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-1",
    "href": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-1",
    "title": "Areal data and creating high-quality graphics",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\nplot1 / plot2"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-2",
    "href": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-2",
    "title": "Areal data and creating high-quality graphics",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\nplot1 / plot2 + plot_annotation(tag_levels = \"A\")"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-3",
    "href": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-3",
    "title": "Areal data and creating high-quality graphics",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\nplot3 &lt;- penguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm,\n             color = species)) +\n  geom_point(alpha = 0.5)\nplot4 &lt;- penguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = body_mass_g,\n             color = species)) +\n  geom_point(alpha = 0.5)\n(plot1 + plot2) / (plot3 + plot4) + plot_layout(guides = 'collect')"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-3-output",
    "href": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-3-output",
    "title": "Areal data and creating high-quality graphics",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-4",
    "href": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-4",
    "title": "Areal data and creating high-quality graphics",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\n(plot1 + plot2) / (plot3 + plot4) + plot_layout(guides = 'collect') +\n  plot_annotation(tag_levels = \"A\")"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-5",
    "href": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-5",
    "title": "Areal data and creating high-quality graphics",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\n(plot1 + plot2) / (plot3 + plot4) + plot_layout(guides = 'collect') +\n  plot_annotation(tag_levels = \"A\", title = \"A plot about penguins\",\n                  subtitle = \"With subtitle...\", caption = \"...and caption\")"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#infographics-vs-figures-in-papersreports",
    "href": "lectures/10-areal-data-creating-graphics.html#infographics-vs-figures-in-papersreports",
    "title": "Areal data and creating high-quality graphics",
    "section": "Infographics vs figures in papers/reports",
    "text": "Infographics vs figures in papers/reports\n\nInfographics should standalone, thus they must have a title along with a relevant subtitle and caption (located within the plot)"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#infographics-vs-figures-in-papersreports-1",
    "href": "lectures/10-areal-data-creating-graphics.html#infographics-vs-figures-in-papersreports-1",
    "title": "Areal data and creating high-quality graphics",
    "section": "Infographics vs figures in papers/reports",
    "text": "Infographics vs figures in papers/reports\n\nFigures in papers/reports will have captions containing the information from the standalone title/subtitle/caption, see example:\n\n\nFigure 1. Corruption and human development. The most developed countries experience the least corruption. Data sources: Transparency International & UN Human Development Report."
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#thinking-about-themes",
    "href": "lectures/10-areal-data-creating-graphics.html#thinking-about-themes",
    "title": "Areal data and creating high-quality graphics",
    "section": "Thinking about themes…",
    "text": "Thinking about themes…\nSee posted demo walking through color scales and customizing themes\nDefault choices tend to treat each element with equal weight, e.g., axes stand out as much as the data or background elements look the same as the points of emphasis\n\nYou want to design your plot with the visual hierarchy in mind:\n\nMake elements of your plot that are more important look more important!\ni.e., customize your plot so that the data is the focus, not the axes and grid lines!\nMatch visual weight to focus of the graphic you want to communicate\n\n\n\nI tend to use theme_bw() or theme_light(), but there are other options from various packages such as ggthemes"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-6",
    "href": "lectures/10-areal-data-creating-graphics.html#using-patchwork-to-arrange-plots-together-6",
    "title": "Areal data and creating high-quality graphics",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\n(plot1 + plot2) / (plot3 + plot4) + plot_layout(guides = 'collect') +\n  plot_annotation(tag_levels = \"A\", title = \"A plot about penguins\",\n                  subtitle = \"With subtitle...\", caption = \"...and caption\") & \n  theme_minimal_grid()"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#annotation",
    "href": "lectures/10-areal-data-creating-graphics.html#annotation",
    "title": "Areal data and creating high-quality graphics",
    "section": "Annotation",
    "text": "Annotation\n\nUsing text can be a great way to highlight and explain aspects of a visualization when you’re not there to explain it\nannotate() is an easy way to add text to ggplot objects or add rectangle layers for highlighting displays\n\n\nmtcars |&gt;\n  ggplot(aes(x = wt, y = mpg)) + \n  geom_point() + \n  annotate(\"text\", x = 4, y = 25, label = \"Some text\") +\n  annotate(\"rect\", xmin = 3, xmax = 4.2, ymin = 12, ymax = 21, alpha = .2)"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#annotation-tools",
    "href": "lectures/10-areal-data-creating-graphics.html#annotation-tools",
    "title": "Areal data and creating high-quality graphics",
    "section": "Annotation tools",
    "text": "Annotation tools\n\nWe’ve discussed gghighlight and ggrepel, but directlabels and ggforce are also useful\n\n\nlibrary(ggforce)\nggplot(iris, aes(Petal.Length, Petal.Width)) +\n  geom_mark_rect(aes(fill = Species, label = Species)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#saving-plots",
    "href": "lectures/10-areal-data-creating-graphics.html#saving-plots",
    "title": "Areal data and creating high-quality graphics",
    "section": "Saving plots",
    "text": "Saving plots\n\nDefault function for saving the last ggplot you created is ggsave\nI tend to use the save_plot() function from cowplot since it has easier customization for handling panels of multiple figures"
  },
  {
    "objectID": "lectures/10-areal-data-creating-graphics.html#recap-and-next-steps",
    "href": "lectures/10-areal-data-creating-graphics.html#recap-and-next-steps",
    "title": "Areal data and creating high-quality graphics",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWalked through basics of visualizing areal data\nDiscussed various aspects of making high-quality graphics and relevant tools\nNo lecture on Wednesday! But I will have virtual office hours during class time\nInfographic draft is due Wednesday night!\nYour EDA report is due Friday Oct 4th by 11:59 PM ET (1 per group)\nKH Chapter 7 Draw Maps, creating bivariate choropleths, CW Chapter 21 Multi-panel figures, CW Chapter 23 Balance the data and the context, KH Chapter 8 Refine your plots\n\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "lectures/11-text-data.html#reminders-previously-and-today",
    "href": "lectures/11-text-data.html#reminders-previously-and-today",
    "title": "Visualizations for text data",
    "section": "Reminders, previously, and today…",
    "text": "Reminders, previously, and today…\n\nInfographic is due Friday night!\nYou should be working on your presentations for Jamie…\n\n\n\nWalked through basics of visualizing areal data\nDiscussed various aspects of making high-quality graphics and relevant tools\nCompleted drafts and provided feedback to each other\n\n\n\nTODAY:\n\nIntroduction to text data\nOverview of common visualizations for text data"
  },
  {
    "objectID": "lectures/11-text-data.html#working-with-raw-text-data",
    "href": "lectures/11-text-data.html#working-with-raw-text-data",
    "title": "Visualizations for text data",
    "section": "Working with raw text data",
    "text": "Working with raw text data\n\nWe’ll work with script from the best episode of ‘The Office’: Season 4, Episode 13 - ‘Dinner Party’\nWe can access the script using the schrute package (yes this is a real thing):\n\n\nlibrary(schrute)\n# Create a table from this package just corresponding to the Dinner Party episode:\ndinner_party_table &lt;- theoffice |&gt;\n  filter(season == 4, episode == 13) |&gt;\n  # Just select columns of interest:\n  dplyr::select(index, character, text)\nhead(dinner_party_table)\n\n# A tibble: 6 × 3\n  index character text                                                          \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                                                         \n1 16791 Stanley   This is ridiculous.                                           \n2 16792 Phyllis   Do you have any idea what time we'll get out of here?         \n3 16793 Michael   Nobody likes to work late, least of all me. Do you have plans…\n4 16794 Jim       Nope I don't, remember when you told us not to make plans 'ca…\n5 16795 Michael   Yes I remember. Mmm, this is B.S. This is B.S. Why are we her…\n6 16796 Dwight    Thank you Michael."
  },
  {
    "objectID": "lectures/11-text-data.html#bag-of-words-representation-of-text",
    "href": "lectures/11-text-data.html#bag-of-words-representation-of-text",
    "title": "Visualizations for text data",
    "section": "Bag of Words representation of text",
    "text": "Bag of Words representation of text\n\nMost common way to store text data is with a document-term matrix (DTM):\n\n\n\n\n\nWord 1\nWord 2\n\\(\\dots\\)\nWord \\(J\\)\n\n\n\n\nDocument 1\n\\(w_{11}\\)\n\\(w_{12}\\)\n\\(\\dots\\)\n\\(w_{1J}\\)\n\n\nDocument 2\n\\(w_{21}\\)\n\\(w_{22}\\)\n\\(\\dots\\)\n\\(w_{2J}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\nDocument N\n\\(w_{N1}\\)\n\\(w_{N2}\\)\n\\(\\dots\\)\n\\(w_{NJ}\\)\n\n\n\n\n\\(w_{ij}\\): count of word \\(j\\) in document \\(i\\), aka term frequencies\n\n\nTwo additional ways to reduce number of columns:\n\nStop words: remove extremely common words (e.g., of, the, a)\nStemming: Reduce all words to their “stem”\n\n\nFor example: Reducing = reduc. Reduce = reduc. Reduces = reduc."
  },
  {
    "objectID": "lectures/11-text-data.html#tokenize-text-into-long-format",
    "href": "lectures/11-text-data.html#tokenize-text-into-long-format",
    "title": "Visualizations for text data",
    "section": "Tokenize text into long format",
    "text": "Tokenize text into long format\n\nConvert raw text into long, tidy table with one-token-per-document-per-row\n\nA token equals a unit of text - typically a word\n\n\n\nlibrary(tidytext)\ntidy_dinner_party_tokens &lt;- dinner_party_table |&gt;\n  unnest_tokens(word, text)\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 3\n  index character word      \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     \n1 16791 Stanley   this      \n2 16791 Stanley   is        \n3 16791 Stanley   ridiculous\n4 16792 Phyllis   do        \n5 16792 Phyllis   you       \n6 16792 Phyllis   have      \n\n\nEasy to convert text into DTM format using tidytext package"
  },
  {
    "objectID": "lectures/11-text-data.html#remove-stop-words",
    "href": "lectures/11-text-data.html#remove-stop-words",
    "title": "Visualizations for text data",
    "section": "Remove stop words",
    "text": "Remove stop words\n\nLoad stop_words from tidytext\n\n\ndata(stop_words)\n\ntidy_dinner_party_tokens &lt;- tidy_dinner_party_tokens |&gt;\n  filter(!(word %in% stop_words$word))\n\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 3\n  index character word      \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     \n1 16791 Stanley   ridiculous\n2 16792 Phyllis   idea      \n3 16792 Phyllis   time      \n4 16793 Michael   likes     \n5 16793 Michael   late      \n6 16793 Michael   plans"
  },
  {
    "objectID": "lectures/11-text-data.html#apply-stemming",
    "href": "lectures/11-text-data.html#apply-stemming",
    "title": "Visualizations for text data",
    "section": "Apply stemming",
    "text": "Apply stemming\n\nCan use SnowballC package to perform stemming\n\n\nlibrary(SnowballC)\n\ntidy_dinner_party_tokens &lt;- tidy_dinner_party_tokens |&gt;\n  mutate(stem = wordStem(word))\n\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 4\n  index character word       stem   \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  \n1 16791 Stanley   ridiculous ridicul\n2 16792 Phyllis   idea       idea   \n3 16792 Phyllis   time       time   \n4 16793 Michael   likes      like   \n5 16793 Michael   late       late   \n6 16793 Michael   plans      plan"
  },
  {
    "objectID": "lectures/11-text-data.html#create-word-cloud-using-term-frequencies",
    "href": "lectures/11-text-data.html#create-word-cloud-using-term-frequencies",
    "title": "Visualizations for text data",
    "section": "Create word cloud using term frequencies",
    "text": "Create word cloud using term frequencies\nWord Cloud: Displays all words mentioned across documents, where more common words are larger\n\nTo do this, you must compute the total word counts:\n\n\\[w_{\\cdot 1} = \\sum_{i=1}^N w_{i1} \\hspace{0.1in} \\dots \\hspace{0.1in} w_{\\cdot J} = \\sum_{i=1}^N w_{iJ}\\]\n\nThen, the size of Word \\(j\\) is proportional to \\(w_{\\cdot j}\\)\n\n\nCreate word clouds in R using wordcloud package\nTakes in two main arguments to create word clouds:\n\nwords: vector of unique words\nfreq: vector of frequencies"
  },
  {
    "objectID": "lectures/11-text-data.html#create-word-cloud-using-term-frequencies-1",
    "href": "lectures/11-text-data.html#create-word-cloud-using-term-frequencies-1",
    "title": "Visualizations for text data",
    "section": "Create word cloud using term frequencies",
    "text": "Create word cloud using term frequencies\n\ntoken_summary &lt;- tidy_dinner_party_tokens |&gt;\n  group_by(stem) |&gt;\n  count() |&gt;\n  ungroup() \n\nlibrary(wordcloud)\nwordcloud(words = token_summary$stem, \n          freq = token_summary$n, \n          random.order = FALSE, \n          max.words = 100, \n          colors = brewer.pal(8, \"Dark2\"))\n\n\n\nSet random.order = FALSE to place biggest words in center\nCan customize to display limited # words (max.words)\nOther options as well like colors"
  },
  {
    "objectID": "lectures/11-text-data.html#create-word-cloud-using-term-frequencies-1-output",
    "href": "lectures/11-text-data.html#create-word-cloud-using-term-frequencies-1-output",
    "title": "Visualizations for text data",
    "section": "Create word cloud using term frequencies",
    "text": "Create word cloud using term frequencies"
  },
  {
    "objectID": "lectures/11-text-data.html#comparison-clouds",
    "href": "lectures/11-text-data.html#comparison-clouds",
    "title": "Visualizations for text data",
    "section": "Comparison clouds",
    "text": "Comparison clouds\nImagine we have two different collections of documents, \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\), that we wish to visually compare.\n\nImagine we create the word clouds for the two collections of documents. Then this means we constructed vectors of total words for each collection:\n\n\\(\\mathbf{w}^{\\mathcal{A}} = (w_{\\cdot 1}^{\\mathcal{A}}, \\dots, w_{\\cdot J}^{\\mathcal{A}})\\)\n\\(\\mathbf{w}^{\\mathcal{B}} = (w_{\\cdot 1}^{\\mathcal{B}}, \\dots, w_{\\cdot J}^{\\mathcal{B}})\\)\n\nConsider the \\(j\\)th word, let’s pretend it’s “dinner”:\n\nIf \\(w_{\\cdot j}^{\\mathcal{A}}\\) is large, then “dinner” is large in the word cloud for \\(\\mathcal{A}\\).\nIf \\(w_{\\cdot j}^{\\mathcal{B}}\\) is large, then “dinner” is large in the word cloud for \\(\\mathcal{B}\\).\nBut if both are large, this doesn’t tell us whether \\(w_{\\cdot j}^{\\mathcal{A}}\\) or \\(w_{\\cdot j}^{\\mathcal{B}}\\) is bigger."
  },
  {
    "objectID": "lectures/11-text-data.html#comparison-clouds-1",
    "href": "lectures/11-text-data.html#comparison-clouds-1",
    "title": "Visualizations for text data",
    "section": "Comparison clouds",
    "text": "Comparison clouds\nThis motivates the construction of comparison word clouds:\n\nFor word \\(j\\), compute \\(\\bar{w}_{\\cdot j} = \\text{average}(w_{\\cdot j}^{\\mathcal{A}}, w_{\\cdot j}^{\\mathcal{B}})\\)\nCompute \\(w_{\\cdot j}^{\\mathcal{A}} - \\bar{w}_{\\cdot j}\\) and \\(w_{\\cdot j}^{\\mathcal{B}} - \\bar{w}_{\\cdot j}\\)\nIf \\(w_{\\cdot j}^{\\mathcal{A}} - \\bar{w}_{\\cdot j}\\) is very positive, make it large for the \\(\\mathcal{A}\\) word cloud. If \\(w_{\\cdot j}^{\\mathcal{B}} - \\bar{w}_{\\cdot j}\\) is very positive, make it large for the \\(\\mathcal{B}\\) word cloud."
  },
  {
    "objectID": "lectures/11-text-data.html#comparison-clouds-2",
    "href": "lectures/11-text-data.html#comparison-clouds-2",
    "title": "Visualizations for text data",
    "section": "Comparison clouds",
    "text": "Comparison clouds"
  },
  {
    "objectID": "lectures/11-text-data.html#tf-idf-weighting",
    "href": "lectures/11-text-data.html#tf-idf-weighting",
    "title": "Visualizations for text data",
    "section": "TF-IDF weighting",
    "text": "TF-IDF weighting\n\nWe saw that michael was the largest word, but what if I’m interested in comparing text across characters (i.e., documents)?\n\n\n\nIt’s arguably of more interest to understand which words are frequently used in one set of texts but not the other, i.e., which words are unique?\nMany text analytics methods will down-weight words that occur frequently across all documents\n\n\n\n\nInverse document frequency (IDF): for word \\(j\\) we compute \\(\\text{idf}_j = \\log \\frac{N}{N_j}\\)\n\nwhere \\(N\\) is number of documents, \\(N_j\\) is number of documents with word \\(j\\)\n\nCompute TF-IDF \\(= w_{ij} \\times \\text{idf}_j\\)"
  },
  {
    "objectID": "lectures/11-text-data.html#tf-idf-example-with-characters",
    "href": "lectures/11-text-data.html#tf-idf-example-with-characters",
    "title": "Visualizations for text data",
    "section": "TF-IDF example with characters",
    "text": "TF-IDF example with characters\nCompute and join TF-IDF using bind_tf_idf():\n\ncharacter_token_summary &lt;- tidy_dinner_party_tokens |&gt;\n  group_by(character, stem) |&gt; \n  count() |&gt;\n  ungroup() \n\ncharacter_token_summary &lt;- character_token_summary |&gt;\n  bind_tf_idf(stem, character, n) \ncharacter_token_summary\n\n# A tibble: 597 × 6\n   character stem        n     tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 All       cheer       1 1      2.77  2.77  \n 2 Andy      anim        1 0.0476 2.77  0.132 \n 3 Andy      bet         1 0.0476 2.08  0.0990\n 4 Andy      capit       1 0.0476 2.77  0.132 \n 5 Andy      dinner      1 0.0476 0.981 0.0467\n 6 Andy      flower      2 0.0952 2.77  0.264 \n 7 Andy      hei         1 0.0476 1.39  0.0660\n 8 Andy      helena      1 0.0476 2.77  0.132 \n 9 Andy      hump        2 0.0952 2.77  0.264 \n10 Andy      michael     1 0.0476 0.981 0.0467\n# ℹ 587 more rows"
  },
  {
    "objectID": "lectures/11-text-data.html#top-10-words-by-tf-idf-for-each-character",
    "href": "lectures/11-text-data.html#top-10-words-by-tf-idf-for-each-character",
    "title": "Visualizations for text data",
    "section": "Top 10 words by TF-IDF for each character",
    "text": "Top 10 words by TF-IDF for each character\n\ncharacter_token_summary |&gt;\n  filter(character %in% c(\"Michael\", \"Jan\", \"Jim\", \"Pam\")) |&gt;\n  group_by(character) |&gt;\n  slice_max(tf_idf, n = 10, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  mutate(stem = reorder_within(stem, tf_idf, character)) |&gt;\n  ggplot(aes(y = tf_idf, x = stem),\n         fill = \"darkblue\", alpha = 0.5) +\n  geom_col() +\n  coord_flip() +\n  scale_x_reordered() +\n  facet_wrap(~ character, ncol = 2, scales = \"free\") +\n  labs(y = \"TF-IDF\", x = NULL)"
  },
  {
    "objectID": "lectures/11-text-data.html#top-10-words-by-tf-idf-for-each-character-output",
    "href": "lectures/11-text-data.html#top-10-words-by-tf-idf-for-each-character-output",
    "title": "Visualizations for text data",
    "section": "Top 10 words by TF-IDF for each character",
    "text": "Top 10 words by TF-IDF for each character"
  },
  {
    "objectID": "lectures/11-text-data.html#sentiment-analysis",
    "href": "lectures/11-text-data.html#sentiment-analysis",
    "title": "Visualizations for text data",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\nThe visualizations so far only look at word frequency (possibly weighted with TF-IDF), but doesn’t tell you how words are used\n\n\n\nA common goal in text analysis is to try to understand the overall sentiment or “feeling” of text, i.e., sentiment analysis\nTypical approach:\n\nFind a sentiment dictionary (e.g., “positive” and “negative” words)\nCount the number of words belonging to each sentiment\nUsing the counts, you can compute an “average sentiment” (e.g., positive counts - negative counts)\n\n\n\n\n\nThis is called a dictionary-based approach\nThe Bing dictionary (named after Bing Liu) provides 6,786 words that are either “positive” or “negative”"
  },
  {
    "objectID": "lectures/11-text-data.html#character-sentiment-analysis",
    "href": "lectures/11-text-data.html#character-sentiment-analysis",
    "title": "Visualizations for text data",
    "section": "Character sentiment analysis",
    "text": "Character sentiment analysis\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows"
  },
  {
    "objectID": "lectures/11-text-data.html#character-sentiment-analysis-1",
    "href": "lectures/11-text-data.html#character-sentiment-analysis-1",
    "title": "Visualizations for text data",
    "section": "Character sentiment analysis",
    "text": "Character sentiment analysis\nJoin sentiment to token table (without stemming)\n\ntidy_all_tokens &lt;- dinner_party_table |&gt;\n  unnest_tokens(word, text)\n\ntidy_sentiment_tokens &lt;- tidy_all_tokens |&gt;\n  inner_join(get_sentiments(\"bing\")) \n\nhead(tidy_sentiment_tokens)\n\n# A tibble: 6 × 4\n  index character word       sentiment\n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    \n1 16791 Stanley   ridiculous negative \n2 16793 Michael   likes      positive \n3 16793 Michael   work       positive \n4 16795 Michael   enough     positive \n5 16795 Michael   enough     positive \n6 16795 Michael   mad        negative"
  },
  {
    "objectID": "lectures/11-text-data.html#character-sentiment-analysis-2",
    "href": "lectures/11-text-data.html#character-sentiment-analysis-2",
    "title": "Visualizations for text data",
    "section": "Character sentiment analysis",
    "text": "Character sentiment analysis\n\ntidy_sentiment_tokens |&gt;\n  group_by(character, sentiment) |&gt;\n  summarize(n_words = n()) |&gt;\n  ungroup() |&gt;\n  group_by(character) |&gt;\n  mutate(total_assigned_words = sum(n_words)) |&gt;\n  ungroup() |&gt;\n  mutate(character = fct_reorder(character, total_assigned_words)) |&gt;\n  ggplot(aes(x = character, y = n_words, fill = sentiment)) + \n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/11-text-data.html#character-sentiment-analysis-2-output",
    "href": "lectures/11-text-data.html#character-sentiment-analysis-2-output",
    "title": "Visualizations for text data",
    "section": "Character sentiment analysis",
    "text": "Character sentiment analysis"
  },
  {
    "objectID": "lectures/11-text-data.html#other-functions-of-text",
    "href": "lectures/11-text-data.html#other-functions-of-text",
    "title": "Visualizations for text data",
    "section": "Other functions of text",
    "text": "Other functions of text\n\nWe’ve just focused on word counts - but there are many functions of text\nFor example: number of unique words is often used to measure vocabulary"
  },
  {
    "objectID": "lectures/11-text-data.html#recap-and-next-steps",
    "href": "lectures/11-text-data.html#recap-and-next-steps",
    "title": "Visualizations for text data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nMost common representation: Bag of words and term frequencies (possibly weighted by TF-IDF)\n\n\n\nWord clouds are the most common way to visualize the most frequent words in a set of documents\nTF-IDF weighting allows you to detect words that are uniquely used in certain documents\n\n\n\n\nCan also measure the “sentiment” of text with sentiment-based dictionaries\n\n\n\n\nInfographic is due Friday night!\nText Mining With R, Supervised Machine Learning for Text Analysis in R\n\n\n\n\nmads-36613-fall24"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Lecture\nDate\nTitle\nMaterials\n\n\n\n\n1\nAug 26\nIntroduction and Grammar of Graphics\nslides\n\n\n2\nAug 28\nPrinciples and 1D Categorical Data\nslides\n\n\n3\nSept 4\n2D Categorical and 1D Quantitative Data\nslides\n\n\n4\nSept 9\nVisualizing Quantitative Distributions\nslides\n\n\n5\nSept 11\n2D Quantitative Data\nslides\n\n\n6\nSept 16\nInto High-Dimensional Data\nslides\n\n\n7\nSept 18\nHigh-Dimensional Data\nslides\n\n\n8\nSept 23\nt-SNE + visualizing trends and time series data\nslides\n\n\n9\nSept 25\nTime series and intro to spatial data\nslides\n\n\n10\nSept 30\nAreal data and creating high-quality graphics\nslides\n\n\n11\nOct 7\nVisualizing text data\nslides",
    "crumbs": [
      "Lectures"
    ]
  },
  {
    "objectID": "demos/06-tsne-umap.html",
    "href": "demos/06-tsne-umap.html",
    "title": "Introduction to t-SNE and UMAP",
    "section": "",
    "text": "Throughout this demo we will again use the dataset about Starbucks drinks available in the #TidyTuesday project.\nYou can read in and manipulate various columns in the dataset with the following code:\nlibrary(tidyverse)\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))"
  },
  {
    "objectID": "demos/06-tsne-umap.html#stochastic-neighbor-embedding-sne",
    "href": "demos/06-tsne-umap.html#stochastic-neighbor-embedding-sne",
    "title": "Introduction to t-SNE and UMAP",
    "section": "Stochastic Neighbor Embedding (SNE)",
    "text": "Stochastic Neighbor Embedding (SNE)\nThe general idea behind SNE is to convert the distances between observations into conditional probabilities that represent similarities. We then match the conditional probability \\(P\\) in the original high dimensional space \\(\\mathcal{X}\\) with the conditional probability \\(Q\\) in a new low dimensional space \\(\\mathcal{Y}\\). This is done by minimizing the distance between these probability distributions based on KL divergence. This is done by assuming that the distance in both the high and low dimensional space are Gaussian-distributed.\nLet \\(x_i\\) be the \\(i^{th}\\) observation in the original, high-dimensional space \\(\\mathcal{X}\\). Let \\(y_i\\) be the \\(i^{th}\\) observation in the new, low-dimensional space \\(\\mathcal{Y}\\). We construct the conditional probability for similarity in the high-dimensional space \\(\\mathcal{X}\\) as:\n\\[p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|x_i-x_j\\right\\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|x_i-x_k\\right\\|^2 / 2 \\sigma_i^2\\right)}\\]\nwhere \\(\\sigma_i\\) is the variance of Gaussian centered at \\(x_i\\) controlled by perplexity: \\(\\log (\\text { perplexity })=-\\sum_j p_{j \\mid i} \\log _2 p_{j \\mid i}\\). The perplexity is loosely interpreted as the number of close neighbors to consider for each point.\nWe then construct the conditional probability for similarity in the low-dimensional space \\(\\mathcal{Y}\\) as:\n\\[q_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|y_i-y_j\\right\\|^2 \\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|y_i-y_k\\right\\|^2 \\right)}\\]\nwhere we set the variance equal to \\(\\frac{1}{\\sqrt{2}}\\).\nWe then proceed to match the conditional probabilities by minimizing the sum of KL divergences (via gradient descent):\n\\[C=\\sum_i K L\\left(P_i|| Q_i\\right)=\\sum_i \\sum_j p_{j \\mid i} \\log \\left(\\frac{p_{j \\mid i}}{q_{j \\mid i}}\\right)\\]"
  },
  {
    "objectID": "demos/06-tsne-umap.html#t-sne-modifications",
    "href": "demos/06-tsne-umap.html#t-sne-modifications",
    "title": "Introduction to t-SNE and UMAP",
    "section": "t-SNE modifications",
    "text": "t-SNE modifications\nOne of the major drawbacks of SNE is the crowding problem: in high dimensions we have more room and points can have a lot of different neighbors, but we don’t have enough room for neighbors in low dimensions. t-SNE modifies the above SNE framework by replacing the Gaussian in the low dimensional space with a heavy tailed \\(t\\)-distribution. Specifically, the Student t-distribution with df=1 (Cauchy) is used in low dimensional space instead of the Gaussian distribution to address the crowding problem.\n\n\n\n\n\n\n\n\n\nAnother modification by t-SNE is to use symmetrized conditional probability: \\(p_{i \\mid j}=p_{j \\mid i}\\) and \\(q_{i \\mid j}=q_{j \\mid i}\\). In comparison to SNE, for t-SNE we construct the conditional probability \\(P\\) for similarity in the high-dimensional space \\(\\mathcal{X}\\) as:\n\\[p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|x_i-x_j\\right\\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|x_i-x_k\\right\\|^2 / 2 \\sigma_i^2\\right)} \\quad p_{i j}=\\frac{\\left(p_{j \\mid i}+p_{i \\mid j}\\right)}{2 n}\\]\nAnd construct the conditional probability \\(Q\\) for similarity in the low-dimensional space \\(\\mathcal{Y}\\) as:\n\\[q_{j \\mid i}=\\frac{\\left(1+\\left\\|y_i-y_j\\right\\|^2\\right)^{-1}}{\\sum_{k \\neq i}\\left(1+\\left\\|y_i-y_k\\right\\|^2\\right)^{-1}}, \\quad q_{i j}=\\frac{q_{i \\mid j}+q_{j \\mid i}}{2 n}\\] We then match the conditional probabilities by minimizing the sum of KL divergences:\n\\[C=\\sum_{i j} p_{i j} \\log \\left(\\frac{p_{i j}}{q_{i j}}\\right)\\]"
  },
  {
    "objectID": "demos/06-tsne-umap.html#t-sne-example-with-starbucks-data",
    "href": "demos/06-tsne-umap.html#t-sne-example-with-starbucks-data",
    "title": "Introduction to t-SNE and UMAP",
    "section": "t-SNE example with Starbucks data",
    "text": "t-SNE example with Starbucks data\nWe use the Rtsne package for constructing t-SNE plots:\n\nlibrary(Rtsne)\n\nWarning: package 'Rtsne' was built under R version 4.2.3\n\nset.seed(2013)\ntsne_fit &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) |&gt;\n  scale() |&gt;\n  Rtsne(check_duplicates = FALSE) #&lt;&lt;\n\nstarbucks |&gt;\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |&gt;\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) + \n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")\n\n\n\n\n\n\n\n\nThis just constructs the t-SNE plot with the defaults, but remember the discussion in lecture about the impact of the perplexity parameter (as well as randomness!)."
  },
  {
    "objectID": "demos/06-tsne-umap.html#criticisms-of-t-sne-plots",
    "href": "demos/06-tsne-umap.html#criticisms-of-t-sne-plots",
    "title": "Introduction to t-SNE and UMAP",
    "section": "Criticisms of t-SNE plots",
    "text": "Criticisms of t-SNE plots\n\nPoor scalability: does not scale well for large data, can practically only embed into 2 or 3 dimensions\nMeaningless global structure: distance between clusters might not have clear interpretation and cluster size doesn’t have any meaning to it\nPoor performance with very high dimensional data: need PCA as pre-dimension reduction step\nSometime random noise can lead to false positive structure in the t-SNE projection\nCan NOT interpret like PCA!"
  },
  {
    "objectID": "demos/06-tsne-umap.html#umap-example-with-starbucks-data",
    "href": "demos/06-tsne-umap.html#umap-example-with-starbucks-data",
    "title": "Introduction to t-SNE and UMAP",
    "section": "UMAP example with Starbucks data",
    "text": "UMAP example with Starbucks data\nWe use the umap package for constructing UMAP plots:\n\nlibrary(umap)\numap_fit &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) |&gt;\n  scale() |&gt;\n  umap() \n\n# Convert umap_fit to table\numap_table &lt;- umap_fit$layout |&gt;\n  as.data.frame()\n\n# Display\nstarbucks |&gt;\n  mutate(umap1 = umap_table[,1],\n         umap2 = umap_table[,2]) |&gt;\n  ggplot(aes(x = umap1, y = umap2, \n             color = size)) +\n  geom_point(alpha = 0.5) + \n  labs(x = \"UMAP 1\", y = \"UMAP 2\")\n\n\n\n\n\n\n\n\nThe plot above was just based on the defaults:\n\numap.defaults\n\numap configuration parameters\n\n\n           n_neighbors: 15\n\n\n          n_components: 2\n\n\n                metric: euclidean\n\n\n              n_epochs: 200\n\n\n                 input: data\n\n\n                  init: spectral\n\n\n              min_dist: 0.1\n\n\n      set_op_mix_ratio: 1\n\n\n    local_connectivity: 1\n\n\n             bandwidth: 1\n\n\n                 alpha: 1\n\n\n                 gamma: 1\n\n\n  negative_sample_rate: 5\n\n\n                     a: NA\n\n\n                     b: NA\n\n\n                spread: 1\n\n\n          random_state: NA\n\n\n       transform_state: NA\n\n\n                   knn: NA\n\n\n           knn_repeats: 1\n\n\n               verbose: FALSE\n\n\n       umap_learn_args: NA\n\n\nWe can choose a different number of neighbors amongst other options. We can customize UMAP by setting up a custom_config list to store the parameter values we want to change from the defaults. Similar to perplexity in t-SNE plots, the most important one is n-neighbors. Other important ones are random_state, which is an arbitrary integer that determines the initial random state (i.e., the seed), and n_epochs, which determines for how many steps the UMAP algorithm is run until it is considered converged (higher is better but just takes longer).\n\n# set up umap configuration  \ncustom_config &lt;- umap.defaults\ncustom_config$n_neighbors &lt;- 50        \ncustom_config$random_state &lt;- 1234\ncustom_config$n_epochs &lt;- 500\n\n# Re-run the UMAP\ncustom_umap_fit &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) |&gt;\n  scale() |&gt;\n  umap(config = custom_config) \n\n# Convert umap_fit to table\ncustom_umap_table &lt;- custom_umap_fit$layout |&gt;\n  as.data.frame()\n\n# Display\nstarbucks |&gt;\n  mutate(umap1 = custom_umap_table[,1],\n         umap2 = custom_umap_table[,2]) |&gt;\n  ggplot(aes(x = umap1, y = umap2, \n             color = size)) +\n  geom_point(alpha = 0.5) + \n  labs(x = \"UMAP 1\", y = \"UMAP 2\")\n\n\n\n\n\n\n\n\nExercise: Run the above code for a different values of the custom_config parameters. Pay attention to how the output changes as you change each of these parameters."
  },
  {
    "objectID": "demos/04-mds.html",
    "href": "demos/04-mds.html",
    "title": "Demo 04: Multi-Dimensional Scaling",
    "section": "",
    "text": "So far…\nWe’ve been working with “tidy data” – data that has \\(n\\) rows and \\(p\\) columns, where each row is an observation, and each column is a variable describing some feature of each observation.\nNow we’ll discuss more complicated data structures.\n\n\nDistance Matrices\nA distance matrix is a data structure that specifies the “distance” between each pair of observations in the original \\(n\\)-row, \\(p\\)-column dataset. For each pair of observations (e.g. \\(x_i, x_j\\)) in the original dataset, we compute the distance between those observations, denoted as \\(d(x_i, x_j)\\) or \\(d_{ij}\\) for short.\nA variety of approaches for calculating the distance between a pair of observations can be used. The most commonly used approach (when we have quantitative variables) is called “Euclidean Distance”. The Euclidean distance between observations \\(x_i\\) and \\(x_j\\) is defined as follows: \\(d(x_i, x_j) = \\sqrt{\\sum_{l = 1}^p (x_{i,l} - x_{j,l}) ^ 2}\\). That is, it is the square root of the sum of squared differences between each column (\\(l \\in \\{1, ..., p\\}\\)) of \\(x_i\\) and \\(x_j\\) (remember, there are \\(p\\) original columns / variables).\nNote that if some variables in our dataset have substantially higher variance than others, the high-variance variables will dominate the calculation of distance, skewing our resulting distances towards the differences in these variables. As such, it’s common to scale the original dataset before calculating the distance, so that each variable is on the same scale.\n\n\nStarbucks Drinks\nIn this R demo we will look at Starbucks drinks (courtesy of the #TidyTuesday project). In short, this is a dataset containing nutritional information about Starbucks drinks. We’re going to consider all of the quantitative variables in this dataset, starting with fifth column serv_size_m_l to the final column caffeine_mg. You can read about the columns in the dataset here.\n\nlibrary(tidyverse)\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))\n\nAfter selecting the desired columns, the first thing we’re going to do is use the scale() function to ensure each variable on the same scale, i.e., variances are equal to 1.\n\n# Select the variables of interest:\nstarbucks_scaled_quant_data &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg) %&gt;%\n  # Now scale each column so that the variance is 1 using the scale function:\n  # We specify here to not center the data and need to follow the directions in\n  # the help page of scale to ensure we are properly standardizing the variance\n  scale(center = FALSE, scale = apply(., 2, sd, na.rm = TRUE))\n# Just for reference - this is equivalent to the following commented out code:\n# starbucks_quant_data &lt;- starbucks |&gt;\n#   dplyr::select(serv_size_m_l:caffeine_mg)\n# starbucks_scaled_quant_data &lt;- apply(starbucks_quant_data, MARGIN = 2,\n#                                      FUN = function(x) x / sd(x))\n\nThe most common way to compute distances in R is to use the dist function. This takes in a dataset and returns the distance matrix for that dataset. By default this computes the euclidean distance (method = \"euclidean\"), but other distance metrics can be used.\n\n# Calculate distance matrix.\n# As an example, we'll just look at the first five rows:\ndist(starbucks_scaled_quant_data[1:5,])\n\n         1        2        3        4\n2 1.059790                           \n3 2.160501 1.101588                  \n4 3.388562 2.331643 1.232345         \n5 1.472299 2.380300 3.425819 4.643997\n\n# You can also include the diagonal if you want:\n# (the diagonal will always be 0s)\ndist(starbucks_scaled_quant_data[1:5,], diag = T)\n\n         1        2        3        4        5\n1 0.000000                                    \n2 1.059790 0.000000                           \n3 2.160501 1.101588 0.000000                  \n4 3.388562 2.331643 1.232345 0.000000         \n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# You can also include the \"upper triangle\" if you want:\ndist(starbucks_scaled_quant_data[1:5,], upper = T)\n\n         1        2        3        4        5\n1          1.059790 2.160501 3.388562 1.472299\n2 1.059790          1.101588 2.331643 2.380300\n3 2.160501 1.101588          1.232345 3.425819\n4 3.388562 2.331643 1.232345          4.643997\n5 1.472299 2.380300 3.425819 4.643997         \n\n# Can also include both:\n# (this is the full distance matrix)\ndist(starbucks_scaled_quant_data[1:5,], diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.059790 2.160501 3.388562 1.472299\n2 1.059790 0.000000 1.101588 2.331643 2.380300\n3 2.160501 1.101588 0.000000 1.232345 3.425819\n4 3.388562 2.331643 1.232345 0.000000 4.643997\n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# Can also consider other distance metrics\n# The default is euclidean, as you can see below:\n# (compare to what you see above)\ndist(starbucks_scaled_quant_data[1:5,], method = \"euclidean\", diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.059790 2.160501 3.388562 1.472299\n2 1.059790 0.000000 1.101588 2.331643 2.380300\n3 2.160501 1.101588 0.000000 1.232345 3.425819\n4 3.388562 2.331643 1.232345 0.000000 4.643997\n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# For example, can consider the Manhattan distance:\ndist(starbucks_scaled_quant_data[1:5,], method = \"manhattan\", diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.552865 3.109038 4.818573 1.472299\n2 1.552865 0.000000 1.556173 3.265708 3.025165\n3 3.109038 1.556173 0.000000 1.709535 4.581337\n4 4.818573 3.265708 1.709535 0.000000 6.290872\n5 1.472299 3.025165 4.581337 6.290872 0.000000\n\n\nFor the purposes of this class, we’ll mostly focus on the Euclidean distance, so let’s define that here:\n\ndist_euc &lt;- dist(starbucks_scaled_quant_data)\n\n\n\nImplementing Multi-dimensional Scaling\nNow we will implement multi-dimensional scaling (MDS) in R. As a reminder, MDS tries to find the “best” \\(k\\)-dimensional projection of the original \\(p\\)-dimensional dataset (\\(k &lt; p\\)).\nAs such, MDS tries to preserve the order of the pairwise distances. That is, pairs of observations with low distances in the original \\(p\\)-column dataset will still be have low distances in the smaller \\(k\\)-column dataset. Similarly, pairs of observations with high distances in the original \\(p\\)-column dataset will still be have high distances in the smaller \\(k\\)-column dataset.\nMDS can be implemented in R using the cmdscale function. This function takes a distance matrix (not a dataset!!):\n\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\nNote that you can change \\(k\\) to be greater than 2 if you want, but usually we want \\(k = 2\\) so that we can plot the (projected) distances in a scatterplot; see below.\nFor the purposes of plotting, let’s add the two coordinates of mds to our original dataset:\n\nstarbucks &lt;- starbucks |&gt;\n  mutate(mds1 = starbucks_mds[,1],\n         mds2 = starbucks_mds[,2])\n\nThen, we can make a plot with ggplot:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nIt can be helpful to add colors and/or shapes of the plot according to categorical variables. For example, here’s the plot colored by size:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nTo get some insight into the contributions by the different continous variables, we could also map them to various aesthetics. For example, the following plots displays points colored by sugar_g:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = sugar_g)) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\") +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nWhat do these two colored plots tell us about the data?"
  },
  {
    "objectID": "demos/08-intro-spatial-data.html",
    "href": "demos/08-intro-spatial-data.html",
    "title": "Demo 08: Visualizations and inference for spatial data",
    "section": "",
    "text": "Before moving on to the code below, you’ll first need to create a free Stadia Maps account in order to access the map styles to use with ggmap. You can find the directions about the process here. After you click on the “Create an Account” button, follow the steps by entering in your relevant information. Then select Mobile/Native App version to generate an API key. You’ll then find your API key in the displayed Stadia-hosted URLs, displayed at the end of the URLs after api_key=. Once you have the API key saved somewhere, after you install the ggmap package you can then run the following code to save your API key once and then never have to run this step again:\n\n# You first need to install the ggmap R package\n# install.packages(\"ggmap\")\nggmap:register_stadiamaps(\"YOUR-API-KEY-HERE\",\n                          # write = TRUE saves this once\n                          write = TRUE)"
  },
  {
    "objectID": "demos/08-intro-spatial-data.html#example-plotting-airline-flight-data-across-the-us",
    "href": "demos/08-intro-spatial-data.html#example-plotting-airline-flight-data-across-the-us",
    "title": "Demo 08: Visualizations and inference for spatial data",
    "section": "Example: Plotting Airline Flight Data across the US",
    "text": "Example: Plotting Airline Flight Data across the US\nHere, we’ll work with airline data from this GitHub repository. You can find out more about this dataset here.\nBefore we begin, note that this is just one example of how you can add interesting information to maps with ggmap. As long as you have latitude and longitude information, you should be able to add data to maps. For more interesting examples and for an in-depth description of ggmap, see the short paper by David Kahle and Hadley Wickham here. (This paper may be helpful for teams that will be working with spatial data for the group project.)"
  },
  {
    "objectID": "demos/08-intro-spatial-data.html#load-flight-data-from-github",
    "href": "demos/08-intro-spatial-data.html#load-flight-data-from-github",
    "title": "Demo 08: Visualizations and inference for spatial data",
    "section": "Load flight data from GitHub",
    "text": "Load flight data from GitHub\nHere we will use a large dataset on GitHub about airline flights across the world. After loading in the raw data, we’ll do some data manipulation to create useful variables for this dataset.\nFirst, we’ll load a dataset on airports across the world:\n\n#  Load and format airports data\nairports &lt;- read_csv(\"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\",\n                     col_names = c(\"ID\", \"name\", \"city\", \"country\", \"IATA_FAA\", \n                                   \"ICAO\", \"lat\", \"lon\", \"altitude\", \"timezone\", \"DST\"))\n# Here's what the data look like:\nairports\n\n# A tibble: 7,698 × 14\n      ID name   city  country IATA_FAA ICAO    lat   lon altitude timezone DST  \n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;\n 1     1 Gorok… Goro… Papua … GKA      AYGA  -6.08 145.      5282 10       U    \n 2     2 Madan… Mada… Papua … MAG      AYMD  -5.21 146.        20 10       U    \n 3     3 Mount… Moun… Papua … HGU      AYMH  -5.83 144.      5388 10       U    \n 4     4 Nadza… Nadz… Papua … LAE      AYNZ  -6.57 147.       239 10       U    \n 5     5 Port … Port… Papua … POM      AYPY  -9.44 147.       146 10       U    \n 6     6 Wewak… Wewak Papua … WWK      AYWK  -3.58 144.        19 10       U    \n 7     7 Narsa… Nars… Greenl… UAK      BGBW  61.2  -45.4      112 -3       E    \n 8     8 Godth… Godt… Greenl… GOH      BGGH  64.2  -51.7      283 -3       E    \n 9     9 Kange… Sond… Greenl… SFJ      BGSF  67.0  -50.7      165 -3       E    \n10    10 Thule… Thule Greenl… THU      BGTL  76.5  -68.7      251 -4       E    \n# ℹ 7,688 more rows\n# ℹ 3 more variables: X12 &lt;chr&gt;, X13 &lt;chr&gt;, X14 &lt;chr&gt;\n\n\nThen, we’ll load a dataset on airline flight routes across the world:\n\n#  Load and format routes data\nroutes &lt;- read_csv(\"https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\",\n                   col_names = c(\"airline\", \"airlineID\", \"sourceAirport\", \n                                 \"sourceAirportID\", \"destinationAirport\", \n                                 \"destinationAirportID\", \"codeshare\", \"stops\",\n                                 \"equipment\"))\n# Here's what the data look like:\nroutes\n\n# A tibble: 67,663 × 9\n   airline airlineID sourceAirport sourceAirportID destinationAirport\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt;             \n 1 2B      410       AER           2965            KZN               \n 2 2B      410       ASF           2966            KZN               \n 3 2B      410       ASF           2966            MRV               \n 4 2B      410       CEK           2968            KZN               \n 5 2B      410       CEK           2968            OVB               \n 6 2B      410       DME           4029            KZN               \n 7 2B      410       DME           4029            NBC               \n 8 2B      410       DME           4029            TGK               \n 9 2B      410       DME           4029            UUA               \n10 2B      410       EGO           6156            KGD               \n# ℹ 67,653 more rows\n# ℹ 4 more variables: destinationAirportID &lt;chr&gt;, codeshare &lt;chr&gt;, stops &lt;dbl&gt;,\n#   equipment &lt;chr&gt;"
  },
  {
    "objectID": "demos/08-intro-spatial-data.html#manipulating-the-data-to-get-some-custom-tibbles",
    "href": "demos/08-intro-spatial-data.html#manipulating-the-data-to-get-some-custom-tibbles",
    "title": "Demo 08: Visualizations and inference for spatial data",
    "section": "Manipulating the data to get some custom tibbles",
    "text": "Manipulating the data to get some custom tibbles\nHere, we’ll do some data manipulation to obtain the number of arrivals/departures per airport.\nThe following code does two tasks that very commonly come up with spatial data:\n\nCount the number of events at a particular location.\nMerge two datasets by a common variable (e.g., an ID or location).\n\nWe’ve seen instances of the first task before (e.g., counting the number of subjects belonging to a certain group). We haven’t done the second task before, but it regularly comes up in spatial data: Often, you’ll find multiple datasets that contain different variables about the same spatial location, and you’ll want to merge them together. We’ll do this using the left_join() function within the dplyr package that’s in the tidyverse.\n\n#  Manipulate the routes data to create two new tibbles:\n#  one for arrivals, one for departures.  \n\n#  Each counts the number of flights arriving/departing from each airport.\ndepartures &lt;- routes |&gt; \n  group_by(sourceAirportID) |&gt;\n  summarize(n_depart = n()) |&gt;\n  # Convert the ID to integer since it's a character (need to have matching\n  # data types for columns you will join on)\n  mutate(sourceAirportID = as.integer(sourceAirportID))\n\narrivals &lt;- routes |&gt; \n  group_by(destinationAirportID) |&gt; \n  summarize(n_arrive = n()) |&gt; \n  mutate(destinationAirportID = as.integer(destinationAirportID))\n\n# Join both of these summaries to the airports data bsed on the ID column to\n# match their respective ID columns\n\n# First join the departures:\nairports &lt;- airports |&gt;\n  # We join the departures so that ID matches sourceAirportID\n  left_join(departures, by = c(\"ID\" = \"sourceAirportID\"))\n\n# And now join arrivals:\nairports &lt;- airports |&gt;\n  # Same process - but different ID\n  left_join(arrivals, by = c(\"ID\" = \"destinationAirportID\"))\n\nThus, each row corresponds to a single airport with columns about the airport along with info about the number of flights (departures and arrivals)."
  },
  {
    "objectID": "demos/08-intro-spatial-data.html#mapping-the-data-we-created",
    "href": "demos/08-intro-spatial-data.html#mapping-the-data-we-created",
    "title": "Demo 08: Visualizations and inference for spatial data",
    "section": "Mapping the data we created",
    "text": "Mapping the data we created\nFirst, we’ll use ggmap to create a base map of the data. Although the dataset is for the whole world, we’ll focus on the US for simplicity.\n\n# First, we'll draw a \"box\" around the US\n# (in terms of latitude and longitude)\nUS &lt;- c(left = -125, bottom = 10, right = -67, top = 49)\nmap &lt;- get_stadiamap(US, zoom = 5, maptype = \"stamen_toner_lite\")\n#  Visualize the basic map\nggmap(map)\n\n\n\n\n\n\n\n\nNow we’ll demonstrate how to add points to the map using geom_point(). Thus, this is equivalent to creating a scatterplot, but within the space of a map.\n\n#  Add points to the map of airports\nggmap(map) + \n  geom_point(data = airports, aes(x = lon, y = lat), alpha = .25)\n\n\n\n\n\n\n\n\nWe can also add contour lines using stat_density2d(), just like we did with scatterplots. In the code below, we also use the bins argument, which we haven’t seen before in stat_density2d, but intuitively it is the same as when we used bins for histograms: More bins corresponds to more granularity in the data. I’ve found that not specifying bins can give very odd results for spatial data, so I recommend setting bins to a somewhat low value such that you can easily see high- and low-density areas on a map.\n\nggmap(map) +\n  stat_density2d(data = airports,\n                 aes(x = lon, y = lat, fill = after_stat(level)),\n                 alpha = 0.2, geom = \"polygon\", bins = 4) +\n  geom_point(data = airports, aes(x = lon, y = lat), alpha = .25)\n\n\n\n\n\n\n\n\nNow, think about what the above map is displaying. It is density of airports (which correspond to the rows of our dataset), but it is not displaying the density of flights (arrivals or departures). How can we display the density of flights on this map?\nA common option is to make the size of the points proportional to a third variable (in this case departures):\n\n#  Add points to the map of airports\n#  Each point will be located at the lat/long of the airport\n#  The size of the points is proportional to the square root of the number of departures at that airport\nmap_point_depart &lt;- ggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, size = sqrt(n_depart)), alpha = .25)\n\n# In this case, it can be helpful to create your own legend for the plot, so \n# that you can set which sizes of the correspond to which values in the data:\nmap_point_depart &lt;- map_point_depart + \n  scale_size_area(breaks = sqrt(c(1, 5, 10, 50, 100, 500)), \n                  labels = c(1, 5, 10, 50, 100, 500), \n                  name = \"# departures\")\nmap_point_depart\n\n\n\n\n\n\n\n\nYou could also change the color of the points. However, as we’ve seen previously, just setting color = variable for a continuous variable often gives unsatisfactory results (because the default colors aren’t great). This is especially true for spatial data, where there is often a wide range of responses across geography.\nIn what follows, I demonstrate two ways that you can manually change the colors on a map; one uses scale_color_gradient2(), and one uses scale_color_distiller. The second one is more straightforward, but the first one allows for more flexibility.\n\n# What happens if we just set sqrt(n_depart) equal to color\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, color = sqrt(n_depart)), alpha = .5)\n\n\n\n\n\n\n\n# Using scale_color_gradient2 to manually set the color scale\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, color = sqrt(n_depart)), alpha = .5) +\n  # Note that you can also set a \"mid\" color, which is what the color will be \n  # at the midpoint (which you also set). Using limits (or breaks, \n  # which I don't do here) is very useful when your variable has a wide range \n  # and/or is very skewed (which is the case for the n_depart variable).\n  scale_color_gradient2(low = \"blue\", mid = \"purple\", high = \"red\", \n                        midpoint = 12.5, limits = sqrt(c(1, 500)))\n\n\n\n\n\n\n\n# Using scale_color_distiller to manually set the color scale\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, color = sqrt(n_depart)), alpha = .5) +\n  # Note that there are many different palettes - see help documentation\n  # for scale_color_distiller\n  scale_color_distiller(palette = \"Spectral\") \n\n\n\n\n\n\n\n\nWe could of course map one variable to size and another to color, e.g., n_depart to size and n_arrive to color. You should NOT map one variable to both aesthetics - that is a redundant display of information.\n\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, \n                 size = sqrt(n_depart), color = sqrt(n_arrive)), \n             alpha = .5) +\n  scale_size_area(breaks = sqrt(c(1, 5, 10, 50, 100, 500)), \n                  labels = c(1, 5, 10, 50, 100, 500), \n                  name = \"# departures\") +\n  scale_color_distiller(palette = \"Spectral\") +\n  labs(color = \"sqrt(# arrivals)\")\n\n\n\n\n\n\n\n\nFinally, we can (as usual) change the shape of the points according to another variable. The following marks airports that have an altitude higher than 1500 feet (which I don’t think would normally be of scientific interest, but this nonetheless shows you how to change shape in this way):\n\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, \n                 size = sqrt(n_depart), color = sqrt(n_arrive),\n                 shape = altitude &gt; 1500), \n             alpha = .5) +\n  scale_size_area(breaks = sqrt(c(1, 5, 10, 50, 100, 500)), \n                  labels = c(1, 5, 10, 50, 100, 500), \n                  name = \"# departures\") +\n  scale_color_distiller(palette = \"Spectral\") +\n  labs(color = \"sqrt(# arrivals)\")\n\n\n\n\n\n\n\n\nThe above plots are nice ways to show the distribution of the number of flights. However, earlier we used stat_density2d to plot the density of airports. How can we use this function to plot the density of flights?\nThis requires a bit of hacking. The stat_density2d will plot the density of points across rows in your dataset. Thus, if we want to look at the density of departures, we can duplicate rows that have multiple departures For example, if an airport has n_depart = 5, we can create 5 rows in a new dataset for that airport. Then, we can use stat_density_2d on this “duplicated” dataset.\n\n# First remove the airports with missing n_depart\nclean_airports &lt;- airports |&gt;\n  filter(!is.na(n_depart)) # ! is the negate operator in R, so this only keeps\n  # rows where n_depart is NOT missing  \n\nairports_duplicated &lt;- clean_airports[rep(row.names(clean_airports), \n                                          clean_airports$n_depart),]\n\nggmap(map) + stat_density2d(data = airports_duplicated,\n                            aes(x = lon, y = lat, fill = after_stat(level)), \n                                alpha = 0.2, geom = \"polygon\")\n\n\n\n\n\n\n\n\nUnsurprisingly, we see there are higher concentrations of flights at major cities. Arguably, it’s much easier to deduce this from the above density plot than from the color plots. But both types of plots (density plots and color plots) serve a purpose, and preference for one or the other will depend on the application and the questions of interest."
  },
  {
    "objectID": "demos/11-text-data.html",
    "href": "demos/11-text-data.html",
    "title": "Demo 11: Visualizations for text data",
    "section": "",
    "text": "In this demo we’ll work with the script from the best episode of ‘The Office’: Season 4, Episode 13 - ‘Dinner Party’. We can access the script using the schrute package (yes this is a real thing):\n\n# install.packages(\"schrute\")\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(schrute)\n\n# Create a table from this package just corresponding to the Dinner Party episode:\ndinner_party_table &lt;- theoffice |&gt;\n  dplyr::filter(season == 4) |&gt;\n  dplyr::filter(episode == 13) |&gt;\n  # Just select columns of interest:\n  dplyr::select(index, character, text)"
  },
  {
    "objectID": "demos/11-text-data.html#sentiment-analysis-1",
    "href": "demos/11-text-data.html#sentiment-analysis-1",
    "title": "Demo 11: Visualizations for text data",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\nSee the tidytext package and tidytext::get_sentiments()\nSee the “Tidy Text Mining in R” book for some examples!\nSentiment Analysis Chapter\nSentiment Analysis of The Lord Of The Rings\nRSentiment Package"
  },
  {
    "objectID": "demos/11-text-data.html#word-embeddings",
    "href": "demos/11-text-data.html#word-embeddings",
    "title": "Demo 11: Visualizations for text data",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\n“King - Man + Woman = Queen”\ntext2vec package on GitHub"
  },
  {
    "objectID": "demos/11-text-data.html#word-relationships",
    "href": "demos/11-text-data.html#word-relationships",
    "title": "Demo 11: Visualizations for text data",
    "section": "Word Relationships",
    "text": "Word Relationships\n\nn-grams, correlations, etc\nSee this chapter in the Tidy Text Mining book."
  },
  {
    "objectID": "demos/11-text-data.html#latent-semantic-analysis-and-topic-models",
    "href": "demos/11-text-data.html#latent-semantic-analysis-and-topic-models",
    "title": "Demo 11: Visualizations for text data",
    "section": "Latent Semantic Analysis and Topic Models",
    "text": "Latent Semantic Analysis and Topic Models\n\n“Analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms”\nLSA of subreddits\nThe Topic Modeling chapter of Tidy Text Mining\nStructural topic models"
  },
  {
    "objectID": "demos/10-colors-themes.html",
    "href": "demos/10-colors-themes.html",
    "title": "Demo 10: Modifying colors and themes",
    "section": "",
    "text": "Throughout this demo we will once again use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\ninstall.packages(\"palmerpenguins\")\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)"
  },
  {
    "objectID": "demos/10-colors-themes.html#options-for-ggplot2-colors",
    "href": "demos/10-colors-themes.html#options-for-ggplot2-colors",
    "title": "Demo 10: Modifying colors and themes",
    "section": "Options for ggplot2 colors",
    "text": "Options for ggplot2 colors\nThe default color scheme is pretty bad to put it bluntly, but ggplot2 has ColorBrewer built in which makes it easy to customize your color scales. For instance, we change the palette for the species plot from before.\n\npenguins |&gt; \n  ggplot(aes(x = body_mass_g, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"Body Mass (g)\", y = \"Bill Length (mm)\") +\n  theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSomething you should keep in mind is to pick a color-blind friendly palette. One simple way to do this is by using the ggthemes package which has color-blind friendly palettes included:\n\npenguins |&gt; \n  ggplot(aes(x = body_mass_g, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5, size = 2) +\n  ggthemes::scale_color_colorblind() +\n  labs(x = \"Body Mass (g)\", y = \"Bill Length (mm)\") +\n  theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn terms of displaying color from low to high, the viridis scales are excellent choices (and are also color-blind friendly!).\n\npenguins |&gt; \n  ggplot(aes(x = body_mass_g, y = bill_length_mm, \n             color = flipper_length_mm)) +\n  geom_point(alpha = 0.5, size = 2) +\n  scale_color_viridis_c() +\n  labs(x = \"Body Mass (g)\", y = \"Bill Length (mm)\",\n       color = \"Flipper Length (mm)\") +\n  theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  }
]