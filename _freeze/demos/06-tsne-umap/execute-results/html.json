{
  "hash": "e87475e9fc01f5a9ab5e02b7da4a8840",
  "result": {
    "markdown": "---\ntitle: \"Introduction to t-SNE and UMAP\"\nformat:\n  html:\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\nThroughout this demo we will again use the dataset about Starbucks drinks available in the [#TidyTuesday project](https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-12-21/readme.md).\n\nYou can read in and manipulate various columns in the dataset with the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nstarbucks <- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |>\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))\n```\n:::\n\n\n# Introduction to t-SNE plots\n\nWe previously talked about PCA, which is a _linear_ dimension reduction technique. [t-SNE (t-Distributed Stochastic Neighbor Embedding)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) is one of the most popular non-linear dimension reduction techniques for visualizing high-dimensional, clustering structure. t-SNE plots are a combination of Student t-distribution + Stochastic Neighbor Embedding (SNE). \n\n## Stochastic Neighbor Embedding (SNE)\n\nThe general idea behind SNE is to convert the distances between observations into conditional probabilities that represent similarities. We then match the conditional probability $P$ in the original high dimensional space $\\mathcal{X}$ with the conditional probability $Q$ in a new low dimensional space $\\mathcal{Y}$. This is done by minimizing the distance between these probability distributions based on KL divergence. This is done by assuming that the distance in both the high and low dimensional space are Gaussian-distributed.\n\nLet $x_i$ be the $i^{th}$ observation in the original, high-dimensional space $\\mathcal{X}$. Let $y_i$ be the $i^{th}$ observation in the new, low-dimensional space $\\mathcal{Y}$. We construct the conditional probability for similarity in the high-dimensional space $\\mathcal{X}$ as:\n\n$$p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|x_i-x_j\\right\\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|x_i-x_k\\right\\|^2 / 2 \\sigma_i^2\\right)}$$\n\nwhere $\\sigma_i$ is the variance of Gaussian centered at $x_i$ controlled by __perplexity__:  $\\log (\\text { perplexity })=-\\sum_j p_{j \\mid i} \\log _2 p_{j \\mid i}$. The __perplexity__ is loosely interpreted as the number of close neighbors to consider for each point. \n\nWe then construct the conditional probability for similarity in the low-dimensional space $\\mathcal{Y}$ as:\n\n$$q_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|y_i-y_j\\right\\|^2 \\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|y_i-y_k\\right\\|^2 \\right)}$$\n\nwhere we set the variance equal to $\\frac{1}{\\sqrt{2}}$. \n\nWe then proceed to match the conditional probabilities by minimizing the sum of KL divergences (via gradient descent):\n\n$$C=\\sum_i K L\\left(P_i|| Q_i\\right)=\\sum_i \\sum_j p_{j \\mid i} \\log \\left(\\frac{p_{j \\mid i}}{q_{j \\mid i}}\\right)$$\n\n## t-SNE modifications \n\nOne of the major drawbacks of SNE is the __crowding problem__: in high dimensions we have more room and points can have a lot of different neighbors, but we don't have enough room for neighbors in low dimensions. t-SNE modifies the above SNE framework by replacing the Gaussian in the low dimensional space with a heavy tailed $t$-distribution. Specifically, the Student t-distribution with df=1 (Cauchy) is used in low dimensional space instead of the Gaussian distribution to address the crowding problem. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://miro.medium.com/max/1286/1*TYeZkYeVSoY_4jPkpBxOgA.png)\n:::\n:::\n\n\n\nAnother modification by t-SNE is to use __symmetrized conditional probability__: $p_{i \\mid j}=p_{j \\mid i}$ and $q_{i \\mid j}=q_{j \\mid i}$. In comparison to SNE, for t-SNE we construct the conditional probability $P$ for similarity in the high-dimensional space $\\mathcal{X}$ as:\n\n$$p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|x_i-x_j\\right\\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|x_i-x_k\\right\\|^2 / 2 \\sigma_i^2\\right)} \\quad p_{i j}=\\frac{\\left(p_{j \\mid i}+p_{i \\mid j}\\right)}{2 n}$$\n\nAnd construct the conditional probability $Q$ for similarity in the low-dimensional space $\\mathcal{Y}$ as: \n\n$$q_{j \\mid i}=\\frac{\\left(1+\\left\\|y_i-y_j\\right\\|^2\\right)^{-1}}{\\sum_{k \\neq i}\\left(1+\\left\\|y_i-y_k\\right\\|^2\\right)^{-1}}, \\quad q_{i j}=\\frac{q_{i \\mid j}+q_{j \\mid i}}{2 n}$$\nWe then match the conditional probabilities by minimizing the sum of KL divergences:\n\n$$C=\\sum_{i j} p_{i j} \\log \\left(\\frac{p_{i j}}{q_{i j}}\\right)$$\n\n## t-SNE example with Starbucks data\n\nWe use the [`Rtsne`](https://github.com/jkrijthe/Rtsne) package for constructing t-SNE plots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Rtsne)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'Rtsne' was built under R version 4.2.3\n```\n:::\n\n```{.r .cell-code}\nset.seed(2013)\ntsne_fit <- starbucks |>\n  dplyr::select(serv_size_m_l:caffeine_mg) |>\n  scale() |>\n  Rtsne(check_duplicates = FALSE) #<<\n\nstarbucks |>\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |>\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) + \n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")\n```\n\n::: {.cell-output-display}\n![](06-tsne-umap_files/figure-html/starbucks-tsne-1.png){width=672}\n:::\n:::\n\n\n\nThis just constructs the t-SNE plot with the defaults, but remember the discussion in [lecture](https://cmu-36613.netlify.app/slides/lec9#9) about the impact of the perplexity parameter ([as well as randomness!](https://cmu-36613.netlify.app/slides/lec9#8)).\n\n## Criticisms of t-SNE plots\n\n- __Poor scalability__: does not scale well for large data, can practically\nonly embed into 2 or 3 dimensions\n\n- __Meaningless global structure__: distance between clusters might not\nhave clear interpretation and cluster size doesn’t have any meaning to\nit\n\n- __Poor performance with very high dimensional data__: need PCA as\npre-dimension reduction step\n\n- [__Sometime random noise can lead to false positive structure in the\nt-SNE projection__](https://distill.pub/2016/misread-tsne/)\n\n- __Can NOT interpret like PCA!__\n\n# Introduction to UMAP \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://miro.medium.com/max/1400/1*GeDO8yZJjC326v06FDTM_A.png)\n:::\n:::\n\n\n\n[UMAP (Uniform Manifold Approximation and projection)](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Uniform_manifold_approximation_and_projection) is another popular approach for nonlinear dimension reduction that has some advantages over t-SNE plots. \n\nIn the high-dimensional space $\\mathcal{X}$, let $N_k(x_i)$ be the $k$ nearest neighbors of $x_i$ and construct the conditional probability as:\n\n$$\\tilde{p}_{j \\mid i}= \\begin{cases}\\exp \\left(-\\frac{d\\left(x_i, x_j\\right)-\\rho_i}{\\sigma_i}\\right) & \\text { if } x_j \\in N_k\\left(x_i\\right) \\\\ 0 & \\text { otherwise }\\end{cases}$$\n\nwhere we have:\n\n+ $d(x_i, x_j)$ can be any pairwise distance\n\n+ $\\rho_i$ is the distance from $x_i$ to its closest neighbor\n\n+ $\\sigma_i$ plays a similar role as the perplexity-based parameter $\\sigma_i$ in t-SNE, where for UMAP it is chosen such that $\\log k = \\sum_i \\tilde{p}_{ij}$ and is controlled by parameter $k$.\n\nAnd again we symmetrize so that:\n\n$$\\tilde{p}_{i j}=\\tilde{p}_{i \\mid j}+\\tilde{p}_{j \\mid i}-\\tilde{p}_{i \\mid j} \\cdot \\tilde{p}_{j \\mid i}$$\n\nWe then construct the conditional probability in the low-dimensional space as:\n\n$$\n\\tilde{q}_{i j}= \\begin{cases}\n\\exp \\big(- (||y_i - y_j||_2 - \\text{min_dist}) \\big) \\text{ if } ||y_i - y_j||_2 > \\text{min_dist} \\\\\n1 \\text{ if } ||y_i - y_j||_2 \\leq \\text{min_dist}\n\\end{cases}\n$$\n\nwhich is  $$\\approx\\left(1+a\\left\\|y_i-y_j\\right\\|^{2 b}\\right)^{-1}$$\n\nwhere min_dist defines the distance between nearest neighbors in the low-dimensional, embedded space $\\mathcal{Y}$ (default 0.001, corresponds to $a = 1.93$, $b = 0.79$, and the Student t-distribution corresponds to $a = b = 1$).\n\nInstead of the KL-divergence, the cost function is the binary cross-entropy:\n\n$$C_{\\mathrm{UMAP}}=\\sum_{i \\neq j} \\tilde{p}_{i j} \\log \\left(\\frac{\\tilde{p}_{i j}}{\\tilde{q}_{i j}}\\right)+\\left(1-\\tilde{p}_{i j}\\right) \\log \\left(\\frac{1-\\tilde{p}_{i j}}{1-\\tilde{q}_{i j}}\\right)$$\nwhich can be written as:\n\n$$C_{\\mathrm{UMAP}}= \\text{Attractive force} + \\text{Repulsive force}$$\nIn comparison to t-SNE, UMAP has the following advantages:\n\n+ __Faster__, mainly due to the drop of normalization step in both high-dim and low-dim space. Allows arbitrary embedding dimension.\n\n+ Better job at preserving the global structure, due to the addition of repulsive loss.\n\n+ Can work with high dimensional data, doesn’t require PCA as pre-dimensional reduction step, thanks to the local connectivity parameter $\\rho_i$.\n\nSimilar to t-SNE plots, UMAP has the following weaknesses:\n\n+ Still lacks interpretability compared to PCA\n\n+ __Potential to detect spurious structure__: UMAP assumes the existence of low-dimensional manifold in the data. Need to be cautious with small sample size or data with only large scale manifold structure.\n\n+ __Local—biased__: though it captures more global structures compared to t-SNE, its still focus more on local structures compared to something like PCA.\n\n## UMAP example with Starbucks data\n\nWe use the [`umap`](https://cran.r-project.org/web/packages/umap/vignettes/umap.html) package for constructing UMAP plots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(umap)\numap_fit <- starbucks |>\n  dplyr::select(serv_size_m_l:caffeine_mg) |>\n  scale() |>\n  umap() \n\n# Convert umap_fit to table\numap_table <- umap_fit$layout |>\n  as.data.frame()\n\n# Display\nstarbucks |>\n  mutate(umap1 = umap_table[,1],\n         umap2 = umap_table[,2]) |>\n  ggplot(aes(x = umap1, y = umap2, \n             color = size)) +\n  geom_point(alpha = 0.5) + \n  labs(x = \"UMAP 1\", y = \"UMAP 2\")\n```\n\n::: {.cell-output-display}\n![](06-tsne-umap_files/figure-html/starbucks-umap-1.png){width=672}\n:::\n:::\n\n\nThe plot above was just based on the defaults:\n\n\n::: {.cell}\n\n```{.r .cell-code}\numap.defaults\n```\n\n::: {.cell-output .cell-output-stderr}\n```\numap configuration parameters\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n           n_neighbors: 15\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n          n_components: 2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n                metric: euclidean\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n              n_epochs: 200\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n                 input: data\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n                  init: spectral\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n              min_dist: 0.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n      set_op_mix_ratio: 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n    local_connectivity: 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n             bandwidth: 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n                 alpha: 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n                 gamma: 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n  negative_sample_rate: 5\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n                     a: NA\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n                     b: NA\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n                spread: 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n          random_state: NA\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n       transform_state: NA\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n                   knn: NA\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n           knn_repeats: 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n               verbose: FALSE\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n       umap_learn_args: NA\n```\n:::\n:::\n\n\nWe can choose a different number of neighbors amongst other options. We can customize UMAP by setting up a `custom_config` list to store the parameter values we want to change from the defaults. Similar to perplexity in t-SNE plots, the most important one is `n-neighbors`. Other important ones are `random_state`, which is an arbitrary integer that determines the initial random state (i.e., the seed), and `n_epochs`, which determines for how many steps the UMAP algorithm is run until it is considered converged (higher is better but just takes longer).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up umap configuration  \ncustom_config <- umap.defaults\ncustom_config$n_neighbors <- 50        \ncustom_config$random_state <- 1234\ncustom_config$n_epochs <- 500\n\n# Re-run the UMAP\ncustom_umap_fit <- starbucks |>\n  dplyr::select(serv_size_m_l:caffeine_mg) |>\n  scale() |>\n  umap(config = custom_config) \n\n# Convert umap_fit to table\ncustom_umap_table <- custom_umap_fit$layout |>\n  as.data.frame()\n\n# Display\nstarbucks |>\n  mutate(umap1 = custom_umap_table[,1],\n         umap2 = custom_umap_table[,2]) |>\n  ggplot(aes(x = umap1, y = umap2, \n             color = size)) +\n  geom_point(alpha = 0.5) + \n  labs(x = \"UMAP 1\", y = \"UMAP 2\")\n```\n\n::: {.cell-output-display}\n![](06-tsne-umap_files/figure-html/starbucks-umap-config-1.png){width=672}\n:::\n:::\n\n\n\n__Exercise__: Run the above code for a different values of the `custom_config` parameters. Pay attention to how the output changes as you change each of these parameters.\n\n",
    "supporting": [
      "06-tsne-umap_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}