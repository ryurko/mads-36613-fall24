{
  "hash": "f4c31eaa5dac9d2b9e5300ca26af4b3d",
  "result": {
    "markdown": "---\ntitle: \"Visualizations for text data\"\nauthor: \"Prof Ron Yurko\"\nfooter:  \"[mads-36613-fall24](https://ryurko.github.io/mads-36613-fall24/)\"\ndate: 2024-10-07\nengine: knitr\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    pdf-separate-fragments: true\n    slide-number: c/t\n    smaller: true\n    code-line-numbers: true\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n## Reminders, previously, and today...\n\n+ **Infographic is due Friday night!**\n\n+ You should be working on your presentations for Jamie...\n\n. . .\n\n\n+ Walked through basics of visualizing areal data\n\n+ Discussed various aspects of making high-quality graphics and relevant tools\n\n+ Completed drafts and provided feedback to each other\n\n. . .\n\n**TODAY:**\n\n+ Introduction to text data\n\n+ Overview of common visualizations for text data\n\n---\n\n## Working with raw text data \n\n- We'll work with script from the best episode of ['The Office': Season 4, Episode 13 - 'Dinner Party'](https://en.wikipedia.org/wiki/Dinner_Party_(The_Office))\n\n- We can access the script using the [`schrute` package (yes this is a real thing)](https://cran.r-project.org/web/packages/schrute/vignettes/theoffice.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(schrute)\n# Create a table from this package just corresponding to the Dinner Party episode:\ndinner_party_table <- theoffice |>\n  filter(season == 4, episode == 13) |>\n  # Just select columns of interest:\n  dplyr::select(index, character, text)\nhead(dinner_party_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  index character text                                                          \n  <int> <chr>     <chr>                                                         \n1 16791 Stanley   This is ridiculous.                                           \n2 16792 Phyllis   Do you have any idea what time we'll get out of here?         \n3 16793 Michael   Nobody likes to work late, least of all me. Do you have plans…\n4 16794 Jim       Nope I don't, remember when you told us not to make plans 'ca…\n5 16795 Michael   Yes I remember. Mmm, this is B.S. This is B.S. Why are we her…\n6 16796 Dwight    Thank you Michael.                                            \n```\n:::\n:::\n\n\n---\n\n## Bag of Words representation of text\n\n- Most common way to store text data is with a __document-term matrix__ (DTM):\n\n|            | Word 1   | Word 2   | $\\dots$  | Word $J$ |\n| ---------- | -------- | -------- | -------- | -------- |\n| Document 1 | $w_{11}$ | $w_{12}$ | $\\dots$  | $w_{1J}$ |\n| Document 2 | $w_{21}$ | $w_{22}$ | $\\dots$  | $w_{2J}$ |\n| $\\dots$    | $\\dots$  | $\\dots$  | $\\dots$  | $\\dots$  |\n| Document N | $w_{N1}$ | $w_{N2}$ | $\\dots$  | $w_{NJ}$ |\n\n- $w_{ij}$: count of word $j$ in document $i$, aka _term frequencies_\n\n. . .\n\nTwo additional ways to reduce number of columns:\n\n1. __Stop words__: remove extremely common words (e.g., of, the, a)\n\n2. __Stemming__: Reduce all words to their \"stem\"\n\n  - For example: Reducing = reduc. Reduce = reduc. Reduces = reduc.\n\n\n---\n\n## Tokenize text into long format\n\n- Convert raw text into long, tidy table with one-token-per-document-per-row\n\n  - A __token__ equals a unit of text - typically a word\n  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidytext)\ntidy_dinner_party_tokens <- dinner_party_table |>\n  unnest_tokens(word, text)\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  index character word      \n  <int> <chr>     <chr>     \n1 16791 Stanley   this      \n2 16791 Stanley   is        \n3 16791 Stanley   ridiculous\n4 16792 Phyllis   do        \n5 16792 Phyllis   you       \n6 16792 Phyllis   have      \n```\n:::\n:::\n\n\nEasy to convert text into DTM format using [`tidytext` package](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)\n\n---\n\n## Remove stop words\n\n- Load `stop_words` from `tidytext`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(stop_words)\n\ntidy_dinner_party_tokens <- tidy_dinner_party_tokens |>\n  filter(!(word %in% stop_words$word))\n\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  index character word      \n  <int> <chr>     <chr>     \n1 16791 Stanley   ridiculous\n2 16792 Phyllis   idea      \n3 16792 Phyllis   time      \n4 16793 Michael   likes     \n5 16793 Michael   late      \n6 16793 Michael   plans     \n```\n:::\n:::\n\n\n---\n\n## Apply stemming\n\n- Can use [`SnowballC` package](https://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf) to perform stemming\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(SnowballC)\n\ntidy_dinner_party_tokens <- tidy_dinner_party_tokens |>\n  mutate(stem = wordStem(word))\n\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  index character word       stem   \n  <int> <chr>     <chr>      <chr>  \n1 16791 Stanley   ridiculous ridicul\n2 16792 Phyllis   idea       idea   \n3 16792 Phyllis   time       time   \n4 16793 Michael   likes      like   \n5 16793 Michael   late       late   \n6 16793 Michael   plans      plan   \n```\n:::\n:::\n\n\n---\n\n## Create word cloud using term frequencies\n\n__Word Cloud__: Displays all words mentioned across documents, where more common words are larger\n\n- To do this, you must compute the _total_ word counts:\n\n$$w_{\\cdot 1} = \\sum_{i=1}^N w_{i1} \\hspace{0.1in} \\dots \\hspace{0.1in} w_{\\cdot J} = \\sum_{i=1}^N w_{iJ}$$\n\n- Then, the size of Word $j$ is proportional to $w_{\\cdot j}$\n\n. . .\n\nCreate word clouds in `R` using [`wordcloud` package](https://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf)\n\nTakes in two main arguments to create word clouds:\n\n1. `words`: vector of unique words\n\n2. `freq`: vector of frequencies\n\n\n---\n\n## Create word cloud using term frequencies\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ntoken_summary <- tidy_dinner_party_tokens |>\n  group_by(stem) |>\n  count() |>\n  ungroup() \n\nlibrary(wordcloud)\nwordcloud(words = token_summary$stem, \n          freq = token_summary$n, \n          random.order = FALSE, \n          max.words = 100, \n          colors = brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](11-text-data_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n- Set `random.order = FALSE` to place biggest words in center\n\n- Can customize to display limited # words (`max.words`)\n\n- Other options as well like `colors`\n\n\n---\n\n## Comparison clouds\n\nImagine we have two different collections of documents, $\\mathcal{A}$ and $\\mathcal{B}$, that we wish to visually compare.\n\n. . .\n\nImagine we create the word clouds for the two collections of documents. Then this means we constructed vectors of total words for each collection:\n\n+ $\\mathbf{w}^{\\mathcal{A}} = (w_{\\cdot 1}^{\\mathcal{A}}, \\dots, w_{\\cdot J}^{\\mathcal{A}})$\n\n+ $\\mathbf{w}^{\\mathcal{B}} = (w_{\\cdot 1}^{\\mathcal{B}}, \\dots, w_{\\cdot J}^{\\mathcal{B}})$\n\nConsider the $j$th word, let's pretend it's \"dinner\":\n\n+ If $w_{\\cdot j}^{\\mathcal{A}}$ is large, then \"dinner\" is large in the word cloud for $\\mathcal{A}$.\n\n+ If $w_{\\cdot j}^{\\mathcal{B}}$ is large, then \"dinner\" is large in the word cloud for $\\mathcal{B}$.\n\n+ But if both are large, this doesn't tell us whether $w_{\\cdot j}^{\\mathcal{A}}$ or $w_{\\cdot j}^{\\mathcal{B}}$ is bigger.\n\n\n---\n\n## Comparison clouds\n\nThis motivates the construction of __comparison word clouds__: \n\n1. For word $j$, compute $\\bar{w}_{\\cdot j} = \\text{average}(w_{\\cdot j}^{\\mathcal{A}}, w_{\\cdot j}^{\\mathcal{B}})$\n\n2. Compute $w_{\\cdot j}^{\\mathcal{A}} - \\bar{w}_{\\cdot j}$ and $w_{\\cdot j}^{\\mathcal{B}} - \\bar{w}_{\\cdot j}$\n\n3. If $w_{\\cdot j}^{\\mathcal{A}} - \\bar{w}_{\\cdot j}$ is very positive, make it large for the $\\mathcal{A}$ word cloud. If $w_{\\cdot j}^{\\mathcal{B}} - \\bar{w}_{\\cdot j}$ is very positive, make it large for the $\\mathcal{B}$ word cloud.\n\n\n---\n\n## Comparison clouds\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-text-data_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## TF-IDF weighting\n\n- We saw that `michael` was the largest word, but what if I'm interested in comparing text across characters (i.e., documents)?\n\n. . .\n\n- It’s arguably of more interest to understand which words are frequently used in one set of texts but not the other, i.e., which words are unique?\n\n- Many text analytics methods will __down-weight__ words that occur frequently across all documents\n\n. . .\n\n- __Inverse document frequency (IDF)__: for word $j$ we compute $\\text{idf}_j = \\log \\frac{N}{N_j}$\n\n  - where $N$ is number of documents, $N_j$ is number of documents with word $j$\n\n- Compute __TF-IDF__ $= w_{ij} \\times \\text{idf}_j$\n\n---\n\n## TF-IDF example with characters\n\nCompute and join TF-IDF using `bind_tf_idf()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncharacter_token_summary <- tidy_dinner_party_tokens |>\n  group_by(character, stem) |> \n  count() |>\n  ungroup() \n\ncharacter_token_summary <- character_token_summary |>\n  bind_tf_idf(stem, character, n) \ncharacter_token_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 597 × 6\n   character stem        n     tf   idf tf_idf\n   <chr>     <chr>   <int>  <dbl> <dbl>  <dbl>\n 1 All       cheer       1 1      2.77  2.77  \n 2 Andy      anim        1 0.0476 2.77  0.132 \n 3 Andy      bet         1 0.0476 2.08  0.0990\n 4 Andy      capit       1 0.0476 2.77  0.132 \n 5 Andy      dinner      1 0.0476 0.981 0.0467\n 6 Andy      flower      2 0.0952 2.77  0.264 \n 7 Andy      hei         1 0.0476 1.39  0.0660\n 8 Andy      helena      1 0.0476 2.77  0.132 \n 9 Andy      hump        2 0.0952 2.77  0.264 \n10 Andy      michael     1 0.0476 0.981 0.0467\n# ℹ 587 more rows\n```\n:::\n:::\n\n\n\n---\n\n## Top 10 words by TF-IDF for each character\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ncharacter_token_summary |>\n  filter(character %in% c(\"Michael\", \"Jan\", \"Jim\", \"Pam\")) |>\n  group_by(character) |>\n  slice_max(tf_idf, n = 10, with_ties = FALSE) |>\n  ungroup() |>\n  mutate(stem = reorder_within(stem, tf_idf, character)) |>\n  ggplot(aes(y = tf_idf, x = stem),\n         fill = \"darkblue\", alpha = 0.5) +\n  geom_col() +\n  coord_flip() +\n  scale_x_reordered() +\n  facet_wrap(~ character, ncol = 2, scales = \"free\") +\n  labs(y = \"TF-IDF\", x = NULL)\n```\n\n::: {.cell-output-display}\n![](11-text-data_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Sentiment Analysis\n\n- The visualizations so far only look at word _frequency_ (possibly weighted with TF-IDF), but doesn't tell you _how_ words are used\n  \n. . .\n\n- A common goal in text analysis is to try to understand the overall __sentiment__ or \"feeling\" of text, i.e., __sentiment analysis__\n\n- Typical approach:\n\n  1.  Find a sentiment dictionary (e.g., \"positive\" and \"negative\" words)\n  \n  2. Count the number of words belonging to each sentiment\n  \n  3. Using the counts, you can compute an \"average sentiment\" (e.g., positive counts - negative counts)\n  \n. . .\n\n- This is called a __dictionary-based approach__\n\n- The __Bing__ dictionary (named after Bing Liu) provides 6,786 words that are either \"positive\" or \"negative\"\n\n---\n\n## Character sentiment analysis\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_sentiments(\"bing\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6,786 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n```\n:::\n:::\n\n\n\n---\n\n## Character sentiment analysis\n\nJoin sentiment to token table (without stemming)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy_all_tokens <- dinner_party_table |>\n  unnest_tokens(word, text)\n\ntidy_sentiment_tokens <- tidy_all_tokens |>\n  inner_join(get_sentiments(\"bing\")) \n\nhead(tidy_sentiment_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  index character word       sentiment\n  <int> <chr>     <chr>      <chr>    \n1 16791 Stanley   ridiculous negative \n2 16793 Michael   likes      positive \n3 16793 Michael   work       positive \n4 16795 Michael   enough     positive \n5 16795 Michael   enough     positive \n6 16795 Michael   mad        negative \n```\n:::\n:::\n\n\n\n---\n\n## Character sentiment analysis\n\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ntidy_sentiment_tokens |>\n  group_by(character, sentiment) |>\n  summarize(n_words = n()) |>\n  ungroup() |>\n  group_by(character) |>\n  mutate(total_assigned_words = sum(n_words)) |>\n  ungroup() |>\n  mutate(character = fct_reorder(character, total_assigned_words)) |>\n  ggplot(aes(x = character, y = n_words, fill = sentiment)) + \n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](11-text-data_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Other functions of text\n\n- We've just focused on word counts - __but there are many functions of text__\n\n- For example: __number of unique words__ is often used to measure vocabulary\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://pbs.twimg.com/media/DxCgsrxWwAAOWO3.jpg){fig-align='center' width=70%}\n:::\n:::\n\n\n---\n\n## Recap and next steps\n\n- Most common representation: Bag of words and term frequencies (possibly weighted by TF-IDF)\n\n. . .\n\n- Word clouds are the most common way to visualize the most frequent\nwords in a set of documents\n\n- TF-IDF weighting allows you to detect words that are uniquely used\nin certain documents\n\n. . .\n\n- Can also measure the \"sentiment\" of text with sentiment-based dictionaries\n\n. . .\n\n+ **Infographic is due Friday night!**\n\n+ [Text Mining With R](https://www.tidytextmining.com/), [Supervised Machine Learning for Text Analysis in R](https://smltar.com/)\n",
    "supporting": [
      "11-text-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}