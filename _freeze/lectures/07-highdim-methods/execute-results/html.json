{
  "hash": "31fe980017fbc2321ed729d3ab7396f7",
  "result": {
    "markdown": "---\ntitle: \"High-Dimensional Data\"\nauthor: \"Prof Ron Yurko\"\nfooter:  \"[mads-36613-fall24](https://ryurko.github.io/mads-36613-fall24/)\"\ndate: 2024-09-18\nengine: knitr\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    pdf-separate-fragments: true\n    slide-number: c/t\n    smaller: true\n    code-line-numbers: true\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n## Reminders, previously, and today...\n\n+ **HW3 is due TONIGHT!**\n\n+ **HW4 is due next Wednesday Sept 25th**\n\n. . .\n\n+ Discussed creating pairs plots for initial inspection of several variables\n\n+ Began thinking about ways to displays dataset structure via correlations\n\n+ Used heatmaps and parallel coordinates plot to capture observation and variable structure\n\n. . .\n\n**TODAY:**\n\n+ Projections based on some notion of \"distance\"\n\n+ **Intuition**: Take high-dimensional data and **represent it in 2-3 dimensions**, then visualize those dimensions\n\n---\n\n## Thinking about distance...\n\nWhen describing visuals, we've implicitly \"clustered\" observations together\n\nThese types of task require characterizing the __distance__ between observations\n\nThis is easy to do for 2 quantitative variables: just make a scatterplot (possibly with contours or heatmap)\n\n**But how do we define \"distance\" for high-dimensional data?**\n\n. . .\n\nLet $\\boldsymbol{x}_i = (x_{i1}, \\dots, x_{ip})$ be a vector of $p$ features for observation $i$\n\nQuestion of interest: How \"far away\" is $\\boldsymbol{x}_i$ from $\\boldsymbol{x}_j$?\n\n. . .\n\nWhen looking at a scatterplot, you're using __Euclidean distance__ (length of the line in $p$-dimensional space):\n\n$$d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\dots + (x_{ip} - x_{jp})^2}$$\n\n---\n\n## Distances in general\n\nThere's a variety of different types of distance metrics: [Manhattan](https://en.wikipedia.org/wiki/Taxicab_geometry), [Mahalanobis](https://en.wikipedia.org/wiki/Mahalanobis_distance), [Cosine](https://en.wikipedia.org/wiki/Cosine_similarity), [Kullback-Leiber Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), [Wasserstein](https://en.wikipedia.org/wiki/Wasserstein_metric), but we're just going to focus on [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n\n. . .\n\n$d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$ measures pairwise distance between two observations $i,j$ and has the following properties:\n\n1. __Identity__: $\\boldsymbol{x}_i = \\boldsymbol{x}_j \\iff d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = 0$\n\n2. __Non-Negativity__: $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\geq 0$\n\n3. __Symmetry__: $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = d(\\boldsymbol{x}_j, \\boldsymbol{x}_i)$\n\n4. __Triange Inequality__: $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\leq d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) + d(\\boldsymbol{x}_k, \\boldsymbol{x}_j)$\n\n. . .\n\n__Distance Matrix__: matrix $D$ of all pairwise distances\n\n- $D_{ij} = d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$\n\n- where $D_{ii} = 0$ and $D_{ij} = D_{ji}$\n\n\n---\n\n## Multi-dimensional scaling (MDS)\n\n**General approach for visualizing distance matrices**\n\n- Puts $n$ observations in a $k$-dimensional space such that the distances are preserved as much as possible (where $k << p$ typically choose $k = 2$)\n  \n. . .\n\nMDS attempts to create new point $\\boldsymbol{y}_i = (y_{i1}, y_{i2})$ for each unit such that:\n\n$$\\sqrt{(y_{i1} - y_{j1})^2 + (y_{i2} - y_{j2})^2} \\approx D_{ij}$$\n\n- i.e., distance in 2D MDS world is approximately equal to the actual distance\n\n. . .\n\n**Then plot the new $\\boldsymbol{y}$s on a scatterplot**\n\n- Use the `scale()` function to ensure variables are comparable\n\n- Make a distance matrix for this dataset\n\n- Visualize it with MDS\n\n---\n\n## MDS example with Starbucks drinks\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\nstarbucks_scaled_quant_data <- starbucks |>\n  dplyr::select(serv_size_m_l:caffeine_mg) %>%\n  scale(center = FALSE, scale = apply(., 2, sd, na.rm = TRUE))\n\ndist_euc <- dist(starbucks_scaled_quant_data)\n\nstarbucks_mds <- cmdscale(d = dist_euc, k = 2)\n\nstarbucks <- starbucks |>\n  mutate(mds1 = starbucks_mds[,1],\n         mds2 = starbucks_mds[,2])\n\nstarbucks |>\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n```\n\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/starbucks-mds-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## View structure with additional variables\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## View structure with additional variables\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Dimension reduction - searching for variance\n\n__GOAL__: Focus on reducing dimensionality of feature space while __retaining__ most of the information in a lower dimensional space\n\n- $n \\times p$ matrix $\\rightarrow$ dimension reduction technique $\\rightarrow$ $n \\times k$ matrix\n\n. . .\n\nSpecial case we just discussed: __MDS__\n\n- $n \\times n$ __distance__ matrix $\\rightarrow$ MDS $\\rightarrow$ $n \\times k$ matrix (usually $k = 2$)\n\n1. Reduce data to a distance matrix\n\n2. Reduce distance matrix to $k = 2$ dimensions\n\n---\n\n## Principal Component Analysis (PCA)\n\n$$\n\\begin{pmatrix}\n& & \\text{really} & & \\\\\n& & \\text{wide} & & \\\\\n& & \\text{matrix} & &\n\\end{pmatrix}\n\\rightarrow \\text{matrix algebra stuff} \\rightarrow \n\\begin{pmatrix}\n\\text{much}  \\\\\n\\text{thinner}  \\\\\n\\text{matrix} \n\\end{pmatrix}\n$$\n\n- Start with $n \\times p$ matrix of __correlated__ variables $\\rightarrow$ $n \\times k$ matrix of __uncorrelated__ variables\n\n. . .\n\n- Each of the $k$ columns in the right-hand matrix are __principal components__, all uncorrelated with each other\n\n- First column accounts for most variation in the data, second column for second-most variation, and so on\n\n**Intuition: first few principal components account for most of the variation in the data**\n\n---\n\n## What are principal components?\n\n- Assume $\\boldsymbol{X}$ is a $n \\times p$ matrix that is __centered__ and __stardardized__\n\n- _Total variation_ $= p$, since Var( $\\boldsymbol{x}_j$ ) = 1 for all $j = 1, \\dots, p$\n\n- PCA will give us $p$ principal components that are $n$-length columns - call these $Z_1, \\dots, Z_p$\n\n. . .\n\n__First principal component__ (aka PC1):\n\n$$Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\dots + \\phi_{p1} X_p$$\n\n. . .\n\n  - $\\phi_{j1}$ are the weights indicating the contributions of each variable $j \\in 1, \\dots, p$\n  \n  - Weights are normalized $\\sum_{j=1}^p \\phi_{j1}^2 = 1$\n  \n  - $\\phi_{1} = (\\phi_{11}, \\phi_{21}, \\dots, \\phi_{p1})$ is the __loading vector__ for PC1\n\n. . .\n  \n  - $Z_1$ is a linear combination of the $p$ variables that has the __largest variance__\n\n---\n\n## What are principal components?\n\n__Second principal component__:\n\n$$Z_2 = \\phi_{12} X_1 + \\phi_{22} X_2 + \\dots + \\phi_{p2} X_p$$\n\n  - $\\phi_{j2}$ are the weights indicating the contributions of each variable $j \\in 1, \\dots, p$\n  \n  - Weights are normalized $\\sum_{j=1}^p \\phi_{j1}^2 = 1$\n  \n  - $\\phi_{2} = (\\phi_{12}, \\phi_{22}, \\dots, \\phi_{p2})$ is the __loading vector__ for PC2\n  \n  - $Z_2$ is a linear combination of the $p$ variables that has the __largest variance__\n  \n    - __Subject to constraint it is uncorrelated with $Z_1$__ \n\n---\n\n## What are principal components?\n\nWe repeat this process to create $p$ principal components\n\n- __Uncorrelated__: Each ($Z_j, Z_{j'}$) is uncorrelated with each other\n\n- __Ordered Variance__: Var( $Z_1$ ) $>$ Var( $Z_2$ ) $> \\dots >$ Var( $Z_p$ )\n\n- __Total Variance__: $\\sum_{j=1}^p \\text{Var}(Z_j) = p$\n\n\n**Intuition: pick some $k << p$ such that if $\\sum_{j=1}^k \\text{Var}(Z_j) \\approx p$, then just using $Z_1, \\dots, Z_k$**\n\n---\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n---\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n---\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n---\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n---\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n---\n\n## So what do we do with the principal components?\n\n__The point__: given a dataset with $p$ variables, we can find $k$ variables $(k << p)$ that account for most of the variation in the data\n\n. . .\n\nNote that the principal components are NOT easy to interpret - these are combinations of all variables\n\nPCA is similar to MDS with these main differences:\n\n1. MDS reduces a _distance_ matrix while PCA reduces a _data_ matrix\n\n2. PCA has a principled way to choose $k$\n\n3. Can visualize how the principal components are related to variables in data\n\n---\n\n## Working with PCA on Starbucks drinks\n\nUse the `prcomp()` function (based on SVD) for PCA on __centered__ and __scaled__ data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstarbucks_pca <- prcomp(dplyr::select(starbucks, serv_size_m_l:caffeine_mg),\n                        center = TRUE, scale. = TRUE)\nsummary(starbucks_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n```\n:::\n:::\n\n\n---\n\n## Computing Principal Components\n\nExtract the matrix of principal components $\\boldsymbol{Z} = XV$ (dimension of $\\boldsymbol{Z}$ will match original data)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstarbucks_pc_matrix <- starbucks_pca$x\nhead(starbucks_pc_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n```\n:::\n:::\n\n\nColumns are uncorrelated, such that Var( $Z_1$ ) $>$ Var( $Z_2$ ) $> \\dots >$ Var( $Z_p$ ) - can start with a scatterplot of $Z_1, Z_2$\n\n---\n\n## Starbucks drinks: PC1 and PC2\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\nstarbucks <- starbucks |>\n  mutate(pc1 = starbucks_pc_matrix[,1], \n         pc2 = starbucks_pc_matrix[,2])\nstarbucks |>\n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"PC 1\", y = \"PC 2\")\n```\n\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/pca-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Making PCs interpretable with biplots ([`factoextra`](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization))\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\nlibrary(factoextra)\n# Designate to only label the variables:\nfviz_pca_biplot( \n  starbucks_pca, label = \"var\",\n  # Change the alpha for observations \n  # which is represented by ind\n  alpha.ind = .25,\n  # Modify the alpha for variables (var):\n  alpha.var = .75,\n  col.var = \"darkblue\")\n```\n\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/pca-biplot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n---\n\n## How many principal components to use?\n\n#### Intuition: Additional principal components will add smaller and smaller variance\n\n- Keep adding components until the added variance _drops off_\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(starbucks_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n```\n:::\n:::\n\n\n---\n\n## Create scree plot (aka \"elbow plot\") to choose\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfviz_eig(starbucks_pca, addlabels = TRUE) + \n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_pca$x)), linetype = \"dashed\", color = \"darkred\")\n```\n\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/scree-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Nonlinear dimension reduction, e.g., t-SNE\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Consider the following spiral structure...\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## PCA simply rotates the data...\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Nonlinear dimension reduction with t-SNE\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## [t-distributed stochastic neighbor embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n\n- Construct conditional probability for similarity between observations in original space, i.e., probability $x_i$ will pick $x_j$ as its neighbor \n\n$$p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|x_i-x_j\\right\\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|x_i-x_k\\right\\|^2 / 2 \\sigma_i^2\\right)},\\quad p_{i j}=\\frac{\\left(p_{j \\mid i}+p_{i \\mid j}\\right)}{2 n}$$\n\n- $\\sigma_i$ is the variance of Gaussian centered at $x_i$ controlled by __perplexity__:  $\\log (\\text { perplexity })=-\\sum_j p_{j \\mid i} \\log _2 p_{j \\mid i}$\n\n---\n\n## [t-distributed stochastic neighbor embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n\n- Find points $y_i$ in lower dimensional space with symmetrized student t-distribution\n\n$$q_{j \\mid i}=\\frac{\\left(1+\\left\\|y_i-y_j\\right\\|^2\\right)^{-1}}{\\sum_{k \\neq i}\\left(1+\\left\\|y_i-y_k\\right\\|^2\\right)^{-1}}, \\quad q_{i j}=\\frac{q_{i \\mid j}+q_{j \\mid i}}{2 n}$$\n\n- Match conditional probabilities by minimize sum of KL divergences $C=\\sum_{i j} p_{i j} \\log \\left(\\frac{p_{i j}}{q_{i j}}\\right)$\n\n---\n\n## Starbucks t-SNE plot with [`Rtsne`](https://github.com/jkrijthe/Rtsne) \n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\nset.seed(2013)\ntsne_fit <- starbucks |>\n  dplyr::select(serv_size_m_l:caffeine_mg) |>\n  scale() |>\n  Rtsne(check_duplicates = FALSE) \n\nstarbucks |>\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |>\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) + \n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")\n```\n\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## Starbucks t-SNE plot - involves randomness!\n\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\nset.seed(2014) \ntsne_fit <- starbucks |>\n  dplyr::select(serv_size_m_l:caffeine_mg) |>\n  scale() |>\n  Rtsne(check_duplicates = FALSE) \n\nstarbucks |>\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |>\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")\n```\n\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## Starbucks t-SNE plot - watch the perplexity!\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\nset.seed(2013) \ntsne_fit <- starbucks |>\n  dplyr::select(serv_size_m_l:caffeine_mg) |>\n  scale() |>\n  Rtsne(perplexity = 100, \n        check_duplicates = FALSE)\n\nstarbucks |>\n  mutate(tsne1 = tsne_fit$Y[,1],\n         tsne2 = tsne_fit$Y[,2]) |>\n  ggplot(aes(x = tsne1, y = tsne2, \n             color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"t-SNE 1\", y = \"t-SNE 2\")\n```\n\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Back to the spirals: results depend on perplexity!\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-highdim-methods_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Criticisms of t-SNE plots\n\n\n- __Poor scalability__: does not scale well for large data, can practically\nonly embed into 2 or 3 dimensions\n\n- __Meaningless global structure__: distance between clusters might not\nhave clear interpretation and cluster size doesn’t have any meaning to\nit\n\n- __Poor performance with very high dimensional data__: need PCA as\npre-dimension reduction step\n\n- [__Sometime random noise can lead to false positive structure in the\nt-SNE projection__](https://distill.pub/2016/misread-tsne/)\n\n- __Can NOT interpret like PCA!__\n\n---\n\n\n## Recap and next steps\n\n+ Walked through PCA for dimension reduction\n\n+ Discussed non-linear dimension reduction with t-SNE plots\n\n. . .\n\n+ **HW3 is due TONIGHT!**\n\n+ **HW4 is posted due next Wednesday Sept 25th**\n\n. . .\n\n+ **Next time**: Visualizing trends and time series data\n\n+ Recommended reading: [CW Chapter 12 Visualizing associations among two or more quantitative variables](https://clauswilke.com/dataviz/visualizing-associations.html), [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/), [Understanding UMAP](https://pair-code.github.io/understanding-umap/)\n\n---\n\n## PCA: [__singular value decomposition (SVD)__](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n\n$$\nX = U D V^T\n$$\n\n- Matrices $U$ and $V$ contain the left and right (respectively) __singular vectors of scaled matrix $X$__\n\n- $D$ is the diagonal matrix of the __singular values__\n\n- SVD simplifies matrix-vector multiplication as __rotate, scale, and rotate again__\n\n$V$ is called the __loading matrix__ for $X$ with $\\phi_{j}$ as columns, \n\n  - $Z = X  V$ is the PC matrix\n  \n---\n\n## Eigenvalue decomposition (aka spectral decomposition)\n\n$$\nX = U D V^T\n$$\n\n- $V$ are the __eigenvectors__ of $X^TX$ (covariance matrix, $^T$ means _transpose_)\n\n- $U$ are the __eigenvectors__ of $XX^T$\n\n- The singular values (diagonal of $D$) are square roots of the __eigenvalues__ of $X^TX$ or $XX^T$\n\n- Meaning that $Z = UD$\n\n---\n\n## Eigenvalues guide dimension reduction\n\nWe want to choose $p^* < p$ such that we are explaining variation in the data\n\nEigenvalues $\\lambda_j$ for $j \\in 1, \\dots, p$ indicate __the variance explained by each component__\n\n  - $\\sum_j^p \\lambda_j = p$, meaning $\\lambda_j \\geq 1$ indicates $\\text{PC}j$ contains at least one variable's worth in variability\n  \n  - $\\lambda_j / p$ equals proportion of variance explained by $\\text{PC}j$\n  \n  - Arranged in descending order so that $\\lambda_1$ is largest eigenvalue and corresponds to PC1\n  \n  - Can compute the cumulative proportion of variance explained (CVE) with $p^*$ components:\n  \n$$\\text{CVE}_{p^*} = \\frac{\\sum_j^{p*} \\lambda_j}{p}$$\n\n\n\n\n",
    "supporting": [
      "07-highdim-methods_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}